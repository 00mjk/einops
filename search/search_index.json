{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This video in better quality. einops Flexible and powerful tensor operations for readable and reliable code. Supports numpy, pytorch, tensorflow, and others . Tweets In case you need convincing arguments for setting aside time to learn about einsum and einops... Tim Rockt\u00e4schel, FAIR Writing better code with PyTorch and einops \ud83d\udc4c Andrej Karpathy, AI at Tesla Slowly but surely, einops is seeping in to every nook and cranny of my code. If you find yourself shuffling around bazillion dimensional tensors, this might change your life Nasim Rahaman, MILA (Montreal) Contents Documentation Tutorial API micro-reference Installation Naming Why using einops Supported frameworks Contributing Github repository (for issues/questions) Tutorials Tutorials are the most convenient way to see einops in action (and right now work as a documentation) part 1: einops fundamentals part 2: einops for deep learning part 3: real code fragments improved with einops (so far only for pytorch) Installation Plain and simple: pip install einops API einops has a minimalistic yet powerful API. Three operations provided ( einops tutorial shows those cover stacking, reshape, transposition, squeeze/unsqueeze, repeat, tile, concatenate, view and numerous reductions) from einops import rearrange , reduce , repeat # rearrange elements according to the pattern output_tensor = rearrange ( input_tensor , 't b c -> b c t' ) # combine rearrangement and reduction output_tensor = reduce ( input_tensor , 'b c (h h2) (w w2) -> b h w c' , 'mean' , h2 = 2 , w2 = 2 ) # copy along a new axis output_tensor = repeat ( input_tensor , 'h w -> h w c' , c = 3 ) And two corresponding layers ( einops keeps a separate version for each framework) with the same API. from einops.layers.chainer import Rearrange , Reduce from einops.layers.gluon import Rearrange , Reduce from einops.layers.keras import Rearrange , Reduce from einops.layers.torch import Rearrange , Reduce from einops.layers.tensorflow import Rearrange , Reduce Layers behave similarly to operations and have the same parameters (with the exception of the first argument, which is passed during call) layer = Rearrange ( pattern , ** axes_lengths ) layer = Reduce ( pattern , reduction , ** axes_lengths ) # apply created layer to a tensor / variable x = layer ( x ) Example of using layers within a model: # example given for pytorch, but code in other frameworks is almost identical from torch.nn import Sequential , Conv2d , MaxPool2d , Linear , ReLU from einops.layers.torch import Rearrange model = Sequential ( Conv2d ( 3 , 6 , kernel_size = 5 ), MaxPool2d ( kernel_size = 2 ), Conv2d ( 6 , 16 , kernel_size = 5 ), MaxPool2d ( kernel_size = 2 ), # flattening Rearrange ( 'b c h w -> b (c h w)' ), Linear ( 16 * 5 * 5 , 120 ), ReLU (), Linear ( 120 , 10 ), ) Naming einops stands for Einstein-Inspired Notation for operations (though \"Einstein operations\" is more attractive and easier to remember). Notation was loosely inspired by Einstein summation (in particular by numpy.einsum operation). Why use einops notation?! Semantic information (being verbose in expectations) y = x . view ( x . shape [ 0 ], - 1 ) y = rearrange ( x , 'b c h w -> b (c h w)' ) While these two lines are doing the same job in some context, the second one provides information about the input and output. In other words, einops focuses on interface: what is the input and output , not how the output is computed. The next operation looks similar: y = rearrange ( x , 'time c h w -> time (c h w)' ) but it gives the reader a hint: this is not an independent batch of images we are processing, but rather a sequence (video). Semantic information makes the code easier to read and maintain. More checks Reconsider the same example: y = x . view ( x . shape [ 0 ], - 1 ) # x: (batch, 256, 19, 19) y = rearrange ( x , 'b c h w -> b (c h w)' ) The second line checks that the input has four dimensions, but you can also specify particular dimensions. That's opposed to just writing comments about shapes since comments don't work and don't prevent mistakes as we know y = x . view ( x . shape [ 0 ], - 1 ) # x: (batch, 256, 19, 19) y = rearrange ( x , 'b c h w -> b (c h w)' , c = 256 , h = 19 , w = 19 ) Result is strictly determined Below we have at least two ways to define the depth-to-space operation # depth-to-space rearrange ( x , 'b c (h h2) (w w2) -> b (c h2 w2) h w' , h2 = 2 , w2 = 2 ) rearrange ( x , 'b c (h h2) (w w2) -> b (h2 w2 c) h w' , h2 = 2 , w2 = 2 ) There are at least four more ways to do it. Which one is used by the framework? These details are ignored, since usually it makes no difference, but it can make a big difference (e.g. if you use grouped convolutions in the next stage), and you'd like to specify this in your code. Uniformity reduce ( x , 'b c (x dx) -> b c x' , 'max' , dx = 2 ) reduce ( x , 'b c (x dx) (y dy) -> b c x y' , 'max' , dx = 2 , dy = 3 ) reduce ( x , 'b c (x dx) (y dy) (z dz)-> b c x y z' , 'max' , dx = 2 , dy = 3 , dz = 4 ) These examples demonstrated that we don't use separate operations for 1d/2d/3d pooling, those are all defined in a uniform way. Space-to-depth and depth-to space are defined in many frameworks but how about width-to-height? rearrange ( x , 'b c h (w w2) -> b c (h w2) w' , w2 = 2 ) Framework independent behavior Even simple functions are defined differently by different frameworks y = x . flatten () # or flatten(x) Suppose x 's shape was (3, 4, 5) , then y has shape ... - numpy, cupy, chainer, pytorch: (60,) - keras, tensorflow.layers, mxnet and gluon: (3, 20) Independence of framework terminology Example: tile vs repeat causes lots of confusion. To copy image along width: np . tile ( image , ( 1 , 2 )) # in numpy image . repeat ( 1 , 2 ) # pytorch's repeat ~ numpy's tile With einops you don't need to decipher which axis was repeated: repeat ( image , 'h w -> h (tile w)' , tile = 2 ) # in numpy repeat ( image , 'h w -> h (tile w)' , tile = 2 ) # in pytorch repeat ( image , 'h w -> h (tile w)' , tile = 2 ) # in tf repeat ( image , 'h w -> h (tile w)' , tile = 2 ) # in jax repeat ( image , 'h w -> h (tile w)' , tile = 2 ) # in mxnet ... ( etc . ) Supported frameworks Einops works with ... numpy pytorch tensorflow jax cupy chainer gluon tf.keras mxnet (experimental) Contributing Best ways to contribute are spread the word about einops if you like explaining things, alternative tutorials are very helpful translating examples in languages other than English is also a good idea use einops notation in your papers to strictly define used operations! Supported python versions einops works with python 3.6 or later.","title":"Introduction"},{"location":"#einops","text":"Flexible and powerful tensor operations for readable and reliable code. Supports numpy, pytorch, tensorflow, and others .","title":"einops"},{"location":"#tweets","text":"In case you need convincing arguments for setting aside time to learn about einsum and einops... Tim Rockt\u00e4schel, FAIR Writing better code with PyTorch and einops \ud83d\udc4c Andrej Karpathy, AI at Tesla Slowly but surely, einops is seeping in to every nook and cranny of my code. If you find yourself shuffling around bazillion dimensional tensors, this might change your life Nasim Rahaman, MILA (Montreal)","title":"Tweets"},{"location":"#contents","text":"Documentation Tutorial API micro-reference Installation Naming Why using einops Supported frameworks Contributing Github repository (for issues/questions)","title":"Contents"},{"location":"#tutorials","text":"Tutorials are the most convenient way to see einops in action (and right now work as a documentation) part 1: einops fundamentals part 2: einops for deep learning part 3: real code fragments improved with einops (so far only for pytorch)","title":"Tutorials"},{"location":"#installation","text":"Plain and simple: pip install einops","title":"Installation"},{"location":"#api","text":"einops has a minimalistic yet powerful API. Three operations provided ( einops tutorial shows those cover stacking, reshape, transposition, squeeze/unsqueeze, repeat, tile, concatenate, view and numerous reductions) from einops import rearrange , reduce , repeat # rearrange elements according to the pattern output_tensor = rearrange ( input_tensor , 't b c -> b c t' ) # combine rearrangement and reduction output_tensor = reduce ( input_tensor , 'b c (h h2) (w w2) -> b h w c' , 'mean' , h2 = 2 , w2 = 2 ) # copy along a new axis output_tensor = repeat ( input_tensor , 'h w -> h w c' , c = 3 ) And two corresponding layers ( einops keeps a separate version for each framework) with the same API. from einops.layers.chainer import Rearrange , Reduce from einops.layers.gluon import Rearrange , Reduce from einops.layers.keras import Rearrange , Reduce from einops.layers.torch import Rearrange , Reduce from einops.layers.tensorflow import Rearrange , Reduce Layers behave similarly to operations and have the same parameters (with the exception of the first argument, which is passed during call) layer = Rearrange ( pattern , ** axes_lengths ) layer = Reduce ( pattern , reduction , ** axes_lengths ) # apply created layer to a tensor / variable x = layer ( x ) Example of using layers within a model: # example given for pytorch, but code in other frameworks is almost identical from torch.nn import Sequential , Conv2d , MaxPool2d , Linear , ReLU from einops.layers.torch import Rearrange model = Sequential ( Conv2d ( 3 , 6 , kernel_size = 5 ), MaxPool2d ( kernel_size = 2 ), Conv2d ( 6 , 16 , kernel_size = 5 ), MaxPool2d ( kernel_size = 2 ), # flattening Rearrange ( 'b c h w -> b (c h w)' ), Linear ( 16 * 5 * 5 , 120 ), ReLU (), Linear ( 120 , 10 ), )","title":"API"},{"location":"#naming","text":"einops stands for Einstein-Inspired Notation for operations (though \"Einstein operations\" is more attractive and easier to remember). Notation was loosely inspired by Einstein summation (in particular by numpy.einsum operation).","title":"Naming"},{"location":"#why-use-einops-notation","text":"","title":"Why use einops notation?!"},{"location":"#semantic-information-being-verbose-in-expectations","text":"y = x . view ( x . shape [ 0 ], - 1 ) y = rearrange ( x , 'b c h w -> b (c h w)' ) While these two lines are doing the same job in some context, the second one provides information about the input and output. In other words, einops focuses on interface: what is the input and output , not how the output is computed. The next operation looks similar: y = rearrange ( x , 'time c h w -> time (c h w)' ) but it gives the reader a hint: this is not an independent batch of images we are processing, but rather a sequence (video). Semantic information makes the code easier to read and maintain.","title":"Semantic information (being verbose in expectations)"},{"location":"#more-checks","text":"Reconsider the same example: y = x . view ( x . shape [ 0 ], - 1 ) # x: (batch, 256, 19, 19) y = rearrange ( x , 'b c h w -> b (c h w)' ) The second line checks that the input has four dimensions, but you can also specify particular dimensions. That's opposed to just writing comments about shapes since comments don't work and don't prevent mistakes as we know y = x . view ( x . shape [ 0 ], - 1 ) # x: (batch, 256, 19, 19) y = rearrange ( x , 'b c h w -> b (c h w)' , c = 256 , h = 19 , w = 19 )","title":"More checks"},{"location":"#result-is-strictly-determined","text":"Below we have at least two ways to define the depth-to-space operation # depth-to-space rearrange ( x , 'b c (h h2) (w w2) -> b (c h2 w2) h w' , h2 = 2 , w2 = 2 ) rearrange ( x , 'b c (h h2) (w w2) -> b (h2 w2 c) h w' , h2 = 2 , w2 = 2 ) There are at least four more ways to do it. Which one is used by the framework? These details are ignored, since usually it makes no difference, but it can make a big difference (e.g. if you use grouped convolutions in the next stage), and you'd like to specify this in your code.","title":"Result is strictly determined"},{"location":"#uniformity","text":"reduce ( x , 'b c (x dx) -> b c x' , 'max' , dx = 2 ) reduce ( x , 'b c (x dx) (y dy) -> b c x y' , 'max' , dx = 2 , dy = 3 ) reduce ( x , 'b c (x dx) (y dy) (z dz)-> b c x y z' , 'max' , dx = 2 , dy = 3 , dz = 4 ) These examples demonstrated that we don't use separate operations for 1d/2d/3d pooling, those are all defined in a uniform way. Space-to-depth and depth-to space are defined in many frameworks but how about width-to-height? rearrange ( x , 'b c h (w w2) -> b c (h w2) w' , w2 = 2 )","title":"Uniformity"},{"location":"#framework-independent-behavior","text":"Even simple functions are defined differently by different frameworks y = x . flatten () # or flatten(x) Suppose x 's shape was (3, 4, 5) , then y has shape ... - numpy, cupy, chainer, pytorch: (60,) - keras, tensorflow.layers, mxnet and gluon: (3, 20)","title":"Framework independent behavior"},{"location":"#independence-of-framework-terminology","text":"Example: tile vs repeat causes lots of confusion. To copy image along width: np . tile ( image , ( 1 , 2 )) # in numpy image . repeat ( 1 , 2 ) # pytorch's repeat ~ numpy's tile With einops you don't need to decipher which axis was repeated: repeat ( image , 'h w -> h (tile w)' , tile = 2 ) # in numpy repeat ( image , 'h w -> h (tile w)' , tile = 2 ) # in pytorch repeat ( image , 'h w -> h (tile w)' , tile = 2 ) # in tf repeat ( image , 'h w -> h (tile w)' , tile = 2 ) # in jax repeat ( image , 'h w -> h (tile w)' , tile = 2 ) # in mxnet ... ( etc . )","title":"Independence of framework terminology"},{"location":"#supported-frameworks","text":"Einops works with ... numpy pytorch tensorflow jax cupy chainer gluon tf.keras mxnet (experimental)","title":"Supported frameworks"},{"location":"#contributing","text":"Best ways to contribute are spread the word about einops if you like explaining things, alternative tutorials are very helpful translating examples in languages other than English is also a good idea use einops notation in your papers to strictly define used operations!","title":"Contributing"},{"location":"#supported-python-versions","text":"einops works with python 3.6 or later.","title":"Supported python versions"},{"location":"1-einops-basics/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); Einops tutorial, part 1: basics \u00b6 Welcome to einops-land! \u00b6 We don't write y = x . transpose ( 0 , 2 , 3 , 1 ) We write comprehensible code y = rearrange ( x , 'b c h w -> b h w c' ) einops supports widely used tensor packages (such as numpy , pytorch , chainer , gluon , tensorflow ), and extends them. What's in this tutorial? \u00b6 fundamentals: reordering, composition and decomposition of axes operations: rearrange , reduce , repeat how much you can do with a single operation! Preparations \u00b6 In [1]: # Examples are given for `numpy`. This code also setups ipython/jupyter # so that numpy arrays in the output are displayed as images import numpy from utils import display_np_arrays_as_images display_np_arrays_as_images () Load a batch of images to play with \u00b6 In [2]: ims = numpy . load ( './resources/test_images.npy' , allow_pickle = False ) # There are 6 images of shape 96x96 with 3 color channels packed into tensor print ( ims . shape , ims . dtype ) (6, 96, 96, 3) float64 In [3]: # display the first image (whole 4d tensor can't be rendered) ims [ 0 ] Out[3]: In [4]: # second image in a batch ims [ 1 ] Out[4]: In [5]: # we'll use three operations from einops import rearrange , reduce , repeat In [6]: # rearrange, as its name suggests, rearranges elements # below we swapped height and width. # In other words, transposed first two axes (dimensions) rearrange ( ims [ 0 ], 'h w c -> w h c' ) Out[6]: Composition of axes \u00b6 transposition is very common and useful, but let's move to other capabilities provided by einops In [7]: # einops allows seamlessly composing batch and height to a new height dimension # We just rendered all images by collapsing to 3d tensor! rearrange ( ims , 'b h w c -> (b h) w c' ) Out[7]: In [8]: # or compose a new dimension of batch and width rearrange ( ims , 'b h w c -> h (b w) c' ) Out[8]: In [9]: # resulting dimensions are computed very simply # length of newly composed axis is a product of components # [6, 96, 96, 3] -> [96, (6 * 96), 3] rearrange ( ims , 'b h w c -> h (b w) c' ) . shape Out[9]: (96, 576, 3) In [10]: # we can compose more than two axes. # let's flatten 4d array into 1d, resulting array has as many elements as the original rearrange ( ims , 'b h w c -> (b h w c)' ) . shape Out[10]: (165888,) Decomposition of axis \u00b6 In [11]: # decomposition is the inverse process - represent an axis as a combination of new axes # several decompositions possible, so b1=2 is to decompose 6 to b1=2 and b2=3 rearrange ( ims , '(b1 b2) h w c -> b1 b2 h w c ' , b1 = 2 ) . shape Out[11]: (2, 3, 96, 96, 3) In [12]: # finally, combine composition and decomposition: rearrange ( ims , '(b1 b2) h w c -> (b1 h) (b2 w) c ' , b1 = 2 ) Out[12]: In [13]: # slightly different composition: b1 is merged with width, b2 with height # ... so letters are ordered by w then by h rearrange ( ims , '(b1 b2) h w c -> (b2 h) (b1 w) c ' , b1 = 2 ) Out[13]: In [14]: # move part of width dimension to height. # we should call this width-to-height as image width shrinked by 2 and height doubled. # but all pixels are the same! # Can you write reverse operation (height-to-width)? rearrange ( ims , 'b h (w w2) c -> (h w2) (b w) c' , w2 = 2 ) Out[14]: Order of axes matters \u00b6 In [15]: # compare with the next example rearrange ( ims , 'b h w c -> h (b w) c' ) Out[15]: In [16]: # order of axes in composition is different # rule is just as for digits in the number: leftmost digit is the most significant, # while neighboring numbers differ in the rightmost axis. # you can also think of this as lexicographic sort rearrange ( ims , 'b h w c -> h (w b) c' ) Out[16]: In [17]: # what if b1 and b2 are reordered before composing to width? rearrange ( ims , '(b1 b2) h w c -> h (b1 b2 w) c ' , b1 = 2 ) # produces 'einops' rearrange ( ims , '(b1 b2) h w c -> h (b2 b1 w) c ' , b1 = 2 ) # produces 'eoipns' Out[17]: Meet einops.reduce \u00b6 In einops-land you don't need to guess what happened x . mean ( - 1 ) Because you write what the operation does reduce ( x , 'b h w c -> b h w' , 'mean' ) if axis is not present in the output \u2014 you guessed it \u2014 axis was reduced. In [18]: # average over batch reduce ( ims , 'b h w c -> h w c' , 'mean' ) Out[18]: In [19]: # the previous is identical to familiar: ims . mean ( axis = 0 ) # but is so much more readable Out[19]: In [20]: # Example of reducing of several axes # besides mean, there are also min, max, sum, prod reduce ( ims , 'b h w c -> h w' , 'min' ) Out[20]: In [21]: # this is mean-pooling with 2x2 kernel # image is split into 2x2 patches, each patch is averaged reduce ( ims , 'b (h h2) (w w2) c -> h (b w) c' , 'mean' , h2 = 2 , w2 = 2 ) Out[21]: In [22]: # max-pooling is similar # result is not as smmoth as for mean-pooling reduce ( ims , 'b (h h2) (w w2) c -> h (b w) c' , 'max' , h2 = 2 , w2 = 2 ) Out[22]: In [23]: # yet another example. Can you compute result shape? reduce ( ims , '(b1 b2) h w c -> (b2 h) (b1 w)' , 'mean' , b1 = 2 ) Out[23]: Stack and concatenate \u00b6 In [24]: # rearrange can also take care of lists of arrays with the same shape x = list ( ims ) print ( type ( x ), 'with' , len ( x ), 'tensors of shape' , x [ 0 ] . shape ) # that's how we can stack inputs # \"list axis\" becomes first (\"b\" in this case), and we left it there rearrange ( x , 'b h w c -> b h w c' ) . shape <class 'list'> with 6 tensors of shape (96, 96, 3) Out[24]: (6, 96, 96, 3) In [25]: # but new axis can appear in the other place: rearrange ( x , 'b h w c -> h w c b' ) . shape Out[25]: (96, 96, 3, 6) In [26]: # that's equivalent to numpy stacking numpy . array_equal ( rearrange ( x , 'b h w c -> h w c b' ), numpy . stack ( x , axis = 3 )) Out[26]: True In [27]: # ... or we can concatenate rearrange ( x , 'b h w c -> h (b w) c' ) . shape # numpy.stack(x, axis=3)) Out[27]: (96, 576, 3) In [28]: # which is behavior of concatenation numpy . array_equal ( rearrange ( x , 'b h w c -> h (b w) c' ), numpy . concatenate ( x , axis = 1 )) Out[28]: True Addition or removal of axes \u00b6 You can write 1 to create new axis of length 1. Similarly you can remove such axis. There is also a synonym () that you can use. That's a composition of zero axes and it also has a unit length. In [29]: x = rearrange ( ims , 'b h w c -> b 1 h w 1 c' ) # functionality of numpy.expand_dims print ( x . shape ) print ( rearrange ( x , 'b 1 h w 1 c -> b h w c' ) . shape ) # functionality of numpy.squeeze (6, 1, 96, 96, 1, 3) (6, 96, 96, 3) Reduce \u21c6 repeat \u00b6 reduce and repeat are like opposite of each other: first one reduces amount of elements, second one increases In [30]: # compute max in each image individually, then show a difference x = reduce ( ims , 'b h w c -> b () () c' , 'max' ) - ims rearrange ( x , 'b h w c -> h (b w) c' ) Out[30]: Fancy examples in random order \u00b6 (a.k.a. mad designer gallery) In [31]: # interweaving pixels of different pictures # all letters are observable rearrange ( ims , '(b1 b2) h w c -> (h b1) (w b2) c ' , b1 = 2 ) Out[31]: In [32]: # interweaving along vertical for couples of images rearrange ( ims , '(b1 b2) h w c -> (h b1) (b2 w) c' , b1 = 2 ) Out[32]: In [33]: # interweaving lines for couples of images # exercise: achieve the same result without einops in your favourite framework reduce ( ims , '(b1 b2) h w c -> h (b2 w) c' , 'max' , b1 = 2 ) Out[33]: In [34]: # color can be also composed into dimension # ... while image is downsampled reduce ( ims , 'b (h 2) (w 2) c -> (c h) (b w)' , 'mean' ) Out[34]: In [35]: # disproportionate resize reduce ( ims , 'b (h 4) (w 3) c -> (h) (b w)' , 'mean' ) Out[35]: In [36]: # spilt each image in two halves, compute mean of the two reduce ( ims , 'b (h1 h2) w c -> h2 (b w)' , 'mean' , h1 = 2 ) Out[36]: In [37]: # split in small patches and transpose each patch rearrange ( ims , 'b (h1 h2) (w1 w2) c -> (h1 w2) (b w1 h2) c' , h2 = 8 , w2 = 8 ) Out[37]: In [38]: # stop me someone! rearrange ( ims , 'b (h1 h2 h3) (w1 w2 w3) c -> (h1 w2 h3) (b w1 h2 w3) c' , h2 = 2 , w2 = 2 , w3 = 2 , h3 = 2 ) Out[38]: In [39]: rearrange ( ims , '(b1 b2) (h1 h2) (w1 w2) c -> (h1 b1 h2) (w1 b2 w2) c' , h1 = 3 , w1 = 3 , b2 = 3 ) Out[39]: In [40]: # patterns can be arbitrarily complicated reduce ( ims , '(b1 b2) (h1 h2 h3) (w1 w2 w3) c -> (h1 w1 h3) (b1 w2 h2 w3 b2) c' , 'mean' , h2 = 2 , w1 = 2 , w3 = 2 , h3 = 2 , b2 = 2 ) Out[40]: In [41]: # subtract background in each image individually and normalize # pay attention to () - this is composition of 0 axis, a dummy axis with 1 element. im2 = reduce ( ims , 'b h w c -> b () () c' , 'max' ) - ims im2 /= reduce ( im2 , 'b h w c -> b () () c' , 'max' ) rearrange ( im2 , 'b h w c -> h (b w) c' ) Out[41]: In [42]: # pixelate: first downscale by averaging, then upscale back using the same pattern averaged = reduce ( ims , 'b (h h2) (w w2) c -> b h w c' , 'mean' , h2 = 6 , w2 = 8 ) repeat ( averaged , 'b h w c -> (h h2) (b w w2) c' , h2 = 6 , w2 = 8 ) Out[42]: In [43]: rearrange ( ims , 'b h w c -> w (b h) c' ) Out[43]: In [44]: # let's bring color dimension as part of horizontal axis # at the same time horizonal axis is downsampled by 2x reduce ( ims , 'b (h h2) (w w2) c -> (h w2) (b w c)' , 'mean' , h2 = 3 , w2 = 3 ) Out[44]: Ok, numpy is fun, but how do I use einops with some other framework? \u00b6 If that's what you've done with ims being numpy array: rearrange ( ims , 'b h w c -> w (b h) c' ) That's how you adapt the code for other frameworks: # pytorch: rearrange ( ims , 'b h w c -> w (b h) c' ) # tensorflow: rearrange ( ims , 'b h w c -> w (b h) c' ) # chainer: rearrange ( ims , 'b h w c -> w (b h) c' ) # gluon: rearrange ( ims , 'b h w c -> w (b h) c' ) # cupy: rearrange ( ims , 'b h w c -> w (b h) c' ) # jax: rearrange ( ims , 'b h w c -> w (b h) c' ) ... well , you got the idea . Einops allows backpropagation as if all operations were native to framework. Operations do not change when moving to another framework Summary \u00b6 rearrange doesn't change number of elements and covers different numpy functions (like transpose , reshape , stack , concatenate , squeeze and expand_dims ) reduce combines same reordering syntax with reductions ( mean , min , max , sum , prod , and any others) repeat additionally covers repeating and tiling composition and decomposition of axes are a corner stone, they can and should be used together Second part of tutorial shows how einops works with other frameworks Third part of tutorial shows how to improve your DL code with einops","title":"Einops Basics"},{"location":"1-einops-basics/#einops-tutorial-part-1-basics","text":"","title":"Einops tutorial, part 1: basics"},{"location":"1-einops-basics/#welcome-to-einops-land","text":"We don't write y = x . transpose ( 0 , 2 , 3 , 1 ) We write comprehensible code y = rearrange ( x , 'b c h w -> b h w c' ) einops supports widely used tensor packages (such as numpy , pytorch , chainer , gluon , tensorflow ), and extends them.","title":"Welcome to einops-land!"},{"location":"1-einops-basics/#whats-in-this-tutorial","text":"fundamentals: reordering, composition and decomposition of axes operations: rearrange , reduce , repeat how much you can do with a single operation!","title":"What's in this tutorial?"},{"location":"1-einops-basics/#preparations","text":"In [1]: # Examples are given for `numpy`. This code also setups ipython/jupyter # so that numpy arrays in the output are displayed as images import numpy from utils import display_np_arrays_as_images display_np_arrays_as_images ()","title":"Preparations"},{"location":"1-einops-basics/#load-a-batch-of-images-to-play-with","text":"In [2]: ims = numpy . load ( './resources/test_images.npy' , allow_pickle = False ) # There are 6 images of shape 96x96 with 3 color channels packed into tensor print ( ims . shape , ims . dtype ) (6, 96, 96, 3) float64 In [3]: # display the first image (whole 4d tensor can't be rendered) ims [ 0 ] Out[3]: In [4]: # second image in a batch ims [ 1 ] Out[4]: In [5]: # we'll use three operations from einops import rearrange , reduce , repeat In [6]: # rearrange, as its name suggests, rearranges elements # below we swapped height and width. # In other words, transposed first two axes (dimensions) rearrange ( ims [ 0 ], 'h w c -> w h c' ) Out[6]:","title":"Load a batch of images to play with"},{"location":"1-einops-basics/#composition-of-axes","text":"transposition is very common and useful, but let's move to other capabilities provided by einops In [7]: # einops allows seamlessly composing batch and height to a new height dimension # We just rendered all images by collapsing to 3d tensor! rearrange ( ims , 'b h w c -> (b h) w c' ) Out[7]: In [8]: # or compose a new dimension of batch and width rearrange ( ims , 'b h w c -> h (b w) c' ) Out[8]: In [9]: # resulting dimensions are computed very simply # length of newly composed axis is a product of components # [6, 96, 96, 3] -> [96, (6 * 96), 3] rearrange ( ims , 'b h w c -> h (b w) c' ) . shape Out[9]: (96, 576, 3) In [10]: # we can compose more than two axes. # let's flatten 4d array into 1d, resulting array has as many elements as the original rearrange ( ims , 'b h w c -> (b h w c)' ) . shape Out[10]: (165888,)","title":"Composition of axes"},{"location":"1-einops-basics/#decomposition-of-axis","text":"In [11]: # decomposition is the inverse process - represent an axis as a combination of new axes # several decompositions possible, so b1=2 is to decompose 6 to b1=2 and b2=3 rearrange ( ims , '(b1 b2) h w c -> b1 b2 h w c ' , b1 = 2 ) . shape Out[11]: (2, 3, 96, 96, 3) In [12]: # finally, combine composition and decomposition: rearrange ( ims , '(b1 b2) h w c -> (b1 h) (b2 w) c ' , b1 = 2 ) Out[12]: In [13]: # slightly different composition: b1 is merged with width, b2 with height # ... so letters are ordered by w then by h rearrange ( ims , '(b1 b2) h w c -> (b2 h) (b1 w) c ' , b1 = 2 ) Out[13]: In [14]: # move part of width dimension to height. # we should call this width-to-height as image width shrinked by 2 and height doubled. # but all pixels are the same! # Can you write reverse operation (height-to-width)? rearrange ( ims , 'b h (w w2) c -> (h w2) (b w) c' , w2 = 2 ) Out[14]:","title":"Decomposition of axis"},{"location":"1-einops-basics/#order-of-axes-matters","text":"In [15]: # compare with the next example rearrange ( ims , 'b h w c -> h (b w) c' ) Out[15]: In [16]: # order of axes in composition is different # rule is just as for digits in the number: leftmost digit is the most significant, # while neighboring numbers differ in the rightmost axis. # you can also think of this as lexicographic sort rearrange ( ims , 'b h w c -> h (w b) c' ) Out[16]: In [17]: # what if b1 and b2 are reordered before composing to width? rearrange ( ims , '(b1 b2) h w c -> h (b1 b2 w) c ' , b1 = 2 ) # produces 'einops' rearrange ( ims , '(b1 b2) h w c -> h (b2 b1 w) c ' , b1 = 2 ) # produces 'eoipns' Out[17]:","title":"Order of axes matters"},{"location":"1-einops-basics/#meet-einopsreduce","text":"In einops-land you don't need to guess what happened x . mean ( - 1 ) Because you write what the operation does reduce ( x , 'b h w c -> b h w' , 'mean' ) if axis is not present in the output \u2014 you guessed it \u2014 axis was reduced. In [18]: # average over batch reduce ( ims , 'b h w c -> h w c' , 'mean' ) Out[18]: In [19]: # the previous is identical to familiar: ims . mean ( axis = 0 ) # but is so much more readable Out[19]: In [20]: # Example of reducing of several axes # besides mean, there are also min, max, sum, prod reduce ( ims , 'b h w c -> h w' , 'min' ) Out[20]: In [21]: # this is mean-pooling with 2x2 kernel # image is split into 2x2 patches, each patch is averaged reduce ( ims , 'b (h h2) (w w2) c -> h (b w) c' , 'mean' , h2 = 2 , w2 = 2 ) Out[21]: In [22]: # max-pooling is similar # result is not as smmoth as for mean-pooling reduce ( ims , 'b (h h2) (w w2) c -> h (b w) c' , 'max' , h2 = 2 , w2 = 2 ) Out[22]: In [23]: # yet another example. Can you compute result shape? reduce ( ims , '(b1 b2) h w c -> (b2 h) (b1 w)' , 'mean' , b1 = 2 ) Out[23]:","title":"Meet einops.reduce"},{"location":"1-einops-basics/#stack-and-concatenate","text":"In [24]: # rearrange can also take care of lists of arrays with the same shape x = list ( ims ) print ( type ( x ), 'with' , len ( x ), 'tensors of shape' , x [ 0 ] . shape ) # that's how we can stack inputs # \"list axis\" becomes first (\"b\" in this case), and we left it there rearrange ( x , 'b h w c -> b h w c' ) . shape <class 'list'> with 6 tensors of shape (96, 96, 3) Out[24]: (6, 96, 96, 3) In [25]: # but new axis can appear in the other place: rearrange ( x , 'b h w c -> h w c b' ) . shape Out[25]: (96, 96, 3, 6) In [26]: # that's equivalent to numpy stacking numpy . array_equal ( rearrange ( x , 'b h w c -> h w c b' ), numpy . stack ( x , axis = 3 )) Out[26]: True In [27]: # ... or we can concatenate rearrange ( x , 'b h w c -> h (b w) c' ) . shape # numpy.stack(x, axis=3)) Out[27]: (96, 576, 3) In [28]: # which is behavior of concatenation numpy . array_equal ( rearrange ( x , 'b h w c -> h (b w) c' ), numpy . concatenate ( x , axis = 1 )) Out[28]: True","title":"Stack and concatenate"},{"location":"1-einops-basics/#addition-or-removal-of-axes","text":"You can write 1 to create new axis of length 1. Similarly you can remove such axis. There is also a synonym () that you can use. That's a composition of zero axes and it also has a unit length. In [29]: x = rearrange ( ims , 'b h w c -> b 1 h w 1 c' ) # functionality of numpy.expand_dims print ( x . shape ) print ( rearrange ( x , 'b 1 h w 1 c -> b h w c' ) . shape ) # functionality of numpy.squeeze (6, 1, 96, 96, 1, 3) (6, 96, 96, 3)","title":"Addition or removal of axes"},{"location":"1-einops-basics/#reduce-repeat","text":"reduce and repeat are like opposite of each other: first one reduces amount of elements, second one increases In [30]: # compute max in each image individually, then show a difference x = reduce ( ims , 'b h w c -> b () () c' , 'max' ) - ims rearrange ( x , 'b h w c -> h (b w) c' ) Out[30]:","title":"Reduce \u21c6 repeat"},{"location":"1-einops-basics/#fancy-examples-in-random-order","text":"(a.k.a. mad designer gallery) In [31]: # interweaving pixels of different pictures # all letters are observable rearrange ( ims , '(b1 b2) h w c -> (h b1) (w b2) c ' , b1 = 2 ) Out[31]: In [32]: # interweaving along vertical for couples of images rearrange ( ims , '(b1 b2) h w c -> (h b1) (b2 w) c' , b1 = 2 ) Out[32]: In [33]: # interweaving lines for couples of images # exercise: achieve the same result without einops in your favourite framework reduce ( ims , '(b1 b2) h w c -> h (b2 w) c' , 'max' , b1 = 2 ) Out[33]: In [34]: # color can be also composed into dimension # ... while image is downsampled reduce ( ims , 'b (h 2) (w 2) c -> (c h) (b w)' , 'mean' ) Out[34]: In [35]: # disproportionate resize reduce ( ims , 'b (h 4) (w 3) c -> (h) (b w)' , 'mean' ) Out[35]: In [36]: # spilt each image in two halves, compute mean of the two reduce ( ims , 'b (h1 h2) w c -> h2 (b w)' , 'mean' , h1 = 2 ) Out[36]: In [37]: # split in small patches and transpose each patch rearrange ( ims , 'b (h1 h2) (w1 w2) c -> (h1 w2) (b w1 h2) c' , h2 = 8 , w2 = 8 ) Out[37]: In [38]: # stop me someone! rearrange ( ims , 'b (h1 h2 h3) (w1 w2 w3) c -> (h1 w2 h3) (b w1 h2 w3) c' , h2 = 2 , w2 = 2 , w3 = 2 , h3 = 2 ) Out[38]: In [39]: rearrange ( ims , '(b1 b2) (h1 h2) (w1 w2) c -> (h1 b1 h2) (w1 b2 w2) c' , h1 = 3 , w1 = 3 , b2 = 3 ) Out[39]: In [40]: # patterns can be arbitrarily complicated reduce ( ims , '(b1 b2) (h1 h2 h3) (w1 w2 w3) c -> (h1 w1 h3) (b1 w2 h2 w3 b2) c' , 'mean' , h2 = 2 , w1 = 2 , w3 = 2 , h3 = 2 , b2 = 2 ) Out[40]: In [41]: # subtract background in each image individually and normalize # pay attention to () - this is composition of 0 axis, a dummy axis with 1 element. im2 = reduce ( ims , 'b h w c -> b () () c' , 'max' ) - ims im2 /= reduce ( im2 , 'b h w c -> b () () c' , 'max' ) rearrange ( im2 , 'b h w c -> h (b w) c' ) Out[41]: In [42]: # pixelate: first downscale by averaging, then upscale back using the same pattern averaged = reduce ( ims , 'b (h h2) (w w2) c -> b h w c' , 'mean' , h2 = 6 , w2 = 8 ) repeat ( averaged , 'b h w c -> (h h2) (b w w2) c' , h2 = 6 , w2 = 8 ) Out[42]: In [43]: rearrange ( ims , 'b h w c -> w (b h) c' ) Out[43]: In [44]: # let's bring color dimension as part of horizontal axis # at the same time horizonal axis is downsampled by 2x reduce ( ims , 'b (h h2) (w w2) c -> (h w2) (b w c)' , 'mean' , h2 = 3 , w2 = 3 ) Out[44]:","title":"Fancy examples in random order"},{"location":"1-einops-basics/#ok-numpy-is-fun-but-how-do-i-use-einops-with-some-other-framework","text":"If that's what you've done with ims being numpy array: rearrange ( ims , 'b h w c -> w (b h) c' ) That's how you adapt the code for other frameworks: # pytorch: rearrange ( ims , 'b h w c -> w (b h) c' ) # tensorflow: rearrange ( ims , 'b h w c -> w (b h) c' ) # chainer: rearrange ( ims , 'b h w c -> w (b h) c' ) # gluon: rearrange ( ims , 'b h w c -> w (b h) c' ) # cupy: rearrange ( ims , 'b h w c -> w (b h) c' ) # jax: rearrange ( ims , 'b h w c -> w (b h) c' ) ... well , you got the idea . Einops allows backpropagation as if all operations were native to framework. Operations do not change when moving to another framework","title":"Ok, numpy is fun, but how do I use einops with some other framework?"},{"location":"1-einops-basics/#summary","text":"rearrange doesn't change number of elements and covers different numpy functions (like transpose , reshape , stack , concatenate , squeeze and expand_dims ) reduce combines same reordering syntax with reductions ( mean , min , max , sum , prod , and any others) repeat additionally covers repeating and tiling composition and decomposition of axes are a corner stone, they can and should be used together Second part of tutorial shows how einops works with other frameworks Third part of tutorial shows how to improve your DL code with einops","title":"Summary"},{"location":"2-einops-for-deep-learning/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); Einops tutorial, part 2: deep learning \u00b6 Previous part of tutorial provides visual examples with numpy. What's in this tutorial? \u00b6 working with deep learning packages important cases for deep learning models einsops.asnumpy and einops.layers In [1]: from einops import rearrange , reduce In [2]: import numpy as np x = np . random . RandomState ( 42 ) . normal ( size = [ 10 , 32 , 100 , 200 ]) In [3]: # utility to hide answers from utils import guess Select your flavour \u00b6 Switch to the framework you're most comfortable with. In [4]: # select one from 'chainer', 'gluon', 'tensorflow', 'pytorch' flavour = 'pytorch' In [5]: print ( 'selected {} backend' . format ( flavour )) if flavour == 'tensorflow' : import tensorflow as tf tape = tf . GradientTape ( persistent = True ) tape . __enter__ () x = tf . Variable ( x ) + 0 elif flavour == 'pytorch' : import torch x = torch . from_numpy ( x ) x . requires_grad = True elif flavour == 'chainer' : import chainer x = chainer . Variable ( x ) else : assert flavour == 'gluon' import mxnet as mx mx . autograd . set_recording ( True ) x = mx . nd . array ( x , dtype = x . dtype ) x . attach_grad () selected pytorch backend In [6]: type ( x ), x . shape Out[6]: (torch.Tensor, torch.Size([10, 32, 100, 200])) Simple computations \u00b6 converting bchw to bhwc format and back is a common operation in CV try to predict output shape and then check your guess! In [7]: y = rearrange ( x , 'b c h w -> b h w c' ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 100, 200, 32) (hover to see) Worked! \u00b6 Did you notice? Code above worked for you backend of choice. Einops functions work with any tensor like they are native to the framework. Backpropagation \u00b6 gradients are a corner stone of deep learning You can back-propagate through einops operations (just as with framework native operations) In [8]: y0 = x y1 = reduce ( y0 , 'b c h w -> b c' , 'max' ) y2 = rearrange ( y1 , 'b c -> c b' ) y3 = reduce ( y2 , 'c b -> ' , 'sum' ) if flavour == 'tensorflow' : print ( reduce ( tape . gradient ( y3 , x ), 'b c h w -> ' , 'sum' )) else : y3 . backward () print ( reduce ( x . grad , 'b c h w -> ' , 'sum' )) tensor(320., dtype=torch.float64) Meet einops.asnumpy \u00b6 Just converts tensors to numpy (and pulls from gpu if necessary) In [9]: from einops import asnumpy y3_numpy = asnumpy ( y3 ) print ( type ( y3_numpy )) <class 'numpy.ndarray'> Common building blocks of deep learning \u00b6 Let's check how some familiar operations can be written with einops Flattening is common operation, frequently appears at the boundary between convolutional layers and fully connected layers In [10]: y = rearrange ( x , 'b c h w -> b (c h w)' ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 640000) (hover to see) space-to-depth In [11]: y = rearrange ( x , 'b c (h h1) (w w1) -> b (h1 w1 c) h w' , h1 = 2 , w1 = 2 ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 128, 50, 100) (hover to see) depth-to-space (notice that it's reverse of the previous) In [12]: y = rearrange ( x , 'b (c h1 w1) h w -> b c (h h1) (w w1)' , h1 = 2 , w1 = 2 ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 8, 200, 400) (hover to see) Reductions \u00b6 Simple global average pooling . In [13]: y = reduce ( x , 'b c h w -> b c' , reduction = 'mean' ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 32) (hover to see) max-pooling with a kernel 2x2 In [14]: y = reduce ( x , 'b c (h h1) (w w1) -> b c h w' , reduction = 'max' , h1 = 2 , w1 = 2 ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 32, 50, 100) (hover to see) In [15]: # you can skip names for reduced axes y = reduce ( x , 'b c (h 2) (w 2) -> b c h w' , reduction = 'max' ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 32, 50, 100) (hover to see) 1d, 2d and 3d pooling are defined in a similar way \u00b6 for sequential 1-d models, you'll probably want pooling over time reduce ( x , '(t 2) b c -> t b c' , reduction = 'max' ) for volumetric models, all three dimensions are pooled reduce ( x , 'b c (x 2) (y 2) (z 2) -> b c x y z' , reduction = 'max' ) Uniformity is a strong point of einops , and you don't need specific operation for each particular case. Good exercises \u00b6 write a version of space-to-depth for 1d and 3d (2d is provided above) write an average / max pooling for 1d models. Squeeze and unsqueeze (expand_dims) \u00b6 In [16]: # models typically work only with batches, # so to predict a single image ... image = rearrange ( x [ 0 , : 3 ], 'c h w -> h w c' ) # ... create a dummy 1-element axis ... y = rearrange ( image , 'h w c -> () c h w' ) # ... imagine you predicted this with a convolutional network for classification, # we'll just flatten axes ... predictions = rearrange ( y , 'b c h w -> b (c h w)' ) # ... finally, decompose (remove) dummy axis predictions = rearrange ( predictions , '() classes -> classes' ) keepdims-like behavior for reductions \u00b6 empty composition () provides dimensions of length 1, which are broadcastable. alternatively, you can use just 1 to introduce new axis, that's a synonym to () per-channel mean-normalization for each image : In [17]: y = x - reduce ( x , 'b c h w -> b c 1 1' , 'mean' ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 32, 100, 200) (hover to see) per-channel mean-normalization for whole batch : In [18]: y = x - reduce ( y , 'b c h w -> 1 c 1 1' , 'mean' ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 32, 100, 200) (hover to see) Stacking \u00b6 let's take a list of tensors In [19]: list_of_tensors = list ( x ) New axis (one that enumerates tensors) appears first on the left side of expression. Just as if you were indexing list - first you'd get tensor by index In [20]: tensors = rearrange ( list_of_tensors , 'b c h w -> b h w c' ) guess ( tensors . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 100, 200, 32) (hover to see) In [21]: # or maybe stack along last dimension? tensors = rearrange ( list_of_tensors , 'b c h w -> h w c b' ) guess ( tensors . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (100, 200, 32, 10) (hover to see) Concatenation \u00b6 concatenate over the first dimension? In [22]: tensors = rearrange ( list_of_tensors , 'b c h w -> (b h) w c' ) guess ( tensors . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (1000, 200, 32) (hover to see) or maybe concatenate along last dimension? In [23]: tensors = rearrange ( list_of_tensors , 'b c h w -> h w (b c)' ) guess ( tensors . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (100, 200, 320) (hover to see) Shuffling within a dimension \u00b6 channel shuffle (as it is drawn in shufflenet paper) In [24]: y = rearrange ( x , 'b (g1 g2 c) h w-> b (g2 g1 c) h w' , g1 = 4 , g2 = 4 ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 32, 100, 200) (hover to see) simpler version of channel shuffle In [25]: y = rearrange ( x , 'b (g c) h w-> b (c g) h w' , g = 4 ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 32, 100, 200) (hover to see) Split a dimension \u00b6 Here's a super-convenient trick. Example: when a network predicts several bboxes for each position Assume we got 8 bboxes, 4 coordinates each. To get coordinated into 4 separate variables, you move corresponding dimension to front and unpack tuple. In [26]: bbox_x , bbox_y , bbox_w , bbox_h = \\ rearrange ( x , 'b (coord bbox) h w -> coord b bbox h w' , coord = 4 , bbox = 8 ) # now you can operate on individual variables max_bbox_area = reduce ( bbox_w * bbox_h , 'b bbox h w -> b h w' , 'max' ) guess ( bbox_x . shape ) guess ( max_bbox_area . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 8, 100, 200) (hover to see) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; } Answer is: (10, 100, 200) (hover to see) Getting into the weeds of tensor packing \u00b6 you can skip this part - it explains why taking a habit of defining splits and packs explicitly when implementing custom gated activation (like GLU), split is needed: y1 , y2 = rearrange ( x , 'b (split c) h w -> split b c h w' , split = 2 ) result = y2 * sigmoid ( y2 ) # or tanh ... but we could split differently y1 , y2 = rearrange ( x , 'b (c split) h w -> split b c h w' , split = 2 ) first one splits channels into consequent groups: y1 = x[:, :x.shape[1] // 2, :, :] while second takes channels with a step: y1 = x[:, 0::2, :, :] This may drive to very surprising results when input is a result of group convolution a result of bidirectional LSTM/RNN multi-head attention Let's focus on the second case (LSTM/RNN), since it is less obvious. For instance, cudnn concatenates LSTM outputs for forward-in-time and backward-in-time Also in pytorch GLU splits channels into consequent groups (first way) So when LSTM's output comes to GLU, forward-in-time produces linear part, and backward-in-time produces activation ... and role of directions is different, and gradients coming to two parts are different that's not what you expect from simple GLU(BLSTM(x)) , right? einops notation makes such inconsistencies explicit and easy-detectable Shape parsing \u00b6 just a handy utility In [27]: from einops import parse_shape In [28]: def convolve_2d ( x ): # imagine we have a simple 2d convolution with padding, # so output has same shape as input. # Sorry for laziness, use imagination! return x In [29]: # imagine we are working with 3d data x_5d = rearrange ( x , 'b c x (y z) -> b c x y z' , z = 20 ) # but we have only 2d convolutions. # That's not a problem, since we can apply y = rearrange ( x_5d , 'b c x y z -> (b z) c x y' ) y = convolve_2d ( y ) # not just specifies additional information, but verifies that all dimensions match y = rearrange ( y , '(b z) c x y -> b c x y z' , ** parse_shape ( x_5d , 'b c x y z' )) In [30]: parse_shape ( x_5d , 'b c x y z' ) Out[30]: {'b': 10, 'c': 32, 'x': 100, 'y': 10, 'z': 20} In [31]: # we can skip some dimensions by writing underscore parse_shape ( x_5d , 'batch c _ _ _' ) Out[31]: {'batch': 10, 'c': 32} Striding anything \u00b6 Finally, how to convert any operation into a strided operation? (like convolution with strides, aka dilated/atrous convolution) In [32]: # each image is split into subgrids, each is now a separate \"image\" y = rearrange ( x , 'b c (h hs) (w ws) -> (hs ws b) c h w' , hs = 2 , ws = 2 ) y = convolve_2d ( y ) # pack subgrids back to an image y = rearrange ( y , '(hs ws b) c h w -> b c (h hs) (w ws)' , hs = 2 , ws = 2 ) assert y . shape == x . shape Layers \u00b6 For frameworks that prefer operating with layers, layers are available. You'll need to import a proper one depending on your backend: from einops.layers.chainer import Rearrange , Reduce from einops.layers.gluon import Rearrange , Reduce from einops.layers.keras import Rearrange , Reduce from einops.layers.torch import Rearrange , Reduce Einops layers are identical to operations, and have same parameters. (for the exception of first argument, which should be passed during call) layer = Rearrange ( pattern , ** axes_lengths ) layer = Reduce ( pattern , reduction , ** axes_lengths ) # apply layer to tensor x = layer ( x ) Usually it is more convenient to use layers, not operations, to build models # example given for pytorch, but code in other frameworks is almost identical from torch.nn import Sequential , Conv2d , MaxPool2d , Linear , ReLU from einops.layers.torch import Reduce model = Sequential ( Conv2d ( 3 , 6 , kernel_size = 5 ), MaxPool2d ( kernel_size = 2 ), Conv2d ( 6 , 16 , kernel_size = 5 ), # combined pooling and flattening in a single step Reduce ( 'b c (h 2) (w 2) -> b (c h w)' , 'max' ), Linear ( 16 * 5 * 5 , 120 ), ReLU (), Linear ( 120 , 10 ), ) What's now? \u00b6 rush through writing better code with einops+pytorch Use different framework? Not a big issue, most recommendations transfer well to other frameworks. einops works the same way in any framework. Finally - just write your code with einops!","title":"Einops for Deep Learning"},{"location":"2-einops-for-deep-learning/#einops-tutorial-part-2-deep-learning","text":"Previous part of tutorial provides visual examples with numpy.","title":"Einops tutorial, part 2: deep learning"},{"location":"2-einops-for-deep-learning/#whats-in-this-tutorial","text":"working with deep learning packages important cases for deep learning models einsops.asnumpy and einops.layers In [1]: from einops import rearrange , reduce In [2]: import numpy as np x = np . random . RandomState ( 42 ) . normal ( size = [ 10 , 32 , 100 , 200 ]) In [3]: # utility to hide answers from utils import guess","title":"What's in this tutorial?"},{"location":"2-einops-for-deep-learning/#select-your-flavour","text":"Switch to the framework you're most comfortable with. In [4]: # select one from 'chainer', 'gluon', 'tensorflow', 'pytorch' flavour = 'pytorch' In [5]: print ( 'selected {} backend' . format ( flavour )) if flavour == 'tensorflow' : import tensorflow as tf tape = tf . GradientTape ( persistent = True ) tape . __enter__ () x = tf . Variable ( x ) + 0 elif flavour == 'pytorch' : import torch x = torch . from_numpy ( x ) x . requires_grad = True elif flavour == 'chainer' : import chainer x = chainer . Variable ( x ) else : assert flavour == 'gluon' import mxnet as mx mx . autograd . set_recording ( True ) x = mx . nd . array ( x , dtype = x . dtype ) x . attach_grad () selected pytorch backend In [6]: type ( x ), x . shape Out[6]: (torch.Tensor, torch.Size([10, 32, 100, 200]))","title":"Select your flavour"},{"location":"2-einops-for-deep-learning/#simple-computations","text":"converting bchw to bhwc format and back is a common operation in CV try to predict output shape and then check your guess! In [7]: y = rearrange ( x , 'b c h w -> b h w c' ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; }","title":"Simple computations"},{"location":"2-einops-for-deep-learning/#worked","text":"Did you notice? Code above worked for you backend of choice. Einops functions work with any tensor like they are native to the framework.","title":"Worked!"},{"location":"2-einops-for-deep-learning/#backpropagation","text":"gradients are a corner stone of deep learning You can back-propagate through einops operations (just as with framework native operations) In [8]: y0 = x y1 = reduce ( y0 , 'b c h w -> b c' , 'max' ) y2 = rearrange ( y1 , 'b c -> c b' ) y3 = reduce ( y2 , 'c b -> ' , 'sum' ) if flavour == 'tensorflow' : print ( reduce ( tape . gradient ( y3 , x ), 'b c h w -> ' , 'sum' )) else : y3 . backward () print ( reduce ( x . grad , 'b c h w -> ' , 'sum' )) tensor(320., dtype=torch.float64)","title":"Backpropagation"},{"location":"2-einops-for-deep-learning/#meet-einopsasnumpy","text":"Just converts tensors to numpy (and pulls from gpu if necessary) In [9]: from einops import asnumpy y3_numpy = asnumpy ( y3 ) print ( type ( y3_numpy )) <class 'numpy.ndarray'>","title":"Meet einops.asnumpy"},{"location":"2-einops-for-deep-learning/#common-building-blocks-of-deep-learning","text":"Let's check how some familiar operations can be written with einops Flattening is common operation, frequently appears at the boundary between convolutional layers and fully connected layers In [10]: y = rearrange ( x , 'b c h w -> b (c h w)' ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; }","title":"Common building blocks of deep learning"},{"location":"2-einops-for-deep-learning/#reductions","text":"Simple global average pooling . In [13]: y = reduce ( x , 'b c h w -> b c' , reduction = 'mean' ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; }","title":"Reductions"},{"location":"2-einops-for-deep-learning/#1d-2d-and-3d-pooling-are-defined-in-a-similar-way","text":"for sequential 1-d models, you'll probably want pooling over time reduce ( x , '(t 2) b c -> t b c' , reduction = 'max' ) for volumetric models, all three dimensions are pooled reduce ( x , 'b c (x 2) (y 2) (z 2) -> b c x y z' , reduction = 'max' ) Uniformity is a strong point of einops , and you don't need specific operation for each particular case.","title":"1d, 2d and 3d pooling are defined in a similar way"},{"location":"2-einops-for-deep-learning/#good-exercises","text":"write a version of space-to-depth for 1d and 3d (2d is provided above) write an average / max pooling for 1d models.","title":"Good exercises"},{"location":"2-einops-for-deep-learning/#squeeze-and-unsqueeze-expand_dims","text":"In [16]: # models typically work only with batches, # so to predict a single image ... image = rearrange ( x [ 0 , : 3 ], 'c h w -> h w c' ) # ... create a dummy 1-element axis ... y = rearrange ( image , 'h w c -> () c h w' ) # ... imagine you predicted this with a convolutional network for classification, # we'll just flatten axes ... predictions = rearrange ( y , 'b c h w -> b (c h w)' ) # ... finally, decompose (remove) dummy axis predictions = rearrange ( predictions , '() classes -> classes' )","title":"Squeeze and unsqueeze (expand_dims)"},{"location":"2-einops-for-deep-learning/#keepdims-like-behavior-for-reductions","text":"empty composition () provides dimensions of length 1, which are broadcastable. alternatively, you can use just 1 to introduce new axis, that's a synonym to () per-channel mean-normalization for each image : In [17]: y = x - reduce ( x , 'b c h w -> b c 1 1' , 'mean' ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; }","title":"keepdims-like behavior for reductions"},{"location":"2-einops-for-deep-learning/#stacking","text":"let's take a list of tensors In [19]: list_of_tensors = list ( x ) New axis (one that enumerates tensors) appears first on the left side of expression. Just as if you were indexing list - first you'd get tensor by index In [20]: tensors = rearrange ( list_of_tensors , 'b c h w -> b h w c' ) guess ( tensors . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; }","title":"Stacking"},{"location":"2-einops-for-deep-learning/#concatenation","text":"concatenate over the first dimension? In [22]: tensors = rearrange ( list_of_tensors , 'b c h w -> (b h) w c' ) guess ( tensors . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; }","title":"Concatenation"},{"location":"2-einops-for-deep-learning/#shuffling-within-a-dimension","text":"channel shuffle (as it is drawn in shufflenet paper) In [24]: y = rearrange ( x , 'b (g1 g2 c) h w-> b (g2 g1 c) h w' , g1 = 4 , g2 = 4 ) guess ( y . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; }","title":"Shuffling within a dimension"},{"location":"2-einops-for-deep-learning/#split-a-dimension","text":"Here's a super-convenient trick. Example: when a network predicts several bboxes for each position Assume we got 8 bboxes, 4 coordinates each. To get coordinated into 4 separate variables, you move corresponding dimension to front and unpack tuple. In [26]: bbox_x , bbox_y , bbox_w , bbox_h = \\ rearrange ( x , 'b (coord bbox) h w -> coord b bbox h w' , coord = 4 , bbox = 8 ) # now you can operate on individual variables max_bbox_area = reduce ( bbox_w * bbox_h , 'b bbox h w -> b h w' , 'max' ) guess ( bbox_x . shape ) guess ( max_bbox_area . shape ) .einops-answer { color: transparent; padding: 5px 15px; background-color: #def; } .einops-answer:hover { color: blue; }","title":"Split a dimension"},{"location":"2-einops-for-deep-learning/#getting-into-the-weeds-of-tensor-packing","text":"you can skip this part - it explains why taking a habit of defining splits and packs explicitly when implementing custom gated activation (like GLU), split is needed: y1 , y2 = rearrange ( x , 'b (split c) h w -> split b c h w' , split = 2 ) result = y2 * sigmoid ( y2 ) # or tanh ... but we could split differently y1 , y2 = rearrange ( x , 'b (c split) h w -> split b c h w' , split = 2 ) first one splits channels into consequent groups: y1 = x[:, :x.shape[1] // 2, :, :] while second takes channels with a step: y1 = x[:, 0::2, :, :] This may drive to very surprising results when input is a result of group convolution a result of bidirectional LSTM/RNN multi-head attention Let's focus on the second case (LSTM/RNN), since it is less obvious. For instance, cudnn concatenates LSTM outputs for forward-in-time and backward-in-time Also in pytorch GLU splits channels into consequent groups (first way) So when LSTM's output comes to GLU, forward-in-time produces linear part, and backward-in-time produces activation ... and role of directions is different, and gradients coming to two parts are different that's not what you expect from simple GLU(BLSTM(x)) , right? einops notation makes such inconsistencies explicit and easy-detectable","title":"Getting into the weeds of tensor packing"},{"location":"2-einops-for-deep-learning/#shape-parsing","text":"just a handy utility In [27]: from einops import parse_shape In [28]: def convolve_2d ( x ): # imagine we have a simple 2d convolution with padding, # so output has same shape as input. # Sorry for laziness, use imagination! return x In [29]: # imagine we are working with 3d data x_5d = rearrange ( x , 'b c x (y z) -> b c x y z' , z = 20 ) # but we have only 2d convolutions. # That's not a problem, since we can apply y = rearrange ( x_5d , 'b c x y z -> (b z) c x y' ) y = convolve_2d ( y ) # not just specifies additional information, but verifies that all dimensions match y = rearrange ( y , '(b z) c x y -> b c x y z' , ** parse_shape ( x_5d , 'b c x y z' )) In [30]: parse_shape ( x_5d , 'b c x y z' ) Out[30]: {'b': 10, 'c': 32, 'x': 100, 'y': 10, 'z': 20} In [31]: # we can skip some dimensions by writing underscore parse_shape ( x_5d , 'batch c _ _ _' ) Out[31]: {'batch': 10, 'c': 32}","title":"Shape parsing"},{"location":"2-einops-for-deep-learning/#striding-anything","text":"Finally, how to convert any operation into a strided operation? (like convolution with strides, aka dilated/atrous convolution) In [32]: # each image is split into subgrids, each is now a separate \"image\" y = rearrange ( x , 'b c (h hs) (w ws) -> (hs ws b) c h w' , hs = 2 , ws = 2 ) y = convolve_2d ( y ) # pack subgrids back to an image y = rearrange ( y , '(hs ws b) c h w -> b c (h hs) (w ws)' , hs = 2 , ws = 2 ) assert y . shape == x . shape","title":"Striding anything"},{"location":"2-einops-for-deep-learning/#layers","text":"For frameworks that prefer operating with layers, layers are available. You'll need to import a proper one depending on your backend: from einops.layers.chainer import Rearrange , Reduce from einops.layers.gluon import Rearrange , Reduce from einops.layers.keras import Rearrange , Reduce from einops.layers.torch import Rearrange , Reduce Einops layers are identical to operations, and have same parameters. (for the exception of first argument, which should be passed during call) layer = Rearrange ( pattern , ** axes_lengths ) layer = Reduce ( pattern , reduction , ** axes_lengths ) # apply layer to tensor x = layer ( x ) Usually it is more convenient to use layers, not operations, to build models # example given for pytorch, but code in other frameworks is almost identical from torch.nn import Sequential , Conv2d , MaxPool2d , Linear , ReLU from einops.layers.torch import Reduce model = Sequential ( Conv2d ( 3 , 6 , kernel_size = 5 ), MaxPool2d ( kernel_size = 2 ), Conv2d ( 6 , 16 , kernel_size = 5 ), # combined pooling and flattening in a single step Reduce ( 'b c (h 2) (w 2) -> b (c h w)' , 'max' ), Linear ( 16 * 5 * 5 , 120 ), ReLU (), Linear ( 120 , 10 ), )","title":"Layers"},{"location":"2-einops-for-deep-learning/#whats-now","text":"rush through writing better code with einops+pytorch Use different framework? Not a big issue, most recommendations transfer well to other frameworks. einops works the same way in any framework. Finally - just write your code with einops!","title":"What's now?"},{"location":"api/asnumpy/","text":"einops.asnumpy Convert a tensor of an imperative framework (i.e. numpy/cupy/torch/gluon/etc.) to numpy.ndarray Parameters: Name Type Description Default tensor tensor of any of known imperative framework required Returns: Type Description numpy.ndarray , converted to numpy Source code in einops/einops.py def asnumpy ( tensor ): \"\"\" Convert a tensor of an imperative framework (i.e. numpy/cupy/torch/gluon/etc.) to `numpy.ndarray` Parameters: tensor: tensor of any of known imperative framework Returns: `numpy.ndarray`, converted to numpy \"\"\" return get_backend ( tensor ) . to_numpy ( tensor )","title":"asnumpy"},{"location":"api/asnumpy/#einopsasnumpy","text":"","title":"einops.asnumpy"},{"location":"api/asnumpy/#einops.einops.asnumpy","text":"Convert a tensor of an imperative framework (i.e. numpy/cupy/torch/gluon/etc.) to numpy.ndarray Parameters: Name Type Description Default tensor tensor of any of known imperative framework required Returns: Type Description numpy.ndarray , converted to numpy Source code in einops/einops.py def asnumpy ( tensor ): \"\"\" Convert a tensor of an imperative framework (i.e. numpy/cupy/torch/gluon/etc.) to `numpy.ndarray` Parameters: tensor: tensor of any of known imperative framework Returns: `numpy.ndarray`, converted to numpy \"\"\" return get_backend ( tensor ) . to_numpy ( tensor )","title":"einops.einops.asnumpy"},{"location":"api/parse_shape/","text":"einops.parse_shape Parse a tensor shape to dictionary mapping axes names to their lengths. # Use underscore to skip the dimension in parsing. >>> x = np . zeros ([ 2 , 3 , 5 , 7 ]) >>> parse_shape ( x , 'batch _ h w' ) { 'batch' : 2 , 'h' : 5 , 'w' : 7 } # `parse_shape` output can be used to specify axes_lengths for other operations: >>> y = np . zeros ([ 700 ]) >>> rearrange ( y , '(b c h w) -> b c h w' , ** parse_shape ( x , 'b _ h w' )) . shape ( 2 , 10 , 5 , 7 ) For symbolic frameworks may return symbols, not integers. Parameters: Name Type Description Default x tensor of any of supported frameworks required pattern str str, space separated names for axes, underscore means skip axis required Returns: Type Description dict, maps axes names to their lengths Source code in einops/einops.py def parse_shape ( x , pattern : str ): \"\"\" Parse a tensor shape to dictionary mapping axes names to their lengths. ```python # Use underscore to skip the dimension in parsing. >>> x = np.zeros([2, 3, 5, 7]) >>> parse_shape(x, 'batch _ h w') {'batch': 2, 'h': 5, 'w': 7} # `parse_shape` output can be used to specify axes_lengths for other operations: >>> y = np.zeros([700]) >>> rearrange(y, '(b c h w) -> b c h w', **parse_shape(x, 'b _ h w')).shape (2, 10, 5, 7) ``` For symbolic frameworks may return symbols, not integers. Parameters: x: tensor of any of supported frameworks pattern: str, space separated names for axes, underscore means skip axis Returns: dict, maps axes names to their lengths \"\"\" names = [ elementary_axis for elementary_axis in pattern . split ( ' ' ) if len ( elementary_axis ) > 0 ] shape = get_backend ( x ) . shape ( x ) if len ( shape ) != len ( names ): raise RuntimeError ( \"Can't parse shape with different number of dimensions: {pattern} {shape} \" . format ( pattern = pattern , shape = shape )) result = {} for axis_name , axis_length in zip ( names , shape ): if axis_name != '_' : result [ axis_name ] = axis_length return result","title":"parse_shape"},{"location":"api/parse_shape/#einopsparse_shape","text":"","title":"einops.parse_shape"},{"location":"api/parse_shape/#einops.einops.parse_shape","text":"Parse a tensor shape to dictionary mapping axes names to their lengths. # Use underscore to skip the dimension in parsing. >>> x = np . zeros ([ 2 , 3 , 5 , 7 ]) >>> parse_shape ( x , 'batch _ h w' ) { 'batch' : 2 , 'h' : 5 , 'w' : 7 } # `parse_shape` output can be used to specify axes_lengths for other operations: >>> y = np . zeros ([ 700 ]) >>> rearrange ( y , '(b c h w) -> b c h w' , ** parse_shape ( x , 'b _ h w' )) . shape ( 2 , 10 , 5 , 7 ) For symbolic frameworks may return symbols, not integers. Parameters: Name Type Description Default x tensor of any of supported frameworks required pattern str str, space separated names for axes, underscore means skip axis required Returns: Type Description dict, maps axes names to their lengths Source code in einops/einops.py def parse_shape ( x , pattern : str ): \"\"\" Parse a tensor shape to dictionary mapping axes names to their lengths. ```python # Use underscore to skip the dimension in parsing. >>> x = np.zeros([2, 3, 5, 7]) >>> parse_shape(x, 'batch _ h w') {'batch': 2, 'h': 5, 'w': 7} # `parse_shape` output can be used to specify axes_lengths for other operations: >>> y = np.zeros([700]) >>> rearrange(y, '(b c h w) -> b c h w', **parse_shape(x, 'b _ h w')).shape (2, 10, 5, 7) ``` For symbolic frameworks may return symbols, not integers. Parameters: x: tensor of any of supported frameworks pattern: str, space separated names for axes, underscore means skip axis Returns: dict, maps axes names to their lengths \"\"\" names = [ elementary_axis for elementary_axis in pattern . split ( ' ' ) if len ( elementary_axis ) > 0 ] shape = get_backend ( x ) . shape ( x ) if len ( shape ) != len ( names ): raise RuntimeError ( \"Can't parse shape with different number of dimensions: {pattern} {shape} \" . format ( pattern = pattern , shape = shape )) result = {} for axis_name , axis_length in zip ( names , shape ): if axis_name != '_' : result [ axis_name ] = axis_length return result","title":"einops.einops.parse_shape"},{"location":"api/rearrange/","text":"einops.rearrange einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors. This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze, stack, concatenate and other operations. Examples for rearrange operation: # suppose we have a set of 32 images in \"h w c\" format (height-width-channel) >>> images = [ np . random . randn ( 30 , 40 , 3 ) for _ in range ( 32 )] # stack along first (batch) axis, output is a single array >>> rearrange ( images , 'b h w c -> b h w c' ) . shape ( 32 , 30 , 40 , 3 ) # concatenate images along height (vertical axis), 960 = 32 * 30 >>> rearrange ( images , 'b h w c -> (b h) w c' ) . shape ( 960 , 40 , 3 ) # concatenated images along horizontal axis, 1280 = 32 * 40 >>> rearrange ( images , 'b h w c -> h (b w) c' ) . shape ( 30 , 1280 , 3 ) # reordered axes to \"b c h w\" format for deep learning >>> rearrange ( images , 'b h w c -> b c h w' ) . shape ( 32 , 3 , 30 , 40 ) # flattened each image into a vector, 3600 = 30 * 40 * 3 >>> rearrange ( images , 'b h w c -> b (c h w)' ) . shape ( 32 , 3600 ) # split each image into 4 smaller (top-left, top-right, bottom-left, bottom-right), 128 = 32 * 2 * 2 >>> rearrange ( images , 'b (h1 h) (w1 w) c -> (b h1 w1) h w c' , h1 = 2 , w1 = 2 ) . shape ( 128 , 15 , 20 , 3 ) # space-to-depth operation >>> rearrange ( images , 'b (h h1) (w w1) c -> b h w (c h1 w1)' , h1 = 2 , w1 = 2 ) . shape ( 32 , 15 , 20 , 12 ) When composing axes, C-order enumeration used (consecutive elements have different last axis) Find more examples in einops tutorial. Parameters: Name Type Description Default tensor tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray). list of tensors is also accepted, those should be of the same type and shape required pattern str string, rearrangement pattern required axes_lengths any additional specifications for dimensions {} Returns: Type Description tensor of the same type as input. If possible, a view to the original tensor is returned. Source code in einops/einops.py def rearrange ( tensor , pattern : str , ** axes_lengths ): \"\"\" einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors. This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze, stack, concatenate and other operations. Examples for rearrange operation: ```python # suppose we have a set of 32 images in \"h w c\" format (height-width-channel) >>> images = [np.random.randn(30, 40, 3) for _ in range(32)] # stack along first (batch) axis, output is a single array >>> rearrange(images, 'b h w c -> b h w c').shape (32, 30, 40, 3) # concatenate images along height (vertical axis), 960 = 32 * 30 >>> rearrange(images, 'b h w c -> (b h) w c').shape (960, 40, 3) # concatenated images along horizontal axis, 1280 = 32 * 40 >>> rearrange(images, 'b h w c -> h (b w) c').shape (30, 1280, 3) # reordered axes to \"b c h w\" format for deep learning >>> rearrange(images, 'b h w c -> b c h w').shape (32, 3, 30, 40) # flattened each image into a vector, 3600 = 30 * 40 * 3 >>> rearrange(images, 'b h w c -> b (c h w)').shape (32, 3600) # split each image into 4 smaller (top-left, top-right, bottom-left, bottom-right), 128 = 32 * 2 * 2 >>> rearrange(images, 'b (h1 h) (w1 w) c -> (b h1 w1) h w c', h1=2, w1=2).shape (128, 15, 20, 3) # space-to-depth operation >>> rearrange(images, 'b (h h1) (w w1) c -> b h w (c h1 w1)', h1=2, w1=2).shape (32, 15, 20, 12) ``` When composing axes, C-order enumeration used (consecutive elements have different last axis) Find more examples in einops tutorial. Parameters: tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray). list of tensors is also accepted, those should be of the same type and shape pattern: string, rearrangement pattern axes_lengths: any additional specifications for dimensions Returns: tensor of the same type as input. If possible, a view to the original tensor is returned. \"\"\" if isinstance ( tensor , list ): if len ( tensor ) == 0 : raise TypeError ( \"Rearrange can't be applied to an empty list\" ) tensor = get_backend ( tensor [ 0 ]) . stack_on_zeroth_dimension ( tensor ) return reduce ( tensor , pattern , reduction = 'rearrange' , ** axes_lengths )","title":"rearrange"},{"location":"api/rearrange/#einopsrearrange","text":"","title":"einops.rearrange"},{"location":"api/rearrange/#einops.einops.rearrange","text":"einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors. This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze, stack, concatenate and other operations. Examples for rearrange operation: # suppose we have a set of 32 images in \"h w c\" format (height-width-channel) >>> images = [ np . random . randn ( 30 , 40 , 3 ) for _ in range ( 32 )] # stack along first (batch) axis, output is a single array >>> rearrange ( images , 'b h w c -> b h w c' ) . shape ( 32 , 30 , 40 , 3 ) # concatenate images along height (vertical axis), 960 = 32 * 30 >>> rearrange ( images , 'b h w c -> (b h) w c' ) . shape ( 960 , 40 , 3 ) # concatenated images along horizontal axis, 1280 = 32 * 40 >>> rearrange ( images , 'b h w c -> h (b w) c' ) . shape ( 30 , 1280 , 3 ) # reordered axes to \"b c h w\" format for deep learning >>> rearrange ( images , 'b h w c -> b c h w' ) . shape ( 32 , 3 , 30 , 40 ) # flattened each image into a vector, 3600 = 30 * 40 * 3 >>> rearrange ( images , 'b h w c -> b (c h w)' ) . shape ( 32 , 3600 ) # split each image into 4 smaller (top-left, top-right, bottom-left, bottom-right), 128 = 32 * 2 * 2 >>> rearrange ( images , 'b (h1 h) (w1 w) c -> (b h1 w1) h w c' , h1 = 2 , w1 = 2 ) . shape ( 128 , 15 , 20 , 3 ) # space-to-depth operation >>> rearrange ( images , 'b (h h1) (w w1) c -> b h w (c h1 w1)' , h1 = 2 , w1 = 2 ) . shape ( 32 , 15 , 20 , 12 ) When composing axes, C-order enumeration used (consecutive elements have different last axis) Find more examples in einops tutorial. Parameters: Name Type Description Default tensor tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray). list of tensors is also accepted, those should be of the same type and shape required pattern str string, rearrangement pattern required axes_lengths any additional specifications for dimensions {} Returns: Type Description tensor of the same type as input. If possible, a view to the original tensor is returned. Source code in einops/einops.py def rearrange ( tensor , pattern : str , ** axes_lengths ): \"\"\" einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors. This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze, stack, concatenate and other operations. Examples for rearrange operation: ```python # suppose we have a set of 32 images in \"h w c\" format (height-width-channel) >>> images = [np.random.randn(30, 40, 3) for _ in range(32)] # stack along first (batch) axis, output is a single array >>> rearrange(images, 'b h w c -> b h w c').shape (32, 30, 40, 3) # concatenate images along height (vertical axis), 960 = 32 * 30 >>> rearrange(images, 'b h w c -> (b h) w c').shape (960, 40, 3) # concatenated images along horizontal axis, 1280 = 32 * 40 >>> rearrange(images, 'b h w c -> h (b w) c').shape (30, 1280, 3) # reordered axes to \"b c h w\" format for deep learning >>> rearrange(images, 'b h w c -> b c h w').shape (32, 3, 30, 40) # flattened each image into a vector, 3600 = 30 * 40 * 3 >>> rearrange(images, 'b h w c -> b (c h w)').shape (32, 3600) # split each image into 4 smaller (top-left, top-right, bottom-left, bottom-right), 128 = 32 * 2 * 2 >>> rearrange(images, 'b (h1 h) (w1 w) c -> (b h1 w1) h w c', h1=2, w1=2).shape (128, 15, 20, 3) # space-to-depth operation >>> rearrange(images, 'b (h h1) (w w1) c -> b h w (c h1 w1)', h1=2, w1=2).shape (32, 15, 20, 12) ``` When composing axes, C-order enumeration used (consecutive elements have different last axis) Find more examples in einops tutorial. Parameters: tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray). list of tensors is also accepted, those should be of the same type and shape pattern: string, rearrangement pattern axes_lengths: any additional specifications for dimensions Returns: tensor of the same type as input. If possible, a view to the original tensor is returned. \"\"\" if isinstance ( tensor , list ): if len ( tensor ) == 0 : raise TypeError ( \"Rearrange can't be applied to an empty list\" ) tensor = get_backend ( tensor [ 0 ]) . stack_on_zeroth_dimension ( tensor ) return reduce ( tensor , pattern , reduction = 'rearrange' , ** axes_lengths )","title":"einops.einops.rearrange"},{"location":"api/reduce/","text":"einops.reduce einops.reduce provides combination of reordering and reduction using reader-friendly notation. Examples for reduce operation: >>> x = np . random . randn ( 100 , 32 , 64 ) # perform max-reduction on the first axis >>> y = reduce ( x , 't b c -> b c' , 'max' ) # same as previous, but with clearer axes meaning >>> y = reduce ( x , 'time batch channel -> batch channel' , 'max' ) >>> x = np . random . randn ( 10 , 20 , 30 , 40 ) # 2d max-pooling with kernel size = 2 * 2 for image processing >>> y1 = reduce ( x , 'b c (h1 h2) (w1 w2) -> b c h1 w1' , 'max' , h2 = 2 , w2 = 2 ) # if one wants to go back to the original height and width, depth-to-space trick can be applied >>> y2 = rearrange ( y1 , 'b (c h2 w2) h1 w1 -> b c (h1 h2) (w1 w2)' , h2 = 2 , w2 = 2 ) >>> assert parse_shape ( x , 'b _ h w' ) == parse_shape ( y2 , 'b _ h w' ) # Adaptive 2d max-pooling to 3 * 4 grid >>> reduce ( x , 'b c (h1 h2) (w1 w2) -> b c h1 w1' , 'max' , h1 = 3 , w1 = 4 ) . shape ( 10 , 20 , 3 , 4 ) # Global average pooling >>> reduce ( x , 'b c h w -> b c' , 'mean' ) . shape ( 10 , 20 ) # Subtracting mean over batch for each channel >>> y = x - reduce ( x , 'b c h w -> () c () ()' , 'mean' ) # Subtracting per-image mean for each channel >>> y = x - reduce ( x , 'b c h w -> b c () ()' , 'mean' ) Parameters: Name Type Description Default tensor tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray). list of tensors is also accepted, those should be of the same type and shape required pattern str string, reduction pattern required reduction Union[str, Callable[[tensor, Tuple[int]], tensor]] one of available reductions ('min', 'max', 'sum', 'mean', 'prod'), case-sensitive alternatively, a callable f(tensor, reduced_axes) -> tensor can be provided. This allows using various reductions, examples: np.max, tf.reduce_logsumexp, torch.var, etc. required axes_lengths int any additional specifications for dimensions {} Returns: Type Description tensor of the same type as input Source code in einops/einops.py def reduce ( tensor , pattern : str , reduction : Reduction , ** axes_lengths : int ): \"\"\" einops.reduce provides combination of reordering and reduction using reader-friendly notation. Examples for reduce operation: ```python >>> x = np.random.randn(100, 32, 64) # perform max-reduction on the first axis >>> y = reduce(x, 't b c -> b c', 'max') # same as previous, but with clearer axes meaning >>> y = reduce(x, 'time batch channel -> batch channel', 'max') >>> x = np.random.randn(10, 20, 30, 40) # 2d max-pooling with kernel size = 2 * 2 for image processing >>> y1 = reduce(x, 'b c (h1 h2) (w1 w2) -> b c h1 w1', 'max', h2=2, w2=2) # if one wants to go back to the original height and width, depth-to-space trick can be applied >>> y2 = rearrange(y1, 'b (c h2 w2) h1 w1 -> b c (h1 h2) (w1 w2)', h2=2, w2=2) >>> assert parse_shape(x, 'b _ h w') == parse_shape(y2, 'b _ h w') # Adaptive 2d max-pooling to 3 * 4 grid >>> reduce(x, 'b c (h1 h2) (w1 w2) -> b c h1 w1', 'max', h1=3, w1=4).shape (10, 20, 3, 4) # Global average pooling >>> reduce(x, 'b c h w -> b c', 'mean').shape (10, 20) # Subtracting mean over batch for each channel >>> y = x - reduce(x, 'b c h w -> () c () ()', 'mean') # Subtracting per-image mean for each channel >>> y = x - reduce(x, 'b c h w -> b c () ()', 'mean') ``` Parameters: tensor: tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray). list of tensors is also accepted, those should be of the same type and shape pattern: string, reduction pattern reduction: one of available reductions ('min', 'max', 'sum', 'mean', 'prod'), case-sensitive alternatively, a callable f(tensor, reduced_axes) -> tensor can be provided. This allows using various reductions, examples: np.max, tf.reduce_logsumexp, torch.var, etc. axes_lengths: any additional specifications for dimensions Returns: tensor of the same type as input \"\"\" try : hashable_axes_lengths = tuple ( sorted ( axes_lengths . items ())) recipe = _prepare_transformation_recipe ( pattern , reduction , axes_lengths = hashable_axes_lengths ) return recipe . apply ( tensor ) except EinopsError as e : message = ' Error while processing {} -reduction pattern \" {} \".' . format ( reduction , pattern ) if not isinstance ( tensor , list ): message += ' \\n Input tensor shape: {} . ' . format ( get_backend ( tensor ) . shape ( tensor )) else : message += ' \\n Input is list. ' message += 'Additional info: {} .' . format ( axes_lengths ) raise EinopsError ( message + ' \\n {} ' . format ( e ))","title":"reduce"},{"location":"api/reduce/#einopsreduce","text":"","title":"einops.reduce"},{"location":"api/reduce/#einops.einops.reduce","text":"einops.reduce provides combination of reordering and reduction using reader-friendly notation. Examples for reduce operation: >>> x = np . random . randn ( 100 , 32 , 64 ) # perform max-reduction on the first axis >>> y = reduce ( x , 't b c -> b c' , 'max' ) # same as previous, but with clearer axes meaning >>> y = reduce ( x , 'time batch channel -> batch channel' , 'max' ) >>> x = np . random . randn ( 10 , 20 , 30 , 40 ) # 2d max-pooling with kernel size = 2 * 2 for image processing >>> y1 = reduce ( x , 'b c (h1 h2) (w1 w2) -> b c h1 w1' , 'max' , h2 = 2 , w2 = 2 ) # if one wants to go back to the original height and width, depth-to-space trick can be applied >>> y2 = rearrange ( y1 , 'b (c h2 w2) h1 w1 -> b c (h1 h2) (w1 w2)' , h2 = 2 , w2 = 2 ) >>> assert parse_shape ( x , 'b _ h w' ) == parse_shape ( y2 , 'b _ h w' ) # Adaptive 2d max-pooling to 3 * 4 grid >>> reduce ( x , 'b c (h1 h2) (w1 w2) -> b c h1 w1' , 'max' , h1 = 3 , w1 = 4 ) . shape ( 10 , 20 , 3 , 4 ) # Global average pooling >>> reduce ( x , 'b c h w -> b c' , 'mean' ) . shape ( 10 , 20 ) # Subtracting mean over batch for each channel >>> y = x - reduce ( x , 'b c h w -> () c () ()' , 'mean' ) # Subtracting per-image mean for each channel >>> y = x - reduce ( x , 'b c h w -> b c () ()' , 'mean' ) Parameters: Name Type Description Default tensor tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray). list of tensors is also accepted, those should be of the same type and shape required pattern str string, reduction pattern required reduction Union[str, Callable[[tensor, Tuple[int]], tensor]] one of available reductions ('min', 'max', 'sum', 'mean', 'prod'), case-sensitive alternatively, a callable f(tensor, reduced_axes) -> tensor can be provided. This allows using various reductions, examples: np.max, tf.reduce_logsumexp, torch.var, etc. required axes_lengths int any additional specifications for dimensions {} Returns: Type Description tensor of the same type as input Source code in einops/einops.py def reduce ( tensor , pattern : str , reduction : Reduction , ** axes_lengths : int ): \"\"\" einops.reduce provides combination of reordering and reduction using reader-friendly notation. Examples for reduce operation: ```python >>> x = np.random.randn(100, 32, 64) # perform max-reduction on the first axis >>> y = reduce(x, 't b c -> b c', 'max') # same as previous, but with clearer axes meaning >>> y = reduce(x, 'time batch channel -> batch channel', 'max') >>> x = np.random.randn(10, 20, 30, 40) # 2d max-pooling with kernel size = 2 * 2 for image processing >>> y1 = reduce(x, 'b c (h1 h2) (w1 w2) -> b c h1 w1', 'max', h2=2, w2=2) # if one wants to go back to the original height and width, depth-to-space trick can be applied >>> y2 = rearrange(y1, 'b (c h2 w2) h1 w1 -> b c (h1 h2) (w1 w2)', h2=2, w2=2) >>> assert parse_shape(x, 'b _ h w') == parse_shape(y2, 'b _ h w') # Adaptive 2d max-pooling to 3 * 4 grid >>> reduce(x, 'b c (h1 h2) (w1 w2) -> b c h1 w1', 'max', h1=3, w1=4).shape (10, 20, 3, 4) # Global average pooling >>> reduce(x, 'b c h w -> b c', 'mean').shape (10, 20) # Subtracting mean over batch for each channel >>> y = x - reduce(x, 'b c h w -> () c () ()', 'mean') # Subtracting per-image mean for each channel >>> y = x - reduce(x, 'b c h w -> b c () ()', 'mean') ``` Parameters: tensor: tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray). list of tensors is also accepted, those should be of the same type and shape pattern: string, reduction pattern reduction: one of available reductions ('min', 'max', 'sum', 'mean', 'prod'), case-sensitive alternatively, a callable f(tensor, reduced_axes) -> tensor can be provided. This allows using various reductions, examples: np.max, tf.reduce_logsumexp, torch.var, etc. axes_lengths: any additional specifications for dimensions Returns: tensor of the same type as input \"\"\" try : hashable_axes_lengths = tuple ( sorted ( axes_lengths . items ())) recipe = _prepare_transformation_recipe ( pattern , reduction , axes_lengths = hashable_axes_lengths ) return recipe . apply ( tensor ) except EinopsError as e : message = ' Error while processing {} -reduction pattern \" {} \".' . format ( reduction , pattern ) if not isinstance ( tensor , list ): message += ' \\n Input tensor shape: {} . ' . format ( get_backend ( tensor ) . shape ( tensor )) else : message += ' \\n Input is list. ' message += 'Additional info: {} .' . format ( axes_lengths ) raise EinopsError ( message + ' \\n {} ' . format ( e ))","title":"einops.einops.reduce"},{"location":"api/repeat/","text":"einops.repeat einops.repeat allows reordering elements and repeating them in arbitrary combinations. This operation includes functionality of repeat, tile, broadcast functions. Examples for repeat operation: # a grayscale image (of shape height x width) >>> image = np . random . randn ( 30 , 40 ) # change it to RGB format by repeating in each channel >>> repeat ( image , 'h w -> h w c' , c = 3 ) . shape ( 30 , 40 , 3 ) # repeat image 2 times along height (vertical axis) >>> repeat ( image , 'h w -> (repeat h) w' , repeat = 2 ) . shape ( 60 , 40 ) # repeat image 2 time along height and 3 times along width >>> repeat ( image , 'h w -> h (repeat w)' , repeat = 3 ) . shape ( 30 , 120 ) # convert each pixel to a small square 2x2. Upsample image by 2x >>> repeat ( image , 'h w -> (h h2) (w w2)' , h2 = 2 , w2 = 2 ) . shape ( 60 , 80 ) # pixelate image first by downsampling by 2x, then upsampling >>> downsampled = reduce ( image , '(h h2) (w w2) -> h w' , 'mean' , h2 = 2 , w2 = 2 ) >>> repeat ( downsampled , 'h w -> (h h2) (w w2)' , h2 = 2 , w2 = 2 ) . shape ( 30 , 40 ) When composing axes, C-order enumeration used (consecutive elements have different last axis) Find more examples in einops tutorial. Parameters: Name Type Description Default tensor tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray). list of tensors is also accepted, those should be of the same type and shape required pattern str string, rearrangement pattern required axes_lengths any additional specifications for dimensions {} Returns: Type Description Tensor of the same type as input. If possible, a view to the original tensor is returned. Source code in einops/einops.py def repeat ( tensor , pattern : str , ** axes_lengths ): \"\"\" einops.repeat allows reordering elements and repeating them in arbitrary combinations. This operation includes functionality of repeat, tile, broadcast functions. Examples for repeat operation: ```python # a grayscale image (of shape height x width) >>> image = np.random.randn(30, 40) # change it to RGB format by repeating in each channel >>> repeat(image, 'h w -> h w c', c=3).shape (30, 40, 3) # repeat image 2 times along height (vertical axis) >>> repeat(image, 'h w -> (repeat h) w', repeat=2).shape (60, 40) # repeat image 2 time along height and 3 times along width >>> repeat(image, 'h w -> h (repeat w)', repeat=3).shape (30, 120) # convert each pixel to a small square 2x2. Upsample image by 2x >>> repeat(image, 'h w -> (h h2) (w w2)', h2=2, w2=2).shape (60, 80) # pixelate image first by downsampling by 2x, then upsampling >>> downsampled = reduce(image, '(h h2) (w w2) -> h w', 'mean', h2=2, w2=2) >>> repeat(downsampled, 'h w -> (h h2) (w w2)', h2=2, w2=2).shape (30, 40) ``` When composing axes, C-order enumeration used (consecutive elements have different last axis) Find more examples in einops tutorial. Parameters: tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray). list of tensors is also accepted, those should be of the same type and shape pattern: string, rearrangement pattern axes_lengths: any additional specifications for dimensions Returns: Tensor of the same type as input. If possible, a view to the original tensor is returned. \"\"\" return reduce ( tensor , pattern , reduction = 'repeat' , ** axes_lengths )","title":"repeat"},{"location":"api/repeat/#einopsrepeat","text":"","title":"einops.repeat"},{"location":"api/repeat/#einops.einops.repeat","text":"einops.repeat allows reordering elements and repeating them in arbitrary combinations. This operation includes functionality of repeat, tile, broadcast functions. Examples for repeat operation: # a grayscale image (of shape height x width) >>> image = np . random . randn ( 30 , 40 ) # change it to RGB format by repeating in each channel >>> repeat ( image , 'h w -> h w c' , c = 3 ) . shape ( 30 , 40 , 3 ) # repeat image 2 times along height (vertical axis) >>> repeat ( image , 'h w -> (repeat h) w' , repeat = 2 ) . shape ( 60 , 40 ) # repeat image 2 time along height and 3 times along width >>> repeat ( image , 'h w -> h (repeat w)' , repeat = 3 ) . shape ( 30 , 120 ) # convert each pixel to a small square 2x2. Upsample image by 2x >>> repeat ( image , 'h w -> (h h2) (w w2)' , h2 = 2 , w2 = 2 ) . shape ( 60 , 80 ) # pixelate image first by downsampling by 2x, then upsampling >>> downsampled = reduce ( image , '(h h2) (w w2) -> h w' , 'mean' , h2 = 2 , w2 = 2 ) >>> repeat ( downsampled , 'h w -> (h h2) (w w2)' , h2 = 2 , w2 = 2 ) . shape ( 30 , 40 ) When composing axes, C-order enumeration used (consecutive elements have different last axis) Find more examples in einops tutorial. Parameters: Name Type Description Default tensor tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray). list of tensors is also accepted, those should be of the same type and shape required pattern str string, rearrangement pattern required axes_lengths any additional specifications for dimensions {} Returns: Type Description Tensor of the same type as input. If possible, a view to the original tensor is returned. Source code in einops/einops.py def repeat ( tensor , pattern : str , ** axes_lengths ): \"\"\" einops.repeat allows reordering elements and repeating them in arbitrary combinations. This operation includes functionality of repeat, tile, broadcast functions. Examples for repeat operation: ```python # a grayscale image (of shape height x width) >>> image = np.random.randn(30, 40) # change it to RGB format by repeating in each channel >>> repeat(image, 'h w -> h w c', c=3).shape (30, 40, 3) # repeat image 2 times along height (vertical axis) >>> repeat(image, 'h w -> (repeat h) w', repeat=2).shape (60, 40) # repeat image 2 time along height and 3 times along width >>> repeat(image, 'h w -> h (repeat w)', repeat=3).shape (30, 120) # convert each pixel to a small square 2x2. Upsample image by 2x >>> repeat(image, 'h w -> (h h2) (w w2)', h2=2, w2=2).shape (60, 80) # pixelate image first by downsampling by 2x, then upsampling >>> downsampled = reduce(image, '(h h2) (w w2) -> h w', 'mean', h2=2, w2=2) >>> repeat(downsampled, 'h w -> (h h2) (w w2)', h2=2, w2=2).shape (30, 40) ``` When composing axes, C-order enumeration used (consecutive elements have different last axis) Find more examples in einops tutorial. Parameters: tensor: tensor of any supported library (e.g. numpy.ndarray, tensorflow, pytorch, mxnet.ndarray). list of tensors is also accepted, those should be of the same type and shape pattern: string, rearrangement pattern axes_lengths: any additional specifications for dimensions Returns: Tensor of the same type as input. If possible, a view to the original tensor is returned. \"\"\" return reduce ( tensor , pattern , reduction = 'repeat' , ** axes_lengths )","title":"einops.einops.repeat"},{"location":"source_examples/Pytorch/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); [github] , tutorials [1] and [2] Writing a better code with pytorch and einops \u00b6 Rewriting building blocks of deep learning \u00b6 Now let's get to examples from real world. These code fragments taken from official tutorials and popular repositories. Learn how to improve code and how einops can help you. Left : as it was, Right : improved version In [1]: #right # start from importing some stuff import torch import torch.nn as nn import torch.nn.functional as F import numpy as np import math from einops import rearrange , reduce , asnumpy , parse_shape from einops.layers.torch import Rearrange , Reduce In [2]: def initialize ( model ): for p in model . parameters (): p . data [:] = torch . from_numpy ( np . random . RandomState ( sum ( p . shape )) . randn ( * p . shape )) return model Simple ConvNet \u00b6 In [3]: #left class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 1 , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , 10 ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , 320 ) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 ) conv_net_old = Net () In [4]: #right conv_net_new = nn . Sequential ( nn . Conv2d ( 1 , 10 , kernel_size = 5 ), nn . MaxPool2d ( kernel_size = 2 ), nn . ReLU (), nn . Conv2d ( 10 , 20 , kernel_size = 5 ), nn . MaxPool2d ( kernel_size = 2 ), nn . ReLU (), nn . Dropout2d (), Rearrange ( 'b c h w -> b (c h w)' ), nn . Linear ( 320 , 50 ), nn . ReLU (), nn . Dropout (), nn . Linear ( 50 , 10 ), nn . LogSoftmax ( dim = 1 ) ) Reasons to prefer new implementation: in the original code (to the left) if input size is changed and batch size is divisible by 16 (that's usualy so), we'll get something senseless after reshaping new code will explicitly raise an error in this case we won't forget to use dropout with flag self.training with new version code is straightforward to read and analyze sequential makes printing / saving / passing trivial. And there is no need in your code to load a model (which also has a number of benefits) don't need logsoftmax? Now you can use conv_net_new[:-1] . One more reason to prefer nn.Sequential ... and we could also add inplace for ReLU In [5]: conv_net_old ( torch . zeros ([ 16 , 1 , 20 , 20 ])) . shape # conv_net_new(torch.zeros([16, 1, 20, 20])).shape Out[5]: torch.Size([4, 10]) Super-resolution \u00b6 In [6]: #left class SuperResolutionNetOld ( nn . Module ): def __init__ ( self , upscale_factor ): super ( SuperResolutionNetOld , self ) . __init__ () self . relu = nn . ReLU () self . conv1 = nn . Conv2d ( 1 , 64 , ( 5 , 5 ), ( 1 , 1 ), ( 2 , 2 )) self . conv2 = nn . Conv2d ( 64 , 64 , ( 3 , 3 ), ( 1 , 1 ), ( 1 , 1 )) self . conv3 = nn . Conv2d ( 64 , 32 , ( 3 , 3 ), ( 1 , 1 ), ( 1 , 1 )) self . conv4 = nn . Conv2d ( 32 , upscale_factor ** 2 , ( 3 , 3 ), ( 1 , 1 ), ( 1 , 1 )) self . pixel_shuffle = nn . PixelShuffle ( upscale_factor ) def forward ( self , x ): x = self . relu ( self . conv1 ( x )) x = self . relu ( self . conv2 ( x )) x = self . relu ( self . conv3 ( x )) x = self . pixel_shuffle ( self . conv4 ( x )) return x In [7]: #right def SuperResolutionNetNew ( upscale_factor ): return nn . Sequential ( nn . Conv2d ( 1 , 64 , kernel_size = 5 , padding = 2 ), nn . ReLU ( inplace = True ), nn . Conv2d ( 64 , 64 , kernel_size = 3 , padding = 1 ), nn . ReLU ( inplace = True ), nn . Conv2d ( 64 , 32 , kernel_size = 3 , padding = 1 ), nn . ReLU ( inplace = True ), nn . Conv2d ( 32 , upscale_factor ** 2 , kernel_size = 3 , padding = 1 ), Rearrange ( 'b (h2 w2) h w -> b (h h2) (w w2)' , h2 = upscale_factor , w2 = upscale_factor ), ) Here is the difference: no need in special instruction pixel_shuffle (and result is transferrable between frameworks) output doesn't contain a fake axis (and we could do the same for the input) inplace ReLU used now, for high resolution pictures that becomes critical and saves us much memory and all the benefits of nn.Sequential again In [8]: model1 = initialize ( SuperResolutionNetOld ( upscale_factor = 3 )) model2 = initialize ( SuperResolutionNetNew ( upscale_factor = 3 )) In [9]: assert torch . allclose ( model1 ( torch . zeros ( 1 , 1 , 30 , 30 )), model2 ( torch . zeros ( 1 , 1 , 30 , 30 ))[ None ]) In [10]: ## that's how this code was mentioned to use # from PIL import Image # img = Image.open(opt.input_image).convert('YCbCr') # y, cb, cr = img.split() # model = torch.load(opt.model) # img_to_tensor = ToTensor() # input = img_to_tensor(y).view(1, -1, y.size[1], y.size[0]) # if opt.cuda: # model = model.cuda() # input = input.cuda() # out = model(input) # out = out.cpu() # out_img_y = out[0].detach().numpy() # out_img_y *= 255.0 # out_img_y = out_img_y.clip(0, 255) # out_img_y = Image.fromarray(np.uint8(out_img_y[0]), mode='L') # out_img_cb = cb.resize(out_img_y.size, Image.BICUBIC) # out_img_cr = cr.resize(out_img_y.size, Image.BICUBIC) # out_img = Image.merge('YCbCr', [out_img_y, out_img_cb, out_img_cr]).convert('RGB') ## Benefits # - no need to remembder the order of components in PIL.Image.size (as you see, it is actually different) # - code explicitly shows shapes passed in and out # - normalization to [0, 1] range and back is also explicit (it is needed to rememebed in original code that division by 255 is done by ToTensor) input_image = '../../logo/einops_logo_350x350.png' from PIL import Image import numpy as np from torchvision.transforms import ToTensor model = SuperResolutionNetOld ( upscale_factor = 2 ) img = Image . open ( input_image ) . convert ( 'YCbCr' ) y , cb , cr = img . split () img_to_tensor = ToTensor () input = img_to_tensor ( y ) . view ( 1 , - 1 , y . size [ 1 ], y . size [ 0 ]) out = model ( input ) out_img_y = out [ 0 ] . detach () . numpy () out_img_y = np . clip ( out_img_y [ 0 ] * 255 , 0 , 255 ) model = SuperResolutionNetNew ( upscale_factor = 2 ) img = Image . open ( input_image ) . convert ( 'YCbCr' ) y , cb , cr = img . split () # TODO numpy.asarray y = torch . from_numpy ( np . array ( y , dtype = 'float32' ) / 255 ) out = model ( rearrange ( y , 'h w -> () () h w' )) out_img_y = asnumpy ( rearrange ( out , '() h w -> h w' )) out_img_y = np . clip ( out_img_y * 255 , 0 , 255 ) Restyling Gram matrix for style transfer \u00b6 Original code is already good - first line shows what kind of input is expected einsum operation should be read like: for each batch and for each pair of channels, we sum over h and w. I've also changed normalization, because that's how Gram matrix is defined, otherwise we should call it normalized Gram matrix or alike In [11]: #left def gram_matrix_old ( y ): ( b , ch , h , w ) = y . size () features = y . view ( b , ch , w * h ) features_t = features . transpose ( 1 , 2 ) gram = features . bmm ( features_t ) / ( ch * h * w ) return gram In [12]: #right def gram_matrix_new ( y ): b , ch , h , w = y . shape return torch . einsum ( 'bchw,bdhw->bcd' , [ y , y ]) / ( h * w ) It would be great to use just 'b c1 h w,b c2 h w->b c1 c2' , but einsum supports only one-letter axes In [13]: x = torch . randn ([ 32 , 128 , 40 , 40 ]) In [14]: % timeit gram_matrix_old ( x ) . sum () % timeit gram_matrix_new ( x ) . sum () 7.58 ms \u00b1 492 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 9.66 ms \u00b1 258 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) In [15]: assert torch . allclose ( gram_matrix_old ( x ), gram_matrix_new ( x ) / 128 ) In [16]: # x = x.to('cuda') # %timeit -n100 gram_matrix_old(x).sum(); torch.cuda.synchronize() # %timeit -n100 gram_matrix_new(x).sum(); torch.cuda.synchronize() Recurrent model \u00b6 All we did here is just made information about shapes explicit to skip deciphering In [17]: #left class RNNModelOld ( nn . Module ): \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\" def __init__ ( self , ntoken , ninp , nhid , nlayers , dropout = 0.5 ): super ( RNNModel , self ) . __init__ () self . drop = nn . Dropout ( dropout ) self . encoder = nn . Embedding ( ntoken , ninp ) self . rnn = nn . LSTM ( ninp , nhid , nlayers , dropout = dropout ) self . decoder = nn . Linear ( nhid , ntoken ) def forward ( self , input , hidden ): emb = self . drop ( self . encoder ( input )) output , hidden = self . rnn ( emb , hidden ) output = self . drop ( output ) decoded = self . decoder ( output . view ( output . size ( 0 ) * output . size ( 1 ), output . size ( 2 ))) return decoded . view ( output . size ( 0 ), output . size ( 1 ), decoded . size ( 1 )), hidden In [18]: #right class RNNModelNew ( nn . Module ): \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\" def __init__ ( self , ntoken , ninp , nhid , nlayers , dropout = 0.5 ): super ( RNNModel , self ) . __init__ () self . drop = nn . Dropout ( p = dropout ) self . encoder = nn . Embedding ( ntoken , ninp ) self . rnn = nn . LSTM ( ninp , nhid , nlayers , dropout = dropout ) self . decoder = nn . Linear ( nhid , ntoken ) def forward ( self , input , hidden ): t , b = input . shape emb = self . drop ( self . encoder ( input )) output , hidden = self . rnn ( emb , hidden ) output = rearrange ( self . drop ( output ), 't b nhid -> (t b) nhid' ) decoded = rearrange ( self . decoder ( output ), '(t b) token -> t b token' , t = t , b = b ) return decoded , hidden Channel shuffle (from shufflenet) \u00b6 In [19]: #left def channel_shuffle_old ( x , groups ): batchsize , num_channels , height , width = x . data . size () channels_per_group = num_channels // groups # reshape x = x . view ( batchsize , groups , channels_per_group , height , width ) # transpose # - contiguous() required if transpose() is used before view(). # See https://github.com/pytorch/pytorch/issues/764 x = torch . transpose ( x , 1 , 2 ) . contiguous () # flatten x = x . view ( batchsize , - 1 , height , width ) return x In [20]: #right def channel_shuffle_new ( x , groups ): return rearrange ( x , 'b (c1 c2) h w -> b (c2 c1) h w' , c1 = groups ) While progress is obvious, this is not the limit. As you'll see below, we don't even need to write these couple of lines. In [21]: x = torch . zeros ([ 32 , 64 , 100 , 100 ]) In [22]: % timeit - n100 channel_shuffle_old ( x , 8 ); torch . cuda . synchronize () % timeit - n100 channel_shuffle_new ( x , 8 ); torch . cuda . synchronize () 51.2 ms \u00b1 2.18 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 49.9 ms \u00b1 594 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) Shufflenet \u00b6 In [23]: def conv3x3 ( in_channels , out_channels , stride = 1 , padding = 1 , bias = True , groups = 1 ): \"\"\"3x3 convolution with padding \"\"\" return nn . Conv2d ( in_channels , out_channels , kernel_size = 3 , stride = stride , padding = padding , bias = bias , groups = groups ) def conv1x1 ( in_channels , out_channels , groups = 1 ): \"\"\"1x1 convolution with padding - Normal pointwise convolution When groups == 1 - Grouped pointwise convolution when groups > 1 \"\"\" return nn . Conv2d ( in_channels , out_channels , kernel_size = 1 , groups = groups , stride = 1 ) In [24]: #left from collections import OrderedDict def channel_shuffle ( x , groups ): batchsize , num_channels , height , width = x . data . size () channels_per_group = num_channels // groups # reshape x = x . view ( batchsize , groups , channels_per_group , height , width ) # transpose # - contiguous() required if transpose() is used before view(). # See https://github.com/pytorch/pytorch/issues/764 x = torch . transpose ( x , 1 , 2 ) . contiguous () # flatten x = x . view ( batchsize , - 1 , height , width ) return x class ShuffleUnitOld ( nn . Module ): def __init__ ( self , in_channels , out_channels , groups = 3 , grouped_conv = True , combine = 'add' ): super ( ShuffleUnitOld , self ) . __init__ () self . in_channels = in_channels self . out_channels = out_channels self . grouped_conv = grouped_conv self . combine = combine self . groups = groups self . bottleneck_channels = self . out_channels // 4 # define the type of ShuffleUnit if self . combine == 'add' : # ShuffleUnit Figure 2b self . depthwise_stride = 1 self . _combine_func = self . _add elif self . combine == 'concat' : # ShuffleUnit Figure 2c self . depthwise_stride = 2 self . _combine_func = self . _concat # ensure output of concat has the same channels as # original output channels. self . out_channels -= self . in_channels else : raise ValueError ( \"Cannot combine tensors with \\\" {} \\\" \" \\ \"Only \\\" add \\\" and \\\" concat \\\" are\" \\ \"supported\" . format ( self . combine )) # Use a 1x1 grouped or non-grouped convolution to reduce input channels # to bottleneck channels, as in a ResNet bottleneck module. # NOTE: Do not use group convolution for the first conv1x1 in Stage 2. self . first_1x1_groups = self . groups if grouped_conv else 1 self . g_conv_1x1_compress = self . _make_grouped_conv1x1 ( self . in_channels , self . bottleneck_channels , self . first_1x1_groups , batch_norm = True , relu = True ) # 3x3 depthwise convolution followed by batch normalization self . depthwise_conv3x3 = conv3x3 ( self . bottleneck_channels , self . bottleneck_channels , stride = self . depthwise_stride , groups = self . bottleneck_channels ) self . bn_after_depthwise = nn . BatchNorm2d ( self . bottleneck_channels ) # Use 1x1 grouped convolution to expand from # bottleneck_channels to out_channels self . g_conv_1x1_expand = self . _make_grouped_conv1x1 ( self . bottleneck_channels , self . out_channels , self . groups , batch_norm = True , relu = False ) @staticmethod def _add ( x , out ): # residual connection return x + out @staticmethod def _concat ( x , out ): # concatenate along channel axis return torch . cat (( x , out ), 1 ) def _make_grouped_conv1x1 ( self , in_channels , out_channels , groups , batch_norm = True , relu = False ): modules = OrderedDict () conv = conv1x1 ( in_channels , out_channels , groups = groups ) modules [ 'conv1x1' ] = conv if batch_norm : modules [ 'batch_norm' ] = nn . BatchNorm2d ( out_channels ) if relu : modules [ 'relu' ] = nn . ReLU () if len ( modules ) > 1 : return nn . Sequential ( modules ) else : return conv def forward ( self , x ): # save for combining later with output residual = x if self . combine == 'concat' : residual = F . avg_pool2d ( residual , kernel_size = 3 , stride = 2 , padding = 1 ) out = self . g_conv_1x1_compress ( x ) out = channel_shuffle ( out , self . groups ) out = self . depthwise_conv3x3 ( out ) out = self . bn_after_depthwise ( out ) out = self . g_conv_1x1_expand ( out ) out = self . _combine_func ( residual , out ) return F . relu ( out ) In [25]: #right class ShuffleUnitNew ( nn . Module ): def __init__ ( self , in_channels , out_channels , groups = 3 , grouped_conv = True , combine = 'add' ): super () . __init__ () first_1x1_groups = groups if grouped_conv else 1 bottleneck_channels = out_channels // 4 self . combine = combine if combine == 'add' : # ShuffleUnit Figure 2b self . left = Rearrange ( '...->...' ) # identity depthwise_stride = 1 else : # ShuffleUnit Figure 2c self . left = nn . AvgPool2d ( kernel_size = 3 , stride = 2 , padding = 1 ) depthwise_stride = 2 # ensure output of concat has the same channels as original output channels. out_channels -= in_channels assert out_channels > 0 self . right = nn . Sequential ( # Use a 1x1 grouped or non-grouped convolution to reduce input channels # to bottleneck channels, as in a ResNet bottleneck module. conv1x1 ( in_channels , bottleneck_channels , groups = first_1x1_groups ), nn . BatchNorm2d ( bottleneck_channels ), nn . ReLU ( inplace = True ), # channel shuffle Rearrange ( 'b (c1 c2) h w -> b (c2 c1) h w' , c1 = groups ), # 3x3 depthwise convolution followed by batch conv3x3 ( bottleneck_channels , bottleneck_channels , stride = depthwise_stride , groups = bottleneck_channels ), nn . BatchNorm2d ( bottleneck_channels ), # Use 1x1 grouped convolution to expand from # bottleneck_channels to out_channels conv1x1 ( bottleneck_channels , out_channels , groups = groups ), nn . BatchNorm2d ( out_channels ), ) def forward ( self , x ): if self . combine == 'add' : combined = self . left ( x ) + self . right ( x ) else : combined = torch . cat ([ self . left ( x ), self . right ( x )], dim = 1 ) return F . relu ( combined , inplace = True ) Rewriting the code helped to identify: There is no sense in doing reshuffling and not using groups in the first convolution (indeed, in the paper it is not so). However, result is an equivalent model. It is also strange that the first convolution may be not grouped, while the last convolution is always grouped (and that is different from the paper) Other comments: There is an identity layer for pytorch introduced here The last thing left is get rid of conv1x1 and conv3x3 in the code - those are not better than standard In [26]: model1 = ShuffleUnitOld ( 32 , 32 , groups = 4 , grouped_conv = True , combine = 'add' ) model2 = ShuffleUnitNew ( 32 , 32 , groups = 4 , grouped_conv = True , combine = 'add' ) In [27]: x = torch . randn ( 1 , 32 , 14 , 14 ) initialize ( model1 ) initialize ( model2 ) torch . allclose ( model1 ( x ), model2 ( x )) Out[27]: True In [28]: import pickle dump1 = pickle . dumps ( model1 . _combine_func ) dump2 = pickle . dumps ( model2 ) Simplifying ResNet \u00b6 In [29]: #left class ResNetOld ( nn . Module ): def __init__ ( self , block , layers , num_classes = 1000 ): self . inplanes = 64 super ( ResNetOld , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 64 , kernel_size = 7 , stride = 2 , padding = 3 , bias = False ) self . bn1 = nn . BatchNorm2d ( 64 ) self . relu = nn . ReLU ( inplace = True ) self . maxpool = nn . MaxPool2d ( kernel_size = 3 , stride = 2 , padding = 1 ) self . layer1 = self . _make_layer ( block , 64 , layers [ 0 ]) self . layer2 = self . _make_layer ( block , 128 , layers [ 1 ], stride = 2 ) self . layer3 = self . _make_layer ( block , 256 , layers [ 2 ], stride = 2 ) self . layer4 = self . _make_layer ( block , 512 , layers [ 3 ], stride = 2 ) self . avgpool = nn . AvgPool2d ( 7 , stride = 1 ) self . fc = nn . Linear ( 512 * block . expansion , num_classes ) for m in self . modules (): if isinstance ( m , nn . Conv2d ): n = m . kernel_size [ 0 ] * m . kernel_size [ 1 ] * m . out_channels m . weight . data . normal_ ( 0 , math . sqrt ( 2. / n )) elif isinstance ( m , nn . BatchNorm2d ): m . weight . data . fill_ ( 1 ) m . bias . data . zero_ () def _make_layer ( self , block , planes , blocks , stride = 1 ): downsample = None if stride != 1 or self . inplanes != planes * block . expansion : downsample = nn . Sequential ( nn . Conv2d ( self . inplanes , planes * block . expansion , kernel_size = 1 , stride = stride , bias = False ), nn . BatchNorm2d ( planes * block . expansion ), ) layers = [] layers . append ( block ( self . inplanes , planes , stride , downsample )) self . inplanes = planes * block . expansion for i in range ( 1 , blocks ): layers . append ( block ( self . inplanes , planes )) return nn . Sequential ( * layers ) def forward ( self , x ): x = self . conv1 ( x ) x = self . bn1 ( x ) x = self . relu ( x ) x = self . maxpool ( x ) x = self . layer1 ( x ) x = self . layer2 ( x ) x = self . layer3 ( x ) x = self . layer4 ( x ) x = self . avgpool ( x ) x = x . view ( x . size ( 0 ), - 1 ) x = self . fc ( x ) return x In [30]: #right def make_layer ( inplanes , planes , block , n_blocks , stride = 1 ): downsample = None if stride != 1 or inplanes != planes * block . expansion : # output size won't match input, so adjust residual downsample = nn . Sequential ( nn . Conv2d ( inplanes , planes * block . expansion , kernel_size = 1 , stride = stride , bias = False ), nn . BatchNorm2d ( planes * block . expansion ), ) return nn . Sequential ( block ( inplanes , planes , stride , downsample ), * [ block ( planes * block . expansion , planes ) for _ in range ( 1 , n_blocks )] ) def ResNetNew ( block , layers , num_classes = 1000 ): e = block . expansion resnet = nn . Sequential ( Rearrange ( 'b c h w -> b c h w' , c = 3 , h = 224 , w = 224 ), nn . Conv2d ( 3 , 64 , kernel_size = 7 , stride = 2 , padding = 3 , bias = False ), nn . BatchNorm2d ( 64 ), nn . ReLU ( inplace = True ), nn . MaxPool2d ( kernel_size = 3 , stride = 2 , padding = 1 ), make_layer ( 64 , 64 , block , layers [ 0 ], stride = 1 ), make_layer ( 64 * e , 128 , block , layers [ 1 ], stride = 2 ), make_layer ( 128 * e , 256 , block , layers [ 2 ], stride = 2 ), make_layer ( 256 * e , 512 , block , layers [ 3 ], stride = 2 ), # combined AvgPool and view in one averaging operation Reduce ( 'b c h w -> b c' , 'mean' ), nn . Linear ( 512 * e , num_classes ), ) # initialization for m in resnet . modules (): if isinstance ( m , nn . Conv2d ): n = m . kernel_size [ 0 ] * m . kernel_size [ 1 ] * m . out_channels m . weight . data . normal_ ( 0 , math . sqrt ( 2. / n )) elif isinstance ( m , nn . BatchNorm2d ): m . weight . data . fill_ ( 1 ) m . bias . data . zero_ () return resnet Changes: explicit check for input shape no views and simple sequential structure, output is just nn.Sequential, so can always be saved/passed/etc no need in AvgPool and additional views, this place is much clearer now make_layer doesn't use internal state (that's quite faulty place) In [31]: from torchvision.models.resnet import BasicBlock , Bottleneck , ResNet In [32]: x = torch . randn ( 2 , 3 , 224 , 224 ) with torch . no_grad (): model_old = ResNetOld ( BasicBlock , layers = [ 2 , 2 , 2 , 3 ]) model_new = ResNetNew ( BasicBlock , layers = [ 2 , 2 , 2 , 3 ]) initialize ( model_old ) initialize ( model_new ) assert torch . allclose ( model_old ( x ), model_new ( x ), atol = 1e-3 ) In [33]: # with torch.no_grad(): # x = torch.randn([2, 512, 7, 7]) # torch.allclose(nn.AvgPool2d(7)(x), reduce(x, 'b c h w -> b c', 'mean'), atol=1e-8) Improving RNN language modelling \u00b6 In [34]: #left class RNNOld ( nn . Module ): def __init__ ( self , vocab_size , embedding_dim , hidden_dim , output_dim , n_layers , bidirectional , dropout ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) self . rnn = nn . LSTM ( embedding_dim , hidden_dim , num_layers = n_layers , bidirectional = bidirectional , dropout = dropout ) self . fc = nn . Linear ( hidden_dim * 2 , output_dim ) self . dropout = nn . Dropout ( dropout ) def forward ( self , x ): #x = [sent len, batch size] embedded = self . dropout ( self . embedding ( x )) #embedded = [sent len, batch size, emb dim] output , ( hidden , cell ) = self . rnn ( embedded ) #output = [sent len, batch size, hid dim * num directions] #hidden = [num layers * num directions, batch size, hid dim] #cell = [num layers * num directions, batch size, hid dim] #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers #and apply dropout hidden = self . dropout ( torch . cat (( hidden [ - 2 ,:,:], hidden [ - 1 ,:,:]), dim = 1 )) #hidden = [batch size, hid dim * num directions] return self . fc ( hidden . squeeze ( 0 )) In [35]: #right class RNNNew ( nn . Module ): def __init__ ( self , vocab_size , embedding_dim , hidden_dim , output_dim , n_layers , bidirectional , dropout ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) self . rnn = nn . LSTM ( embedding_dim , hidden_dim , num_layers = n_layers , bidirectional = bidirectional , dropout = dropout ) self . dropout = nn . Dropout ( dropout ) self . directions = 2 if bidirectional else 1 self . fc = nn . Linear ( hidden_dim * self . directions , output_dim ) def forward ( self , x ): #x = [sent len, batch size] embedded = self . dropout ( self . embedding ( x )) #embedded = [sent len, batch size, emb dim] output , ( hidden , cell ) = self . rnn ( embedded ) hidden = rearrange ( hidden , '(layer dir) b c -> layer b (dir c)' , dir = self . directions ) # take the final layer's hidden return self . fc ( self . dropout ( hidden [ - 1 ])) In [36]: model_old = initialize ( RNNOld ( 10 , 10 , 10 , output_dim = 15 , n_layers = 2 , bidirectional = True , dropout = 0.1 )) . eval () model_new = initialize ( RNNNew ( 10 , 10 , 10 , output_dim = 15 , n_layers = 2 , bidirectional = True , dropout = 0.1 )) . eval () x = torch . randint ( 0 , 10 , size = [ 23 , 10 ]) . long () assert torch . allclose ( model_old ( x ), model_new ( x )) In [37]: # this code fails # model_old = initialize(RNNOld(10, 10, 10, output_dim=15, n_layers=1, bidirectional=False, dropout=0.1)).eval() # model_old(x).shape original code misbehaves for non-bidirectional models ... and fails when bidirectional = False, and there is only one layer modification of the code shows both how hidden is structured and how it is modified Writing FastText faster \u00b6 In [38]: #left class FastTextOld ( nn . Module ): def __init__ ( self , vocab_size , embedding_dim , output_dim ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) self . fc = nn . Linear ( embedding_dim , output_dim ) def forward ( self , x ): #x = [sent len, batch size] embedded = self . embedding ( x ) #embedded = [sent len, batch size, emb dim] embedded = embedded . permute ( 1 , 0 , 2 ) #embedded = [batch size, sent len, emb dim] pooled = F . avg_pool2d ( embedded , ( embedded . shape [ 1 ], 1 )) . squeeze ( 1 ) #pooled = [batch size, embedding_dim] return self . fc ( pooled ) In [39]: #right def FastTextNew ( vocab_size , embedding_dim , output_dim ): return nn . Sequential ( Rearrange ( 't b -> t b' ), nn . Embedding ( vocab_size , embedding_dim ), Reduce ( 't b c -> b c' , 'mean' ), nn . Linear ( embedding_dim , output_dim ), Rearrange ( 'b c -> b c' ), ) Some comments on new code: first and last operations do nothing and can be removed but were added to explicitly show expected input and output this also gives you a flexibility of changing interface by editing a single line. Should you need to accept inputs as (batch, time), you just change first line to Rearrange('b t -> t b'), CNNs for text classification \u00b6 In [40]: #left class CNNOld ( nn . Module ): def __init__ ( self , vocab_size , embedding_dim , n_filters , filter_sizes , output_dim , dropout ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) self . conv_0 = nn . Conv2d ( in_channels = 1 , out_channels = n_filters , kernel_size = ( filter_sizes [ 0 ], embedding_dim )) self . conv_1 = nn . Conv2d ( in_channels = 1 , out_channels = n_filters , kernel_size = ( filter_sizes [ 1 ], embedding_dim )) self . conv_2 = nn . Conv2d ( in_channels = 1 , out_channels = n_filters , kernel_size = ( filter_sizes [ 2 ], embedding_dim )) self . fc = nn . Linear ( len ( filter_sizes ) * n_filters , output_dim ) self . dropout = nn . Dropout ( dropout ) def forward ( self , x ): #x = [sent len, batch size] x = x . permute ( 1 , 0 ) #x = [batch size, sent len] embedded = self . embedding ( x ) #embedded = [batch size, sent len, emb dim] embedded = embedded . unsqueeze ( 1 ) #embedded = [batch size, 1, sent len, emb dim] conved_0 = F . relu ( self . conv_0 ( embedded ) . squeeze ( 3 )) conved_1 = F . relu ( self . conv_1 ( embedded ) . squeeze ( 3 )) conved_2 = F . relu ( self . conv_2 ( embedded ) . squeeze ( 3 )) #conv_n = [batch size, n_filters, sent len - filter_sizes[n]] pooled_0 = F . max_pool1d ( conved_0 , conved_0 . shape [ 2 ]) . squeeze ( 2 ) pooled_1 = F . max_pool1d ( conved_1 , conved_1 . shape [ 2 ]) . squeeze ( 2 ) pooled_2 = F . max_pool1d ( conved_2 , conved_2 . shape [ 2 ]) . squeeze ( 2 ) #pooled_n = [batch size, n_filters] cat = self . dropout ( torch . cat (( pooled_0 , pooled_1 , pooled_2 ), dim = 1 )) #cat = [batch size, n_filters * len(filter_sizes)] return self . fc ( cat ) In [41]: #right class CNNNew ( nn . Module ): def __init__ ( self , vocab_size , embedding_dim , n_filters , filter_sizes , output_dim , dropout ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) self . convs = nn . ModuleList ([ nn . Conv1d ( embedding_dim , n_filters , kernel_size = size ) for size in filter_sizes ]) self . fc = nn . Linear ( len ( filter_sizes ) * n_filters , output_dim ) self . dropout = nn . Dropout ( dropout ) def forward ( self , x ): x = rearrange ( x , 't b -> t b' ) emb = rearrange ( self . embedding ( x ), 't b c -> b c t' ) pooled = [ reduce ( conv ( emb ), 'b c t -> b c' , 'max' ) for conv in self . convs ] concatenated = rearrange ( pooled , 'filter b c -> b (filter c)' ) return self . fc ( self . dropout ( F . relu ( concatenated ))) Original code misuses Conv2d, while Conv1d is the right choice Fixed code can work with any number of filter_sizes (and won't fail) First line in new code does nothing, but was added for simplicity In [42]: # old_model = initialize(CNNOld(32, 32, 32, [1, 2, 4], 32, dropout=0.1)).eval() # new_model = initialize(CNNNew(32, 32, 32, [1, 2, 4], 32, dropout=0.1)).eval() # x = torch.zeros([10, 20]).long() # assert torch.allclose(old_model(x), new_model(x), atol=1e-3) Highway convolutions \u00b6 Highway convolutions are common in TTS systems. Code below makes splitting a bit more explicit. Splitting policy may eventually turn out to be important if input had previously groups over channel axes (group convolutions or bidirectional LSTMs/GRUs) Same applies to GLU and gated units in general In [43]: #left class HighwayConv1dOld ( nn . Conv1d ): def forward ( self , inputs ): L = super ( HighwayConv1dOld , self ) . forward ( inputs ) H1 , H2 = torch . chunk ( L , 2 , 1 ) # chunk at the feature dim torch . sigmoid_ ( H1 ) return H1 * H2 + ( 1.0 - H1 ) * inputs In [44]: #right class HighwayConv1dNew ( nn . Conv1d ): def forward ( self , inputs ): L = super () . forward ( inputs ) H1 , H2 = rearrange ( L , 'b (split c) t -> split b c t' , split = 2 ) torch . sigmoid_ ( H1 ) return H1 * H2 + ( 1.0 - H1 ) * inputs In [45]: hc1 = HighwayConv1dOld ( 10 , 20 , kernel_size = 3 , padding = 1 ) hc2 = HighwayConv1dNew ( 10 , 20 , kernel_size = 3 , padding = 1 ) initialize ( hc1 ) initialize ( hc2 ) fw1 = hc1 ( torch . zeros ( 1 , 10 , 100 )) fw2 = hc2 ( torch . zeros ( 1 , 10 , 100 )) assert torch . allclose ( fw1 , fw2 ) Tacotron's CBHG module \u00b6 In [46]: #right class CBHG_Old ( nn . Module ): \"\"\"CBHG module: a recurrent neural network composed of: - 1-d convolution banks - Highway networks + residual connections - Bidirectional gated recurrent units \"\"\" def __init__ ( self , in_dim , K = 16 , projections = [ 128 , 128 ]): super ( CBHG , self ) . __init__ () self . in_dim = in_dim self . relu = nn . ReLU () self . conv1d_banks = nn . ModuleList ( [ BatchNormConv1d ( in_dim , in_dim , kernel_size = k , stride = 1 , padding = k // 2 , activation = self . relu ) for k in range ( 1 , K + 1 )]) self . max_pool1d = nn . MaxPool1d ( kernel_size = 2 , stride = 1 , padding = 1 ) in_sizes = [ K * in_dim ] + projections [: - 1 ] activations = [ self . relu ] * ( len ( projections ) - 1 ) + [ None ] self . conv1d_projections = nn . ModuleList ( [ BatchNormConv1d ( in_size , out_size , kernel_size = 3 , stride = 1 , padding = 1 , activation = ac ) for ( in_size , out_size , ac ) in zip ( in_sizes , projections , activations )]) self . pre_highway = nn . Linear ( projections [ - 1 ], in_dim , bias = False ) self . highways = nn . ModuleList ( [ Highway ( in_dim , in_dim ) for _ in range ( 4 )]) self . gru = nn . GRU ( in_dim , in_dim , 1 , batch_first = True , bidirectional = True ) In [47]: #left def forward_old ( self , inputs ): # (B, T_in, in_dim) x = inputs # Needed to perform conv1d on time-axis # (B, in_dim, T_in) if x . size ( - 1 ) == self . in_dim : x = x . transpose ( 1 , 2 ) T = x . size ( - 1 ) # (B, in_dim*K, T_in) # Concat conv1d bank outputs x = torch . cat ([ conv1d ( x )[:, :, : T ] for conv1d in self . conv1d_banks ], dim = 1 ) assert x . size ( 1 ) == self . in_dim * len ( self . conv1d_banks ) x = self . max_pool1d ( x )[:, :, : T ] for conv1d in self . conv1d_projections : x = conv1d ( x ) # (B, T_in, in_dim) # Back to the original shape x = x . transpose ( 1 , 2 ) if x . size ( - 1 ) != self . in_dim : x = self . pre_highway ( x ) # Residual connection x += inputs for highway in self . highways : x = highway ( x ) # (B, T_in, in_dim*2) outputs , _ = self . gru ( x ) return outputs In [48]: #right def forward_new ( self , inputs , input_lengths = None ): x = rearrange ( inputs , 'b t c -> b c t' ) _ , _ , T = x . shape # Concat conv1d bank outputs x = rearrange ([ conv1d ( x )[:, :, : T ] for conv1d in self . conv1d_banks ], 'bank b c t -> b (bank c) t' , c = self . in_dim ) x = self . max_pool1d ( x )[:, :, : T ] for conv1d in self . conv1d_projections : x = conv1d ( x ) x = rearrange ( x , 'b c t -> b t c' ) if x . size ( - 1 ) != self . in_dim : x = self . pre_highway ( x ) # Residual connection x += inputs for highway in self . highways : x = highway ( x ) # (B, T_in, in_dim*2) outputs , _ = self . gru ( self . highways ( x )) return outputs There is still a large room for improvements, but in this example only forward function was changed Simple attention \u00b6 Good news: there is no more need to guess order of dimensions. Neither for inputs nor for outputs In [49]: #left class Attention ( nn . Module ): def __init__ ( self ): super ( Attention , self ) . __init__ () def forward ( self , K , V , Q ): A = torch . bmm ( K . transpose ( 1 , 2 ), Q ) / np . sqrt ( Q . shape [ 1 ]) A = F . softmax ( A , 1 ) R = torch . bmm ( V , A ) return torch . cat (( R , Q ), dim = 1 ) In [50]: #right def attention ( K , V , Q ): _ , n_channels , _ = K . shape A = torch . einsum ( 'bct,bcl->btl' , [ K , Q ]) A = F . softmax ( A * n_channels ** ( - 0.5 ), 1 ) R = torch . einsum ( 'bct,btl->bcl' , [ V , A ]) return torch . cat (( R , Q ), dim = 1 ) In [51]: args = dict ( K = torch . zeros ( 32 , 128 , 40 ) . cuda (), V = torch . zeros ( 32 , 128 , 40 ) . cuda (), Q = torch . zeros ( 32 , 128 , 30 ) . cuda (), ) % timeit - n100 result_old = Attention ()( ** args ); torch . cuda . synchronize () % timeit - n100 result_new = attention ( ** args ); torch . cuda . synchronize () 336 \u00b5s \u00b1 7.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 336 \u00b5s \u00b1 7.48 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) In [52]: result_old = Attention ()( ** args ); torch . cuda . synchronize () result_new = attention ( ** args ); torch . cuda . synchronize () assert torch . allclose ( result_old , result_new ) Transformer's attention needs more attention \u00b6 In [53]: #left class ScaledDotProductAttention ( nn . Module ): ''' Scaled Dot-Product Attention ''' def __init__ ( self , temperature , attn_dropout = 0.1 ): super () . __init__ () self . temperature = temperature self . dropout = nn . Dropout ( attn_dropout ) self . softmax = nn . Softmax ( dim = 2 ) def forward ( self , q , k , v , mask = None ): attn = torch . bmm ( q , k . transpose ( 1 , 2 )) attn = attn / self . temperature if mask is not None : attn = attn . masked_fill ( mask , - np . inf ) attn = self . softmax ( attn ) attn = self . dropout ( attn ) output = torch . bmm ( attn , v ) return output , attn class MultiHeadAttentionOld ( nn . Module ): ''' Multi-Head Attention module ''' def __init__ ( self , n_head , d_model , d_k , d_v , dropout = 0.1 ): super () . __init__ () self . n_head = n_head self . d_k = d_k self . d_v = d_v self . w_qs = nn . Linear ( d_model , n_head * d_k ) self . w_ks = nn . Linear ( d_model , n_head * d_k ) self . w_vs = nn . Linear ( d_model , n_head * d_v ) nn . init . normal_ ( self . w_qs . weight , mean = 0 , std = np . sqrt ( 2.0 / ( d_model + d_k ))) nn . init . normal_ ( self . w_ks . weight , mean = 0 , std = np . sqrt ( 2.0 / ( d_model + d_k ))) nn . init . normal_ ( self . w_vs . weight , mean = 0 , std = np . sqrt ( 2.0 / ( d_model + d_v ))) self . attention = ScaledDotProductAttention ( temperature = np . power ( d_k , 0.5 )) self . layer_norm = nn . LayerNorm ( d_model ) self . fc = nn . Linear ( n_head * d_v , d_model ) nn . init . xavier_normal_ ( self . fc . weight ) self . dropout = nn . Dropout ( dropout ) def forward ( self , q , k , v , mask = None ): d_k , d_v , n_head = self . d_k , self . d_v , self . n_head sz_b , len_q , _ = q . size () sz_b , len_k , _ = k . size () sz_b , len_v , _ = v . size () residual = q q = self . w_qs ( q ) . view ( sz_b , len_q , n_head , d_k ) k = self . w_ks ( k ) . view ( sz_b , len_k , n_head , d_k ) v = self . w_vs ( v ) . view ( sz_b , len_v , n_head , d_v ) q = q . permute ( 2 , 0 , 1 , 3 ) . contiguous () . view ( - 1 , len_q , d_k ) # (n*b) x lq x dk k = k . permute ( 2 , 0 , 1 , 3 ) . contiguous () . view ( - 1 , len_k , d_k ) # (n*b) x lk x dk v = v . permute ( 2 , 0 , 1 , 3 ) . contiguous () . view ( - 1 , len_v , d_v ) # (n*b) x lv x dv mask = mask . repeat ( n_head , 1 , 1 ) # (n*b) x .. x .. output , attn = self . attention ( q , k , v , mask = mask ) output = output . view ( n_head , sz_b , len_q , d_v ) output = output . permute ( 1 , 2 , 0 , 3 ) . contiguous () . view ( sz_b , len_q , - 1 ) # b x lq x (n*dv) output = self . dropout ( self . fc ( output )) output = self . layer_norm ( output + residual ) return output , attn In [54]: #right class MultiHeadAttentionNew ( nn . Module ): def __init__ ( self , n_head , d_model , d_k , d_v , dropout = 0.1 ): super () . __init__ () self . n_head = n_head self . w_qs = nn . Linear ( d_model , n_head * d_k ) self . w_ks = nn . Linear ( d_model , n_head * d_k ) self . w_vs = nn . Linear ( d_model , n_head * d_v ) nn . init . normal_ ( self . w_qs . weight , mean = 0 , std = np . sqrt ( 2.0 / ( d_model + d_k ))) nn . init . normal_ ( self . w_ks . weight , mean = 0 , std = np . sqrt ( 2.0 / ( d_model + d_k ))) nn . init . normal_ ( self . w_vs . weight , mean = 0 , std = np . sqrt ( 2.0 / ( d_model + d_v ))) self . fc = nn . Linear ( n_head * d_v , d_model ) nn . init . xavier_normal_ ( self . fc . weight ) self . dropout = nn . Dropout ( p = dropout ) self . layer_norm = nn . LayerNorm ( d_model ) def forward ( self , q , k , v , mask = None ): residual = q q = rearrange ( self . w_qs ( q ), 'b l (head k) -> head b l k' , head = self . n_head ) k = rearrange ( self . w_ks ( k ), 'b t (head k) -> head b t k' , head = self . n_head ) v = rearrange ( self . w_vs ( v ), 'b t (head v) -> head b t v' , head = self . n_head ) attn = torch . einsum ( 'hblk,hbtk->hblt' , [ q , k ]) / np . sqrt ( q . shape [ - 1 ]) if mask is not None : attn = attn . masked_fill ( mask [ None ], - np . inf ) attn = torch . softmax ( attn , dim = 3 ) output = torch . einsum ( 'hblt,hbtv->hblv' , [ attn , v ]) output = rearrange ( output , 'head b l v -> b l (head v)' ) output = self . dropout ( self . fc ( output )) output = self . layer_norm ( output + residual ) return output , attn Benefits of new implementation we have one module, not two now code does not fail for None mask the amount of caveats in the original code that we removed is huge. Try erasing comments and deciphering what happens there In [55]: # Poor implementation of torch.einsum, so code below doesn't work class MultiHeadAttentionHard ( nn . Module ): def __init__ ( self , n_head , d_model , d_k , d_v , dropout = 0.1 ): super () . __init__ () self . w_qs = nn . Parameter ( torch . randn ( d_model , n_head , d_k ) * np . sqrt ( 2.0 / ( d_model + d_k ))) self . w_ks = nn . Parameter ( torch . randn ( d_model , n_head , d_k ) * np . sqrt ( 2.0 / ( d_model + d_k ))) self . w_vs = nn . Parameter ( torch . randn ( d_model , n_head , d_v ) * np . sqrt ( 2.0 / ( d_model + d_v ))) self . w_fc = nn . Parameter ( torch . randn ( d_model , n_head , d_v ) * np . sqrt ( 2.0 / ( d_model + n_head * d_v ))) self . dropout = nn . Dropout ( p = dropout ) self . layer_norm = nn . LayerNorm ( d_model ) def forward ( self , q , k , v , mask = None ): attn = torch . einsum ( 'bld,dhc,bte,ehc->hblt' , [ q , self . w_qs , k , self . w_ks ]) if mask is not None : attn = attn . masked_fill ( mask [ None ], - np . inf ) attn = torch . softmax ( attn , dim = 3 ) output = torch . einsum ( 'hblt,bte,ehv,dhv->hbd' , [ attn , v , self . w_vs , self . w_fc ]) output = self . dropout ( output ) output = self . layer_norm ( output + q ) return output , attn In [56]: n_heads = 8 d_k = 32 d_v = 64 d_model = 100 t = 51 l = 53 batch = 30 layer1 = initialize ( MultiHeadAttentionOld ( n_heads , d_k = d_k , d_v = d_v , d_model = d_model )) . eval () . cuda () layer2 = initialize ( MultiHeadAttentionNew ( n_heads , d_k = d_k , d_v = d_v , d_model = d_model )) . eval () . cuda () args = dict ( q = torch . randn ( batch , l , d_model ), k = torch . randn ( batch , t , d_model ) * 0.1 , v = torch . randn ( batch , t , d_model ), mask = torch . randn ( batch , l , t ) > 0 , ) args = { k : v . cuda () for k , v in args . items ()} o1 , a1 = layer1 ( ** args ) o2 , a2 = layer2 ( ** args ) In [57]: a1 . shape , a2 . shape Out[57]: (torch.Size([240, 53, 51]), torch.Size([8, 30, 53, 51])) In [58]: assert torch . allclose ( o1 , o2 ) In [59]: % timeit - n 200 layer1 ( ** args ); torch . cuda . synchronize () % timeit - n 200 layer2 ( ** args ); torch . cuda . synchronize () 4.82 ms \u00b1 73.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 200 loops each) 4.6 ms \u00b1 116 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 200 loops each) Self-attention GANs \u00b6 SAGANs are currently SotA for image generation, and can be simplified using same tricks. In [60]: #left class Self_Attn_Old ( nn . Module ): \"\"\" Self attention Layer\"\"\" def __init__ ( self , in_dim , activation ): super ( Self_Attn_Old , self ) . __init__ () self . chanel_in = in_dim self . activation = activation self . query_conv = nn . Conv2d ( in_channels = in_dim , out_channels = in_dim // 8 , kernel_size = 1 ) self . key_conv = nn . Conv2d ( in_channels = in_dim , out_channels = in_dim // 8 , kernel_size = 1 ) self . value_conv = nn . Conv2d ( in_channels = in_dim , out_channels = in_dim , kernel_size = 1 ) self . gamma = nn . Parameter ( torch . zeros ( 1 )) self . softmax = nn . Softmax ( dim =- 1 ) # def forward ( self , x ): \"\"\" inputs : x : input feature maps( B X C X W X H) returns : out : self attention value + input feature attention: B X N X N (N is Width*Height) \"\"\" m_batchsize , C , width , height = x . size () proj_query = self . query_conv ( x ) . view ( m_batchsize , - 1 , width * height ) . permute ( 0 , 2 , 1 ) # B X CX(N) proj_key = self . key_conv ( x ) . view ( m_batchsize , - 1 , width * height ) # B X C x (*W*H) energy = torch . bmm ( proj_query , proj_key ) # transpose check attention = self . softmax ( energy ) # BX (N) X (N) proj_value = self . value_conv ( x ) . view ( m_batchsize , - 1 , width * height ) # B X C X N out = torch . bmm ( proj_value , attention . permute ( 0 , 2 , 1 ) ) out = out . view ( m_batchsize , C , width , height ) out = self . gamma * out + x return out , attention In [61]: #right class Self_Attn_New ( nn . Module ): \"\"\" Self attention Layer\"\"\" def __init__ ( self , in_dim ): super () . __init__ () self . query_conv = nn . Conv2d ( in_dim , out_channels = in_dim // 8 , kernel_size = 1 ) self . key_conv = nn . Conv2d ( in_dim , out_channels = in_dim // 8 , kernel_size = 1 ) self . value_conv = nn . Conv2d ( in_dim , out_channels = in_dim , kernel_size = 1 ) self . gamma = nn . Parameter ( torch . zeros ([ 1 ])) def forward ( self , x ): proj_query = rearrange ( self . query_conv ( x ), 'b c h w -> b (h w) c' ) proj_key = rearrange ( self . key_conv ( x ), 'b c h w -> b c (h w)' ) proj_value = rearrange ( self . value_conv ( x ), 'b c h w -> b (h w) c' ) energy = torch . bmm ( proj_query , proj_key ) attention = F . softmax ( energy , dim = 2 ) out = torch . bmm ( attention , proj_value ) out = x + self . gamma * rearrange ( out , 'b (h w) c -> b c h w' , ** parse_shape ( x , 'b c h w' )) return out , attention In [62]: model_old = initialize ( Self_Attn_Old ( 128 , None )) model_new = initialize ( Self_Attn_New ( 128 )) In [63]: x = torch . randn ( 2 , 128 , 30 , 30 ) assert torch . allclose ( model_old ( x )[ 0 ], model_new ( x )[ 0 ], atol = 1e-4 ) # returned attention is transposed assert torch . allclose ( model_old ( x )[ 1 ], model_new ( x )[ 1 ], atol = 1e-4 ) In [64]: % timeit model_old ( x )[ 0 ] . sum () . item () % timeit model_new ( x )[ 0 ] . sum () . item () # surprise - I had slow down here due to the order of softmax, not einops 15.9 ms \u00b1 990 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 14.5 ms \u00b1 81.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) Improving time sequence prediction \u00b6 While this example was considered to be simplistic, I had to analyze surrounding code to understand what kind of input was expected. You can try yourself. Additionally now the code works with any dtype, not only double; and new code supports using GPU. In [65]: #left class SequencePredictionOld ( nn . Module ): def __init__ ( self ): super ( SequencePredictionOld , self ) . __init__ () self . lstm1 = nn . LSTMCell ( 1 , 51 ) self . lstm2 = nn . LSTMCell ( 51 , 51 ) self . linear = nn . Linear ( 51 , 1 ) def forward ( self , input , future = 0 ): outputs = [] h_t = torch . zeros ( input . size ( 0 ), 51 , dtype = torch . double ) c_t = torch . zeros ( input . size ( 0 ), 51 , dtype = torch . double ) h_t2 = torch . zeros ( input . size ( 0 ), 51 , dtype = torch . double ) c_t2 = torch . zeros ( input . size ( 0 ), 51 , dtype = torch . double ) for i , input_t in enumerate ( input . chunk ( input . size ( 1 ), dim = 1 )): h_t , c_t = self . lstm1 ( input_t , ( h_t , c_t )) h_t2 , c_t2 = self . lstm2 ( h_t , ( h_t2 , c_t2 )) output = self . linear ( h_t2 ) outputs += [ output ] for i in range ( future ): # if we should predict the future h_t , c_t = self . lstm1 ( output , ( h_t , c_t )) h_t2 , c_t2 = self . lstm2 ( h_t , ( h_t2 , c_t2 )) output = self . linear ( h_t2 ) outputs += [ output ] outputs = torch . stack ( outputs , 1 ) . squeeze ( 2 ) return outputs In [66]: #right class SequencePredictionNew ( nn . Module ): def __init__ ( self ): super ( SequencePredictionNew , self ) . __init__ () self . lstm1 = nn . LSTMCell ( 1 , 51 ) self . lstm2 = nn . LSTMCell ( 51 , 51 ) self . linear = nn . Linear ( 51 , 1 ) def forward ( self , input , future = 0 ): b , t = input . shape h_t , c_t , h_t2 , c_t2 = torch . zeros ( 4 , b , 51 , dtype = self . linear . weight . dtype , device = self . linear . weight . device ) outputs = [] for input_t in rearrange ( input , 'b t -> t b ()' ): h_t , c_t = self . lstm1 ( input_t , ( h_t , c_t )) h_t2 , c_t2 = self . lstm2 ( h_t , ( h_t2 , c_t2 )) output = self . linear ( h_t2 ) outputs += [ output ] for i in range ( future ): # if we should predict the future h_t , c_t = self . lstm1 ( output , ( h_t , c_t )) h_t2 , c_t2 = self . lstm2 ( h_t , ( h_t2 , c_t2 )) output = self . linear ( h_t2 ) outputs += [ output ] return rearrange ( outputs , 't b () -> b t' ) In [67]: seq_old = SequencePredictionOld () . double () seq_new = SequencePredictionNew () . double () initialize ( seq_old ) initialize ( seq_new ) x = torch . randn ([ 10 , 10 ], dtype = torch . double ) In [68]: result_old = seq_old ( x ) result_new = seq_new ( x ) assert torch . allclose ( result_old , result_new ) Transforming spacial transformer network (STN) \u00b6 In [69]: #left class SpacialTransformOld ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () # Spatial transformer localization-network self . localization = nn . Sequential ( nn . Conv2d ( 1 , 8 , kernel_size = 7 ), nn . MaxPool2d ( 2 , stride = 2 ), nn . ReLU ( True ), nn . Conv2d ( 8 , 10 , kernel_size = 5 ), nn . MaxPool2d ( 2 , stride = 2 ), nn . ReLU ( True ) ) # Regressor for the 3 * 2 affine matrix self . fc_loc = nn . Sequential ( nn . Linear ( 10 * 3 * 3 , 32 ), nn . ReLU ( True ), nn . Linear ( 32 , 3 * 2 ) ) # Initialize the weights/bias with identity transformation self . fc_loc [ 2 ] . weight . data . zero_ () self . fc_loc [ 2 ] . bias . data . copy_ ( torch . tensor ([ 1 , 0 , 0 , 0 , 1 , 0 ], dtype = torch . float )) # Spatial transformer network forward function def stn ( self , x ): xs = self . localization ( x ) xs = xs . view ( - 1 , 10 * 3 * 3 ) theta = self . fc_loc ( xs ) theta = theta . view ( - 1 , 2 , 3 ) grid = F . affine_grid ( theta , x . size ()) x = F . grid_sample ( x , grid ) return x In [70]: #right class SpacialTransformNew ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () # Spatial transformer localization-network linear = nn . Linear ( 32 , 3 * 2 ) # Initialize the weights/bias with identity transformation linear . weight . data . zero_ () linear . bias . data . copy_ ( torch . tensor ([ 1 , 0 , 0 , 0 , 1 , 0 ], dtype = torch . float )) self . compute_theta = nn . Sequential ( nn . Conv2d ( 1 , 8 , kernel_size = 7 ), nn . MaxPool2d ( 2 , stride = 2 ), nn . ReLU ( True ), nn . Conv2d ( 8 , 10 , kernel_size = 5 ), nn . MaxPool2d ( 2 , stride = 2 ), nn . ReLU ( True ), Rearrange ( 'b c h w -> b (c h w)' , h = 3 , w = 3 ), nn . Linear ( 10 * 3 * 3 , 32 ), nn . ReLU ( True ), linear , Rearrange ( 'b (row col) -> b row col' , row = 2 , col = 3 ), ) # Spatial transformer network forward function def stn ( self , x ): grid = F . affine_grid ( self . compute_theta ( x ), x . size ()) return F . grid_sample ( x , grid ) new code will give reasonable errors when passed image size is different from expected if batch size is divisible by 18, whatever you input in the old code, it'll fail no sooner than affine_grid. Improving GLOW \u00b6 That's a good old depth-to-space written manually! Since GLOW is revertible, it will frequently rely on rearrange -like operations. In [71]: #left def unsqueeze2d_old ( input , factor = 2 ): assert factor >= 1 and isinstance ( factor , int ) factor2 = factor ** 2 if factor == 1 : return input size = input . size () B = size [ 0 ] C = size [ 1 ] H = size [ 2 ] W = size [ 3 ] assert C % ( factor2 ) == 0 , \" {} \" . format ( C ) x = input . view ( B , C // factor2 , factor , factor , H , W ) x = x . permute ( 0 , 1 , 4 , 2 , 5 , 3 ) . contiguous () x = x . view ( B , C // ( factor2 ), H * factor , W * factor ) return x def squeeze2d_old ( input , factor = 2 ): assert factor >= 1 and isinstance ( factor , int ) if factor == 1 : return input size = input . size () B = size [ 0 ] C = size [ 1 ] H = size [ 2 ] W = size [ 3 ] assert H % factor == 0 and W % factor == 0 , \" {} \" . format (( H , W )) x = input . view ( B , C , H // factor , factor , W // factor , factor ) x = x . permute ( 0 , 1 , 3 , 5 , 2 , 4 ) . contiguous () x = x . view ( B , C * factor * factor , H // factor , W // factor ) return x In [72]: #right def unsqueeze2d_new ( input , factor = 2 ): return rearrange ( input , 'b (c h2 w2) h w -> b c (h h2) (w w2)' , h2 = factor , w2 = factor ) def squeeze2d_new ( input , factor = 2 ): return rearrange ( input , 'b c (h h2) (w w2) -> b (c h2 w2) h w' , h2 = factor , w2 = factor ) term squeeze isn't very helpful: which dimension is squeezed? There is torch.squeeze , but it's very different. in fact, we could skip creating functions completely - it is a single call to einops anyway Detecting problems in YOLO detection \u00b6 In [73]: #left def YOLO_prediction_old ( input , num_classes , num_anchors , anchors , stride_h , stride_w ): bs = input . size ( 0 ) in_h = input . size ( 2 ) in_w = input . size ( 3 ) scaled_anchors = [( a_w / stride_w , a_h / stride_h ) for a_w , a_h in anchors ] prediction = input . view ( bs , num_anchors , 5 + num_classes , in_h , in_w ) . permute ( 0 , 1 , 3 , 4 , 2 ) . contiguous () # Get outputs x = torch . sigmoid ( prediction [ ... , 0 ]) # Center x y = torch . sigmoid ( prediction [ ... , 1 ]) # Center y w = prediction [ ... , 2 ] # Width h = prediction [ ... , 3 ] # Height conf = torch . sigmoid ( prediction [ ... , 4 ]) # Conf pred_cls = torch . sigmoid ( prediction [ ... , 5 :]) # Cls pred. FloatTensor = torch . cuda . FloatTensor if x . is_cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if x . is_cuda else torch . LongTensor # Calculate offsets for each grid grid_x = torch . linspace ( 0 , in_w - 1 , in_w ) . repeat ( in_w , 1 ) . repeat ( bs * num_anchors , 1 , 1 ) . view ( x . shape ) . type ( FloatTensor ) grid_y = torch . linspace ( 0 , in_h - 1 , in_h ) . repeat ( in_h , 1 ) . t () . repeat ( bs * num_anchors , 1 , 1 ) . view ( y . shape ) . type ( FloatTensor ) # Calculate anchor w, h anchor_w = FloatTensor ( scaled_anchors ) . index_select ( 1 , LongTensor ([ 0 ])) anchor_h = FloatTensor ( scaled_anchors ) . index_select ( 1 , LongTensor ([ 1 ])) anchor_w = anchor_w . repeat ( bs , 1 ) . repeat ( 1 , 1 , in_h * in_w ) . view ( w . shape ) anchor_h = anchor_h . repeat ( bs , 1 ) . repeat ( 1 , 1 , in_h * in_w ) . view ( h . shape ) # Add offset and scale with anchors pred_boxes = FloatTensor ( prediction [ ... , : 4 ] . shape ) pred_boxes [ ... , 0 ] = x . data + grid_x pred_boxes [ ... , 1 ] = y . data + grid_y pred_boxes [ ... , 2 ] = torch . exp ( w . data ) * anchor_w pred_boxes [ ... , 3 ] = torch . exp ( h . data ) * anchor_h # Results _scale = torch . Tensor ([ stride_w , stride_h ] * 2 ) . type ( FloatTensor ) output = torch . cat (( pred_boxes . view ( bs , - 1 , 4 ) * _scale , conf . view ( bs , - 1 , 1 ), pred_cls . view ( bs , - 1 , num_classes )), - 1 ) return output In [74]: #right def YOLO_prediction_new ( input , num_classes , num_anchors , anchors , stride_h , stride_w ): raw_predictions = rearrange ( input , 'b (anchor prediction) h w -> prediction b anchor h w' , anchor = num_anchors , prediction = 5 + num_classes ) anchors = torch . FloatTensor ( anchors ) . to ( input . device ) anchor_sizes = rearrange ( anchors , 'anchor dim -> dim () anchor () ()' ) _ , _ , _ , in_h , in_w = raw_predictions . shape grid_h = rearrange ( torch . arange ( in_h ) . float (), 'h -> () () h ()' ) . to ( input . device ) grid_w = rearrange ( torch . arange ( in_w ) . float (), 'w -> () () () w' ) . to ( input . device ) predicted_bboxes = torch . zeros_like ( raw_predictions ) predicted_bboxes [ 0 ] = ( raw_predictions [ 0 ] . sigmoid () + grid_w ) * stride_w # center x predicted_bboxes [ 1 ] = ( raw_predictions [ 1 ] . sigmoid () + grid_h ) * stride_h # center y predicted_bboxes [ 2 : 4 ] = ( raw_predictions [ 2 : 4 ] . exp ()) * anchor_sizes # bbox width and height predicted_bboxes [ 4 ] = raw_predictions [ 4 ] . sigmoid () # confidence predicted_bboxes [ 5 :] = raw_predictions [ 5 :] . sigmoid () # class predictions # merging all predicted bboxes for each image return rearrange ( predicted_bboxes , 'prediction b anchor h w -> b (anchor h w) prediction' ) We changed and fixed a lot: new code won't fail if input is not on the first GPU old code has wrong grid_x and grid_y for non-square images new code doesn't use replication when broadcasting is sufficient old code strangely sometimes takes .data , but this has no real effect, as some branches preserve gradient till the end if gradients not needed, torch.no_grad should be used, so it's redundant Simpler output for a bunch of pictures \u00b6 Next time you need to output drawings of you generative models, you can use this trick In [75]: fake_batch = torch . rand ([ 100 , 3 , 1 , 1 ]) + torch . zeros ([ 100 , 3 , 32 , 32 ]) from matplotlib import pyplot as plt import torchvision.utils as vutils In [76]: #right device = 'cpu' plt . imshow ( np . transpose ( vutils . make_grid ( fake_batch . to ( device )[: 64 ], padding = 2 , normalize = True ) . cpu (),( 1 , 2 , 0 ))) Out[76]: <matplotlib.image.AxesImage at 0x7fdd8d606908> In [77]: #right padded = F . pad ( fake_batch [: 64 ], [ 1 , 1 , 1 , 1 ]) plt . imshow ( rearrange ( padded , '(b1 b2) c h w -> (b1 h) (b2 w) c' , b1 = 8 ) . cpu ()) Out[77]: <matplotlib.image.AxesImage at 0x7fdd8d595390> In [78]: # TODO: Hierarchical softmax # TODO: some reinforcement stuff would also be needed Instead of conclusion \u00b6 Better code is a vague term; to be specific, code is expected to be: reliable: does what expected and does not fail. Explicitly fails for wrong inputs maintainable and modifiable reusable: understanding and modifying code should be easier than writing from scratch fast: in my measurements, proposed versions have speed similar to the original code readability counts, as a mean to achieve previous goals Provided examples show how to improve these criteria for deep learning code. And einops helps a lot. Links \u00b6 pytorch and einops significant part of the code was taken from the official examples and tutorials . All code fragments were taken for educational purpose. (references for other code are given in source of this html) einops has a tutorial for a more gentle introduction In [ ]:","title":"Pytorch"},{"location":"source_examples/Pytorch/#writing-a-better-code-with-pytorch-and-einops","text":"","title":"Writing a better code with pytorch and einops"},{"location":"source_examples/Pytorch/#rewriting-building-blocks-of-deep-learning","text":"Now let's get to examples from real world. These code fragments taken from official tutorials and popular repositories. Learn how to improve code and how einops can help you. Left : as it was, Right : improved version In [1]: #right # start from importing some stuff import torch import torch.nn as nn import torch.nn.functional as F import numpy as np import math from einops import rearrange , reduce , asnumpy , parse_shape from einops.layers.torch import Rearrange , Reduce In [2]: def initialize ( model ): for p in model . parameters (): p . data [:] = torch . from_numpy ( np . random . RandomState ( sum ( p . shape )) . randn ( * p . shape )) return model","title":"Rewriting building blocks of deep learning"},{"location":"source_examples/Pytorch/#simple-convnet","text":"In [3]: #left class Net ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () self . conv1 = nn . Conv2d ( 1 , 10 , kernel_size = 5 ) self . conv2 = nn . Conv2d ( 10 , 20 , kernel_size = 5 ) self . conv2_drop = nn . Dropout2d () self . fc1 = nn . Linear ( 320 , 50 ) self . fc2 = nn . Linear ( 50 , 10 ) def forward ( self , x ): x = F . relu ( F . max_pool2d ( self . conv1 ( x ), 2 )) x = F . relu ( F . max_pool2d ( self . conv2_drop ( self . conv2 ( x )), 2 )) x = x . view ( - 1 , 320 ) x = F . relu ( self . fc1 ( x )) x = F . dropout ( x , training = self . training ) x = self . fc2 ( x ) return F . log_softmax ( x , dim = 1 ) conv_net_old = Net () In [4]: #right conv_net_new = nn . Sequential ( nn . Conv2d ( 1 , 10 , kernel_size = 5 ), nn . MaxPool2d ( kernel_size = 2 ), nn . ReLU (), nn . Conv2d ( 10 , 20 , kernel_size = 5 ), nn . MaxPool2d ( kernel_size = 2 ), nn . ReLU (), nn . Dropout2d (), Rearrange ( 'b c h w -> b (c h w)' ), nn . Linear ( 320 , 50 ), nn . ReLU (), nn . Dropout (), nn . Linear ( 50 , 10 ), nn . LogSoftmax ( dim = 1 ) ) Reasons to prefer new implementation: in the original code (to the left) if input size is changed and batch size is divisible by 16 (that's usualy so), we'll get something senseless after reshaping new code will explicitly raise an error in this case we won't forget to use dropout with flag self.training with new version code is straightforward to read and analyze sequential makes printing / saving / passing trivial. And there is no need in your code to load a model (which also has a number of benefits) don't need logsoftmax? Now you can use conv_net_new[:-1] . One more reason to prefer nn.Sequential ... and we could also add inplace for ReLU In [5]: conv_net_old ( torch . zeros ([ 16 , 1 , 20 , 20 ])) . shape # conv_net_new(torch.zeros([16, 1, 20, 20])).shape Out[5]: torch.Size([4, 10])","title":"Simple ConvNet"},{"location":"source_examples/Pytorch/#super-resolution","text":"In [6]: #left class SuperResolutionNetOld ( nn . Module ): def __init__ ( self , upscale_factor ): super ( SuperResolutionNetOld , self ) . __init__ () self . relu = nn . ReLU () self . conv1 = nn . Conv2d ( 1 , 64 , ( 5 , 5 ), ( 1 , 1 ), ( 2 , 2 )) self . conv2 = nn . Conv2d ( 64 , 64 , ( 3 , 3 ), ( 1 , 1 ), ( 1 , 1 )) self . conv3 = nn . Conv2d ( 64 , 32 , ( 3 , 3 ), ( 1 , 1 ), ( 1 , 1 )) self . conv4 = nn . Conv2d ( 32 , upscale_factor ** 2 , ( 3 , 3 ), ( 1 , 1 ), ( 1 , 1 )) self . pixel_shuffle = nn . PixelShuffle ( upscale_factor ) def forward ( self , x ): x = self . relu ( self . conv1 ( x )) x = self . relu ( self . conv2 ( x )) x = self . relu ( self . conv3 ( x )) x = self . pixel_shuffle ( self . conv4 ( x )) return x In [7]: #right def SuperResolutionNetNew ( upscale_factor ): return nn . Sequential ( nn . Conv2d ( 1 , 64 , kernel_size = 5 , padding = 2 ), nn . ReLU ( inplace = True ), nn . Conv2d ( 64 , 64 , kernel_size = 3 , padding = 1 ), nn . ReLU ( inplace = True ), nn . Conv2d ( 64 , 32 , kernel_size = 3 , padding = 1 ), nn . ReLU ( inplace = True ), nn . Conv2d ( 32 , upscale_factor ** 2 , kernel_size = 3 , padding = 1 ), Rearrange ( 'b (h2 w2) h w -> b (h h2) (w w2)' , h2 = upscale_factor , w2 = upscale_factor ), ) Here is the difference: no need in special instruction pixel_shuffle (and result is transferrable between frameworks) output doesn't contain a fake axis (and we could do the same for the input) inplace ReLU used now, for high resolution pictures that becomes critical and saves us much memory and all the benefits of nn.Sequential again In [8]: model1 = initialize ( SuperResolutionNetOld ( upscale_factor = 3 )) model2 = initialize ( SuperResolutionNetNew ( upscale_factor = 3 )) In [9]: assert torch . allclose ( model1 ( torch . zeros ( 1 , 1 , 30 , 30 )), model2 ( torch . zeros ( 1 , 1 , 30 , 30 ))[ None ]) In [10]: ## that's how this code was mentioned to use # from PIL import Image # img = Image.open(opt.input_image).convert('YCbCr') # y, cb, cr = img.split() # model = torch.load(opt.model) # img_to_tensor = ToTensor() # input = img_to_tensor(y).view(1, -1, y.size[1], y.size[0]) # if opt.cuda: # model = model.cuda() # input = input.cuda() # out = model(input) # out = out.cpu() # out_img_y = out[0].detach().numpy() # out_img_y *= 255.0 # out_img_y = out_img_y.clip(0, 255) # out_img_y = Image.fromarray(np.uint8(out_img_y[0]), mode='L') # out_img_cb = cb.resize(out_img_y.size, Image.BICUBIC) # out_img_cr = cr.resize(out_img_y.size, Image.BICUBIC) # out_img = Image.merge('YCbCr', [out_img_y, out_img_cb, out_img_cr]).convert('RGB') ## Benefits # - no need to remembder the order of components in PIL.Image.size (as you see, it is actually different) # - code explicitly shows shapes passed in and out # - normalization to [0, 1] range and back is also explicit (it is needed to rememebed in original code that division by 255 is done by ToTensor) input_image = '../../logo/einops_logo_350x350.png' from PIL import Image import numpy as np from torchvision.transforms import ToTensor model = SuperResolutionNetOld ( upscale_factor = 2 ) img = Image . open ( input_image ) . convert ( 'YCbCr' ) y , cb , cr = img . split () img_to_tensor = ToTensor () input = img_to_tensor ( y ) . view ( 1 , - 1 , y . size [ 1 ], y . size [ 0 ]) out = model ( input ) out_img_y = out [ 0 ] . detach () . numpy () out_img_y = np . clip ( out_img_y [ 0 ] * 255 , 0 , 255 ) model = SuperResolutionNetNew ( upscale_factor = 2 ) img = Image . open ( input_image ) . convert ( 'YCbCr' ) y , cb , cr = img . split () # TODO numpy.asarray y = torch . from_numpy ( np . array ( y , dtype = 'float32' ) / 255 ) out = model ( rearrange ( y , 'h w -> () () h w' )) out_img_y = asnumpy ( rearrange ( out , '() h w -> h w' )) out_img_y = np . clip ( out_img_y * 255 , 0 , 255 )","title":"Super-resolution"},{"location":"source_examples/Pytorch/#restyling-gram-matrix-for-style-transfer","text":"Original code is already good - first line shows what kind of input is expected einsum operation should be read like: for each batch and for each pair of channels, we sum over h and w. I've also changed normalization, because that's how Gram matrix is defined, otherwise we should call it normalized Gram matrix or alike In [11]: #left def gram_matrix_old ( y ): ( b , ch , h , w ) = y . size () features = y . view ( b , ch , w * h ) features_t = features . transpose ( 1 , 2 ) gram = features . bmm ( features_t ) / ( ch * h * w ) return gram In [12]: #right def gram_matrix_new ( y ): b , ch , h , w = y . shape return torch . einsum ( 'bchw,bdhw->bcd' , [ y , y ]) / ( h * w ) It would be great to use just 'b c1 h w,b c2 h w->b c1 c2' , but einsum supports only one-letter axes In [13]: x = torch . randn ([ 32 , 128 , 40 , 40 ]) In [14]: % timeit gram_matrix_old ( x ) . sum () % timeit gram_matrix_new ( x ) . sum () 7.58 ms \u00b1 492 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 9.66 ms \u00b1 258 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) In [15]: assert torch . allclose ( gram_matrix_old ( x ), gram_matrix_new ( x ) / 128 ) In [16]: # x = x.to('cuda') # %timeit -n100 gram_matrix_old(x).sum(); torch.cuda.synchronize() # %timeit -n100 gram_matrix_new(x).sum(); torch.cuda.synchronize()","title":"Restyling Gram matrix for style transfer"},{"location":"source_examples/Pytorch/#recurrent-model","text":"All we did here is just made information about shapes explicit to skip deciphering In [17]: #left class RNNModelOld ( nn . Module ): \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\" def __init__ ( self , ntoken , ninp , nhid , nlayers , dropout = 0.5 ): super ( RNNModel , self ) . __init__ () self . drop = nn . Dropout ( dropout ) self . encoder = nn . Embedding ( ntoken , ninp ) self . rnn = nn . LSTM ( ninp , nhid , nlayers , dropout = dropout ) self . decoder = nn . Linear ( nhid , ntoken ) def forward ( self , input , hidden ): emb = self . drop ( self . encoder ( input )) output , hidden = self . rnn ( emb , hidden ) output = self . drop ( output ) decoded = self . decoder ( output . view ( output . size ( 0 ) * output . size ( 1 ), output . size ( 2 ))) return decoded . view ( output . size ( 0 ), output . size ( 1 ), decoded . size ( 1 )), hidden In [18]: #right class RNNModelNew ( nn . Module ): \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\" def __init__ ( self , ntoken , ninp , nhid , nlayers , dropout = 0.5 ): super ( RNNModel , self ) . __init__ () self . drop = nn . Dropout ( p = dropout ) self . encoder = nn . Embedding ( ntoken , ninp ) self . rnn = nn . LSTM ( ninp , nhid , nlayers , dropout = dropout ) self . decoder = nn . Linear ( nhid , ntoken ) def forward ( self , input , hidden ): t , b = input . shape emb = self . drop ( self . encoder ( input )) output , hidden = self . rnn ( emb , hidden ) output = rearrange ( self . drop ( output ), 't b nhid -> (t b) nhid' ) decoded = rearrange ( self . decoder ( output ), '(t b) token -> t b token' , t = t , b = b ) return decoded , hidden","title":"Recurrent model"},{"location":"source_examples/Pytorch/#channel-shuffle-from-shufflenet","text":"In [19]: #left def channel_shuffle_old ( x , groups ): batchsize , num_channels , height , width = x . data . size () channels_per_group = num_channels // groups # reshape x = x . view ( batchsize , groups , channels_per_group , height , width ) # transpose # - contiguous() required if transpose() is used before view(). # See https://github.com/pytorch/pytorch/issues/764 x = torch . transpose ( x , 1 , 2 ) . contiguous () # flatten x = x . view ( batchsize , - 1 , height , width ) return x In [20]: #right def channel_shuffle_new ( x , groups ): return rearrange ( x , 'b (c1 c2) h w -> b (c2 c1) h w' , c1 = groups ) While progress is obvious, this is not the limit. As you'll see below, we don't even need to write these couple of lines. In [21]: x = torch . zeros ([ 32 , 64 , 100 , 100 ]) In [22]: % timeit - n100 channel_shuffle_old ( x , 8 ); torch . cuda . synchronize () % timeit - n100 channel_shuffle_new ( x , 8 ); torch . cuda . synchronize () 51.2 ms \u00b1 2.18 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 49.9 ms \u00b1 594 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)","title":"Channel shuffle (from shufflenet)"},{"location":"source_examples/Pytorch/#shufflenet","text":"In [23]: def conv3x3 ( in_channels , out_channels , stride = 1 , padding = 1 , bias = True , groups = 1 ): \"\"\"3x3 convolution with padding \"\"\" return nn . Conv2d ( in_channels , out_channels , kernel_size = 3 , stride = stride , padding = padding , bias = bias , groups = groups ) def conv1x1 ( in_channels , out_channels , groups = 1 ): \"\"\"1x1 convolution with padding - Normal pointwise convolution When groups == 1 - Grouped pointwise convolution when groups > 1 \"\"\" return nn . Conv2d ( in_channels , out_channels , kernel_size = 1 , groups = groups , stride = 1 ) In [24]: #left from collections import OrderedDict def channel_shuffle ( x , groups ): batchsize , num_channels , height , width = x . data . size () channels_per_group = num_channels // groups # reshape x = x . view ( batchsize , groups , channels_per_group , height , width ) # transpose # - contiguous() required if transpose() is used before view(). # See https://github.com/pytorch/pytorch/issues/764 x = torch . transpose ( x , 1 , 2 ) . contiguous () # flatten x = x . view ( batchsize , - 1 , height , width ) return x class ShuffleUnitOld ( nn . Module ): def __init__ ( self , in_channels , out_channels , groups = 3 , grouped_conv = True , combine = 'add' ): super ( ShuffleUnitOld , self ) . __init__ () self . in_channels = in_channels self . out_channels = out_channels self . grouped_conv = grouped_conv self . combine = combine self . groups = groups self . bottleneck_channels = self . out_channels // 4 # define the type of ShuffleUnit if self . combine == 'add' : # ShuffleUnit Figure 2b self . depthwise_stride = 1 self . _combine_func = self . _add elif self . combine == 'concat' : # ShuffleUnit Figure 2c self . depthwise_stride = 2 self . _combine_func = self . _concat # ensure output of concat has the same channels as # original output channels. self . out_channels -= self . in_channels else : raise ValueError ( \"Cannot combine tensors with \\\" {} \\\" \" \\ \"Only \\\" add \\\" and \\\" concat \\\" are\" \\ \"supported\" . format ( self . combine )) # Use a 1x1 grouped or non-grouped convolution to reduce input channels # to bottleneck channels, as in a ResNet bottleneck module. # NOTE: Do not use group convolution for the first conv1x1 in Stage 2. self . first_1x1_groups = self . groups if grouped_conv else 1 self . g_conv_1x1_compress = self . _make_grouped_conv1x1 ( self . in_channels , self . bottleneck_channels , self . first_1x1_groups , batch_norm = True , relu = True ) # 3x3 depthwise convolution followed by batch normalization self . depthwise_conv3x3 = conv3x3 ( self . bottleneck_channels , self . bottleneck_channels , stride = self . depthwise_stride , groups = self . bottleneck_channels ) self . bn_after_depthwise = nn . BatchNorm2d ( self . bottleneck_channels ) # Use 1x1 grouped convolution to expand from # bottleneck_channels to out_channels self . g_conv_1x1_expand = self . _make_grouped_conv1x1 ( self . bottleneck_channels , self . out_channels , self . groups , batch_norm = True , relu = False ) @staticmethod def _add ( x , out ): # residual connection return x + out @staticmethod def _concat ( x , out ): # concatenate along channel axis return torch . cat (( x , out ), 1 ) def _make_grouped_conv1x1 ( self , in_channels , out_channels , groups , batch_norm = True , relu = False ): modules = OrderedDict () conv = conv1x1 ( in_channels , out_channels , groups = groups ) modules [ 'conv1x1' ] = conv if batch_norm : modules [ 'batch_norm' ] = nn . BatchNorm2d ( out_channels ) if relu : modules [ 'relu' ] = nn . ReLU () if len ( modules ) > 1 : return nn . Sequential ( modules ) else : return conv def forward ( self , x ): # save for combining later with output residual = x if self . combine == 'concat' : residual = F . avg_pool2d ( residual , kernel_size = 3 , stride = 2 , padding = 1 ) out = self . g_conv_1x1_compress ( x ) out = channel_shuffle ( out , self . groups ) out = self . depthwise_conv3x3 ( out ) out = self . bn_after_depthwise ( out ) out = self . g_conv_1x1_expand ( out ) out = self . _combine_func ( residual , out ) return F . relu ( out ) In [25]: #right class ShuffleUnitNew ( nn . Module ): def __init__ ( self , in_channels , out_channels , groups = 3 , grouped_conv = True , combine = 'add' ): super () . __init__ () first_1x1_groups = groups if grouped_conv else 1 bottleneck_channels = out_channels // 4 self . combine = combine if combine == 'add' : # ShuffleUnit Figure 2b self . left = Rearrange ( '...->...' ) # identity depthwise_stride = 1 else : # ShuffleUnit Figure 2c self . left = nn . AvgPool2d ( kernel_size = 3 , stride = 2 , padding = 1 ) depthwise_stride = 2 # ensure output of concat has the same channels as original output channels. out_channels -= in_channels assert out_channels > 0 self . right = nn . Sequential ( # Use a 1x1 grouped or non-grouped convolution to reduce input channels # to bottleneck channels, as in a ResNet bottleneck module. conv1x1 ( in_channels , bottleneck_channels , groups = first_1x1_groups ), nn . BatchNorm2d ( bottleneck_channels ), nn . ReLU ( inplace = True ), # channel shuffle Rearrange ( 'b (c1 c2) h w -> b (c2 c1) h w' , c1 = groups ), # 3x3 depthwise convolution followed by batch conv3x3 ( bottleneck_channels , bottleneck_channels , stride = depthwise_stride , groups = bottleneck_channels ), nn . BatchNorm2d ( bottleneck_channels ), # Use 1x1 grouped convolution to expand from # bottleneck_channels to out_channels conv1x1 ( bottleneck_channels , out_channels , groups = groups ), nn . BatchNorm2d ( out_channels ), ) def forward ( self , x ): if self . combine == 'add' : combined = self . left ( x ) + self . right ( x ) else : combined = torch . cat ([ self . left ( x ), self . right ( x )], dim = 1 ) return F . relu ( combined , inplace = True ) Rewriting the code helped to identify: There is no sense in doing reshuffling and not using groups in the first convolution (indeed, in the paper it is not so). However, result is an equivalent model. It is also strange that the first convolution may be not grouped, while the last convolution is always grouped (and that is different from the paper) Other comments: There is an identity layer for pytorch introduced here The last thing left is get rid of conv1x1 and conv3x3 in the code - those are not better than standard In [26]: model1 = ShuffleUnitOld ( 32 , 32 , groups = 4 , grouped_conv = True , combine = 'add' ) model2 = ShuffleUnitNew ( 32 , 32 , groups = 4 , grouped_conv = True , combine = 'add' ) In [27]: x = torch . randn ( 1 , 32 , 14 , 14 ) initialize ( model1 ) initialize ( model2 ) torch . allclose ( model1 ( x ), model2 ( x )) Out[27]: True In [28]: import pickle dump1 = pickle . dumps ( model1 . _combine_func ) dump2 = pickle . dumps ( model2 )","title":"Shufflenet"},{"location":"source_examples/Pytorch/#simplifying-resnet","text":"In [29]: #left class ResNetOld ( nn . Module ): def __init__ ( self , block , layers , num_classes = 1000 ): self . inplanes = 64 super ( ResNetOld , self ) . __init__ () self . conv1 = nn . Conv2d ( 3 , 64 , kernel_size = 7 , stride = 2 , padding = 3 , bias = False ) self . bn1 = nn . BatchNorm2d ( 64 ) self . relu = nn . ReLU ( inplace = True ) self . maxpool = nn . MaxPool2d ( kernel_size = 3 , stride = 2 , padding = 1 ) self . layer1 = self . _make_layer ( block , 64 , layers [ 0 ]) self . layer2 = self . _make_layer ( block , 128 , layers [ 1 ], stride = 2 ) self . layer3 = self . _make_layer ( block , 256 , layers [ 2 ], stride = 2 ) self . layer4 = self . _make_layer ( block , 512 , layers [ 3 ], stride = 2 ) self . avgpool = nn . AvgPool2d ( 7 , stride = 1 ) self . fc = nn . Linear ( 512 * block . expansion , num_classes ) for m in self . modules (): if isinstance ( m , nn . Conv2d ): n = m . kernel_size [ 0 ] * m . kernel_size [ 1 ] * m . out_channels m . weight . data . normal_ ( 0 , math . sqrt ( 2. / n )) elif isinstance ( m , nn . BatchNorm2d ): m . weight . data . fill_ ( 1 ) m . bias . data . zero_ () def _make_layer ( self , block , planes , blocks , stride = 1 ): downsample = None if stride != 1 or self . inplanes != planes * block . expansion : downsample = nn . Sequential ( nn . Conv2d ( self . inplanes , planes * block . expansion , kernel_size = 1 , stride = stride , bias = False ), nn . BatchNorm2d ( planes * block . expansion ), ) layers = [] layers . append ( block ( self . inplanes , planes , stride , downsample )) self . inplanes = planes * block . expansion for i in range ( 1 , blocks ): layers . append ( block ( self . inplanes , planes )) return nn . Sequential ( * layers ) def forward ( self , x ): x = self . conv1 ( x ) x = self . bn1 ( x ) x = self . relu ( x ) x = self . maxpool ( x ) x = self . layer1 ( x ) x = self . layer2 ( x ) x = self . layer3 ( x ) x = self . layer4 ( x ) x = self . avgpool ( x ) x = x . view ( x . size ( 0 ), - 1 ) x = self . fc ( x ) return x In [30]: #right def make_layer ( inplanes , planes , block , n_blocks , stride = 1 ): downsample = None if stride != 1 or inplanes != planes * block . expansion : # output size won't match input, so adjust residual downsample = nn . Sequential ( nn . Conv2d ( inplanes , planes * block . expansion , kernel_size = 1 , stride = stride , bias = False ), nn . BatchNorm2d ( planes * block . expansion ), ) return nn . Sequential ( block ( inplanes , planes , stride , downsample ), * [ block ( planes * block . expansion , planes ) for _ in range ( 1 , n_blocks )] ) def ResNetNew ( block , layers , num_classes = 1000 ): e = block . expansion resnet = nn . Sequential ( Rearrange ( 'b c h w -> b c h w' , c = 3 , h = 224 , w = 224 ), nn . Conv2d ( 3 , 64 , kernel_size = 7 , stride = 2 , padding = 3 , bias = False ), nn . BatchNorm2d ( 64 ), nn . ReLU ( inplace = True ), nn . MaxPool2d ( kernel_size = 3 , stride = 2 , padding = 1 ), make_layer ( 64 , 64 , block , layers [ 0 ], stride = 1 ), make_layer ( 64 * e , 128 , block , layers [ 1 ], stride = 2 ), make_layer ( 128 * e , 256 , block , layers [ 2 ], stride = 2 ), make_layer ( 256 * e , 512 , block , layers [ 3 ], stride = 2 ), # combined AvgPool and view in one averaging operation Reduce ( 'b c h w -> b c' , 'mean' ), nn . Linear ( 512 * e , num_classes ), ) # initialization for m in resnet . modules (): if isinstance ( m , nn . Conv2d ): n = m . kernel_size [ 0 ] * m . kernel_size [ 1 ] * m . out_channels m . weight . data . normal_ ( 0 , math . sqrt ( 2. / n )) elif isinstance ( m , nn . BatchNorm2d ): m . weight . data . fill_ ( 1 ) m . bias . data . zero_ () return resnet Changes: explicit check for input shape no views and simple sequential structure, output is just nn.Sequential, so can always be saved/passed/etc no need in AvgPool and additional views, this place is much clearer now make_layer doesn't use internal state (that's quite faulty place) In [31]: from torchvision.models.resnet import BasicBlock , Bottleneck , ResNet In [32]: x = torch . randn ( 2 , 3 , 224 , 224 ) with torch . no_grad (): model_old = ResNetOld ( BasicBlock , layers = [ 2 , 2 , 2 , 3 ]) model_new = ResNetNew ( BasicBlock , layers = [ 2 , 2 , 2 , 3 ]) initialize ( model_old ) initialize ( model_new ) assert torch . allclose ( model_old ( x ), model_new ( x ), atol = 1e-3 ) In [33]: # with torch.no_grad(): # x = torch.randn([2, 512, 7, 7]) # torch.allclose(nn.AvgPool2d(7)(x), reduce(x, 'b c h w -> b c', 'mean'), atol=1e-8)","title":"Simplifying ResNet"},{"location":"source_examples/Pytorch/#improving-rnn-language-modelling","text":"In [34]: #left class RNNOld ( nn . Module ): def __init__ ( self , vocab_size , embedding_dim , hidden_dim , output_dim , n_layers , bidirectional , dropout ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) self . rnn = nn . LSTM ( embedding_dim , hidden_dim , num_layers = n_layers , bidirectional = bidirectional , dropout = dropout ) self . fc = nn . Linear ( hidden_dim * 2 , output_dim ) self . dropout = nn . Dropout ( dropout ) def forward ( self , x ): #x = [sent len, batch size] embedded = self . dropout ( self . embedding ( x )) #embedded = [sent len, batch size, emb dim] output , ( hidden , cell ) = self . rnn ( embedded ) #output = [sent len, batch size, hid dim * num directions] #hidden = [num layers * num directions, batch size, hid dim] #cell = [num layers * num directions, batch size, hid dim] #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers #and apply dropout hidden = self . dropout ( torch . cat (( hidden [ - 2 ,:,:], hidden [ - 1 ,:,:]), dim = 1 )) #hidden = [batch size, hid dim * num directions] return self . fc ( hidden . squeeze ( 0 )) In [35]: #right class RNNNew ( nn . Module ): def __init__ ( self , vocab_size , embedding_dim , hidden_dim , output_dim , n_layers , bidirectional , dropout ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) self . rnn = nn . LSTM ( embedding_dim , hidden_dim , num_layers = n_layers , bidirectional = bidirectional , dropout = dropout ) self . dropout = nn . Dropout ( dropout ) self . directions = 2 if bidirectional else 1 self . fc = nn . Linear ( hidden_dim * self . directions , output_dim ) def forward ( self , x ): #x = [sent len, batch size] embedded = self . dropout ( self . embedding ( x )) #embedded = [sent len, batch size, emb dim] output , ( hidden , cell ) = self . rnn ( embedded ) hidden = rearrange ( hidden , '(layer dir) b c -> layer b (dir c)' , dir = self . directions ) # take the final layer's hidden return self . fc ( self . dropout ( hidden [ - 1 ])) In [36]: model_old = initialize ( RNNOld ( 10 , 10 , 10 , output_dim = 15 , n_layers = 2 , bidirectional = True , dropout = 0.1 )) . eval () model_new = initialize ( RNNNew ( 10 , 10 , 10 , output_dim = 15 , n_layers = 2 , bidirectional = True , dropout = 0.1 )) . eval () x = torch . randint ( 0 , 10 , size = [ 23 , 10 ]) . long () assert torch . allclose ( model_old ( x ), model_new ( x )) In [37]: # this code fails # model_old = initialize(RNNOld(10, 10, 10, output_dim=15, n_layers=1, bidirectional=False, dropout=0.1)).eval() # model_old(x).shape original code misbehaves for non-bidirectional models ... and fails when bidirectional = False, and there is only one layer modification of the code shows both how hidden is structured and how it is modified","title":"Improving RNN language modelling"},{"location":"source_examples/Pytorch/#writing-fasttext-faster","text":"In [38]: #left class FastTextOld ( nn . Module ): def __init__ ( self , vocab_size , embedding_dim , output_dim ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) self . fc = nn . Linear ( embedding_dim , output_dim ) def forward ( self , x ): #x = [sent len, batch size] embedded = self . embedding ( x ) #embedded = [sent len, batch size, emb dim] embedded = embedded . permute ( 1 , 0 , 2 ) #embedded = [batch size, sent len, emb dim] pooled = F . avg_pool2d ( embedded , ( embedded . shape [ 1 ], 1 )) . squeeze ( 1 ) #pooled = [batch size, embedding_dim] return self . fc ( pooled ) In [39]: #right def FastTextNew ( vocab_size , embedding_dim , output_dim ): return nn . Sequential ( Rearrange ( 't b -> t b' ), nn . Embedding ( vocab_size , embedding_dim ), Reduce ( 't b c -> b c' , 'mean' ), nn . Linear ( embedding_dim , output_dim ), Rearrange ( 'b c -> b c' ), ) Some comments on new code: first and last operations do nothing and can be removed but were added to explicitly show expected input and output this also gives you a flexibility of changing interface by editing a single line. Should you need to accept inputs as (batch, time), you just change first line to Rearrange('b t -> t b'),","title":"Writing FastText faster"},{"location":"source_examples/Pytorch/#cnns-for-text-classification","text":"In [40]: #left class CNNOld ( nn . Module ): def __init__ ( self , vocab_size , embedding_dim , n_filters , filter_sizes , output_dim , dropout ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) self . conv_0 = nn . Conv2d ( in_channels = 1 , out_channels = n_filters , kernel_size = ( filter_sizes [ 0 ], embedding_dim )) self . conv_1 = nn . Conv2d ( in_channels = 1 , out_channels = n_filters , kernel_size = ( filter_sizes [ 1 ], embedding_dim )) self . conv_2 = nn . Conv2d ( in_channels = 1 , out_channels = n_filters , kernel_size = ( filter_sizes [ 2 ], embedding_dim )) self . fc = nn . Linear ( len ( filter_sizes ) * n_filters , output_dim ) self . dropout = nn . Dropout ( dropout ) def forward ( self , x ): #x = [sent len, batch size] x = x . permute ( 1 , 0 ) #x = [batch size, sent len] embedded = self . embedding ( x ) #embedded = [batch size, sent len, emb dim] embedded = embedded . unsqueeze ( 1 ) #embedded = [batch size, 1, sent len, emb dim] conved_0 = F . relu ( self . conv_0 ( embedded ) . squeeze ( 3 )) conved_1 = F . relu ( self . conv_1 ( embedded ) . squeeze ( 3 )) conved_2 = F . relu ( self . conv_2 ( embedded ) . squeeze ( 3 )) #conv_n = [batch size, n_filters, sent len - filter_sizes[n]] pooled_0 = F . max_pool1d ( conved_0 , conved_0 . shape [ 2 ]) . squeeze ( 2 ) pooled_1 = F . max_pool1d ( conved_1 , conved_1 . shape [ 2 ]) . squeeze ( 2 ) pooled_2 = F . max_pool1d ( conved_2 , conved_2 . shape [ 2 ]) . squeeze ( 2 ) #pooled_n = [batch size, n_filters] cat = self . dropout ( torch . cat (( pooled_0 , pooled_1 , pooled_2 ), dim = 1 )) #cat = [batch size, n_filters * len(filter_sizes)] return self . fc ( cat ) In [41]: #right class CNNNew ( nn . Module ): def __init__ ( self , vocab_size , embedding_dim , n_filters , filter_sizes , output_dim , dropout ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) self . convs = nn . ModuleList ([ nn . Conv1d ( embedding_dim , n_filters , kernel_size = size ) for size in filter_sizes ]) self . fc = nn . Linear ( len ( filter_sizes ) * n_filters , output_dim ) self . dropout = nn . Dropout ( dropout ) def forward ( self , x ): x = rearrange ( x , 't b -> t b' ) emb = rearrange ( self . embedding ( x ), 't b c -> b c t' ) pooled = [ reduce ( conv ( emb ), 'b c t -> b c' , 'max' ) for conv in self . convs ] concatenated = rearrange ( pooled , 'filter b c -> b (filter c)' ) return self . fc ( self . dropout ( F . relu ( concatenated ))) Original code misuses Conv2d, while Conv1d is the right choice Fixed code can work with any number of filter_sizes (and won't fail) First line in new code does nothing, but was added for simplicity In [42]: # old_model = initialize(CNNOld(32, 32, 32, [1, 2, 4], 32, dropout=0.1)).eval() # new_model = initialize(CNNNew(32, 32, 32, [1, 2, 4], 32, dropout=0.1)).eval() # x = torch.zeros([10, 20]).long() # assert torch.allclose(old_model(x), new_model(x), atol=1e-3)","title":"CNNs for text classification"},{"location":"source_examples/Pytorch/#highway-convolutions","text":"Highway convolutions are common in TTS systems. Code below makes splitting a bit more explicit. Splitting policy may eventually turn out to be important if input had previously groups over channel axes (group convolutions or bidirectional LSTMs/GRUs) Same applies to GLU and gated units in general In [43]: #left class HighwayConv1dOld ( nn . Conv1d ): def forward ( self , inputs ): L = super ( HighwayConv1dOld , self ) . forward ( inputs ) H1 , H2 = torch . chunk ( L , 2 , 1 ) # chunk at the feature dim torch . sigmoid_ ( H1 ) return H1 * H2 + ( 1.0 - H1 ) * inputs In [44]: #right class HighwayConv1dNew ( nn . Conv1d ): def forward ( self , inputs ): L = super () . forward ( inputs ) H1 , H2 = rearrange ( L , 'b (split c) t -> split b c t' , split = 2 ) torch . sigmoid_ ( H1 ) return H1 * H2 + ( 1.0 - H1 ) * inputs In [45]: hc1 = HighwayConv1dOld ( 10 , 20 , kernel_size = 3 , padding = 1 ) hc2 = HighwayConv1dNew ( 10 , 20 , kernel_size = 3 , padding = 1 ) initialize ( hc1 ) initialize ( hc2 ) fw1 = hc1 ( torch . zeros ( 1 , 10 , 100 )) fw2 = hc2 ( torch . zeros ( 1 , 10 , 100 )) assert torch . allclose ( fw1 , fw2 )","title":"Highway convolutions"},{"location":"source_examples/Pytorch/#tacotrons-cbhg-module","text":"In [46]: #right class CBHG_Old ( nn . Module ): \"\"\"CBHG module: a recurrent neural network composed of: - 1-d convolution banks - Highway networks + residual connections - Bidirectional gated recurrent units \"\"\" def __init__ ( self , in_dim , K = 16 , projections = [ 128 , 128 ]): super ( CBHG , self ) . __init__ () self . in_dim = in_dim self . relu = nn . ReLU () self . conv1d_banks = nn . ModuleList ( [ BatchNormConv1d ( in_dim , in_dim , kernel_size = k , stride = 1 , padding = k // 2 , activation = self . relu ) for k in range ( 1 , K + 1 )]) self . max_pool1d = nn . MaxPool1d ( kernel_size = 2 , stride = 1 , padding = 1 ) in_sizes = [ K * in_dim ] + projections [: - 1 ] activations = [ self . relu ] * ( len ( projections ) - 1 ) + [ None ] self . conv1d_projections = nn . ModuleList ( [ BatchNormConv1d ( in_size , out_size , kernel_size = 3 , stride = 1 , padding = 1 , activation = ac ) for ( in_size , out_size , ac ) in zip ( in_sizes , projections , activations )]) self . pre_highway = nn . Linear ( projections [ - 1 ], in_dim , bias = False ) self . highways = nn . ModuleList ( [ Highway ( in_dim , in_dim ) for _ in range ( 4 )]) self . gru = nn . GRU ( in_dim , in_dim , 1 , batch_first = True , bidirectional = True ) In [47]: #left def forward_old ( self , inputs ): # (B, T_in, in_dim) x = inputs # Needed to perform conv1d on time-axis # (B, in_dim, T_in) if x . size ( - 1 ) == self . in_dim : x = x . transpose ( 1 , 2 ) T = x . size ( - 1 ) # (B, in_dim*K, T_in) # Concat conv1d bank outputs x = torch . cat ([ conv1d ( x )[:, :, : T ] for conv1d in self . conv1d_banks ], dim = 1 ) assert x . size ( 1 ) == self . in_dim * len ( self . conv1d_banks ) x = self . max_pool1d ( x )[:, :, : T ] for conv1d in self . conv1d_projections : x = conv1d ( x ) # (B, T_in, in_dim) # Back to the original shape x = x . transpose ( 1 , 2 ) if x . size ( - 1 ) != self . in_dim : x = self . pre_highway ( x ) # Residual connection x += inputs for highway in self . highways : x = highway ( x ) # (B, T_in, in_dim*2) outputs , _ = self . gru ( x ) return outputs In [48]: #right def forward_new ( self , inputs , input_lengths = None ): x = rearrange ( inputs , 'b t c -> b c t' ) _ , _ , T = x . shape # Concat conv1d bank outputs x = rearrange ([ conv1d ( x )[:, :, : T ] for conv1d in self . conv1d_banks ], 'bank b c t -> b (bank c) t' , c = self . in_dim ) x = self . max_pool1d ( x )[:, :, : T ] for conv1d in self . conv1d_projections : x = conv1d ( x ) x = rearrange ( x , 'b c t -> b t c' ) if x . size ( - 1 ) != self . in_dim : x = self . pre_highway ( x ) # Residual connection x += inputs for highway in self . highways : x = highway ( x ) # (B, T_in, in_dim*2) outputs , _ = self . gru ( self . highways ( x )) return outputs There is still a large room for improvements, but in this example only forward function was changed","title":"Tacotron's CBHG module"},{"location":"source_examples/Pytorch/#simple-attention","text":"Good news: there is no more need to guess order of dimensions. Neither for inputs nor for outputs In [49]: #left class Attention ( nn . Module ): def __init__ ( self ): super ( Attention , self ) . __init__ () def forward ( self , K , V , Q ): A = torch . bmm ( K . transpose ( 1 , 2 ), Q ) / np . sqrt ( Q . shape [ 1 ]) A = F . softmax ( A , 1 ) R = torch . bmm ( V , A ) return torch . cat (( R , Q ), dim = 1 ) In [50]: #right def attention ( K , V , Q ): _ , n_channels , _ = K . shape A = torch . einsum ( 'bct,bcl->btl' , [ K , Q ]) A = F . softmax ( A * n_channels ** ( - 0.5 ), 1 ) R = torch . einsum ( 'bct,btl->bcl' , [ V , A ]) return torch . cat (( R , Q ), dim = 1 ) In [51]: args = dict ( K = torch . zeros ( 32 , 128 , 40 ) . cuda (), V = torch . zeros ( 32 , 128 , 40 ) . cuda (), Q = torch . zeros ( 32 , 128 , 30 ) . cuda (), ) % timeit - n100 result_old = Attention ()( ** args ); torch . cuda . synchronize () % timeit - n100 result_new = attention ( ** args ); torch . cuda . synchronize () 336 \u00b5s \u00b1 7.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 336 \u00b5s \u00b1 7.48 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) In [52]: result_old = Attention ()( ** args ); torch . cuda . synchronize () result_new = attention ( ** args ); torch . cuda . synchronize () assert torch . allclose ( result_old , result_new )","title":"Simple attention"},{"location":"source_examples/Pytorch/#transformers-attention-needs-more-attention","text":"In [53]: #left class ScaledDotProductAttention ( nn . Module ): ''' Scaled Dot-Product Attention ''' def __init__ ( self , temperature , attn_dropout = 0.1 ): super () . __init__ () self . temperature = temperature self . dropout = nn . Dropout ( attn_dropout ) self . softmax = nn . Softmax ( dim = 2 ) def forward ( self , q , k , v , mask = None ): attn = torch . bmm ( q , k . transpose ( 1 , 2 )) attn = attn / self . temperature if mask is not None : attn = attn . masked_fill ( mask , - np . inf ) attn = self . softmax ( attn ) attn = self . dropout ( attn ) output = torch . bmm ( attn , v ) return output , attn class MultiHeadAttentionOld ( nn . Module ): ''' Multi-Head Attention module ''' def __init__ ( self , n_head , d_model , d_k , d_v , dropout = 0.1 ): super () . __init__ () self . n_head = n_head self . d_k = d_k self . d_v = d_v self . w_qs = nn . Linear ( d_model , n_head * d_k ) self . w_ks = nn . Linear ( d_model , n_head * d_k ) self . w_vs = nn . Linear ( d_model , n_head * d_v ) nn . init . normal_ ( self . w_qs . weight , mean = 0 , std = np . sqrt ( 2.0 / ( d_model + d_k ))) nn . init . normal_ ( self . w_ks . weight , mean = 0 , std = np . sqrt ( 2.0 / ( d_model + d_k ))) nn . init . normal_ ( self . w_vs . weight , mean = 0 , std = np . sqrt ( 2.0 / ( d_model + d_v ))) self . attention = ScaledDotProductAttention ( temperature = np . power ( d_k , 0.5 )) self . layer_norm = nn . LayerNorm ( d_model ) self . fc = nn . Linear ( n_head * d_v , d_model ) nn . init . xavier_normal_ ( self . fc . weight ) self . dropout = nn . Dropout ( dropout ) def forward ( self , q , k , v , mask = None ): d_k , d_v , n_head = self . d_k , self . d_v , self . n_head sz_b , len_q , _ = q . size () sz_b , len_k , _ = k . size () sz_b , len_v , _ = v . size () residual = q q = self . w_qs ( q ) . view ( sz_b , len_q , n_head , d_k ) k = self . w_ks ( k ) . view ( sz_b , len_k , n_head , d_k ) v = self . w_vs ( v ) . view ( sz_b , len_v , n_head , d_v ) q = q . permute ( 2 , 0 , 1 , 3 ) . contiguous () . view ( - 1 , len_q , d_k ) # (n*b) x lq x dk k = k . permute ( 2 , 0 , 1 , 3 ) . contiguous () . view ( - 1 , len_k , d_k ) # (n*b) x lk x dk v = v . permute ( 2 , 0 , 1 , 3 ) . contiguous () . view ( - 1 , len_v , d_v ) # (n*b) x lv x dv mask = mask . repeat ( n_head , 1 , 1 ) # (n*b) x .. x .. output , attn = self . attention ( q , k , v , mask = mask ) output = output . view ( n_head , sz_b , len_q , d_v ) output = output . permute ( 1 , 2 , 0 , 3 ) . contiguous () . view ( sz_b , len_q , - 1 ) # b x lq x (n*dv) output = self . dropout ( self . fc ( output )) output = self . layer_norm ( output + residual ) return output , attn In [54]: #right class MultiHeadAttentionNew ( nn . Module ): def __init__ ( self , n_head , d_model , d_k , d_v , dropout = 0.1 ): super () . __init__ () self . n_head = n_head self . w_qs = nn . Linear ( d_model , n_head * d_k ) self . w_ks = nn . Linear ( d_model , n_head * d_k ) self . w_vs = nn . Linear ( d_model , n_head * d_v ) nn . init . normal_ ( self . w_qs . weight , mean = 0 , std = np . sqrt ( 2.0 / ( d_model + d_k ))) nn . init . normal_ ( self . w_ks . weight , mean = 0 , std = np . sqrt ( 2.0 / ( d_model + d_k ))) nn . init . normal_ ( self . w_vs . weight , mean = 0 , std = np . sqrt ( 2.0 / ( d_model + d_v ))) self . fc = nn . Linear ( n_head * d_v , d_model ) nn . init . xavier_normal_ ( self . fc . weight ) self . dropout = nn . Dropout ( p = dropout ) self . layer_norm = nn . LayerNorm ( d_model ) def forward ( self , q , k , v , mask = None ): residual = q q = rearrange ( self . w_qs ( q ), 'b l (head k) -> head b l k' , head = self . n_head ) k = rearrange ( self . w_ks ( k ), 'b t (head k) -> head b t k' , head = self . n_head ) v = rearrange ( self . w_vs ( v ), 'b t (head v) -> head b t v' , head = self . n_head ) attn = torch . einsum ( 'hblk,hbtk->hblt' , [ q , k ]) / np . sqrt ( q . shape [ - 1 ]) if mask is not None : attn = attn . masked_fill ( mask [ None ], - np . inf ) attn = torch . softmax ( attn , dim = 3 ) output = torch . einsum ( 'hblt,hbtv->hblv' , [ attn , v ]) output = rearrange ( output , 'head b l v -> b l (head v)' ) output = self . dropout ( self . fc ( output )) output = self . layer_norm ( output + residual ) return output , attn Benefits of new implementation we have one module, not two now code does not fail for None mask the amount of caveats in the original code that we removed is huge. Try erasing comments and deciphering what happens there In [55]: # Poor implementation of torch.einsum, so code below doesn't work class MultiHeadAttentionHard ( nn . Module ): def __init__ ( self , n_head , d_model , d_k , d_v , dropout = 0.1 ): super () . __init__ () self . w_qs = nn . Parameter ( torch . randn ( d_model , n_head , d_k ) * np . sqrt ( 2.0 / ( d_model + d_k ))) self . w_ks = nn . Parameter ( torch . randn ( d_model , n_head , d_k ) * np . sqrt ( 2.0 / ( d_model + d_k ))) self . w_vs = nn . Parameter ( torch . randn ( d_model , n_head , d_v ) * np . sqrt ( 2.0 / ( d_model + d_v ))) self . w_fc = nn . Parameter ( torch . randn ( d_model , n_head , d_v ) * np . sqrt ( 2.0 / ( d_model + n_head * d_v ))) self . dropout = nn . Dropout ( p = dropout ) self . layer_norm = nn . LayerNorm ( d_model ) def forward ( self , q , k , v , mask = None ): attn = torch . einsum ( 'bld,dhc,bte,ehc->hblt' , [ q , self . w_qs , k , self . w_ks ]) if mask is not None : attn = attn . masked_fill ( mask [ None ], - np . inf ) attn = torch . softmax ( attn , dim = 3 ) output = torch . einsum ( 'hblt,bte,ehv,dhv->hbd' , [ attn , v , self . w_vs , self . w_fc ]) output = self . dropout ( output ) output = self . layer_norm ( output + q ) return output , attn In [56]: n_heads = 8 d_k = 32 d_v = 64 d_model = 100 t = 51 l = 53 batch = 30 layer1 = initialize ( MultiHeadAttentionOld ( n_heads , d_k = d_k , d_v = d_v , d_model = d_model )) . eval () . cuda () layer2 = initialize ( MultiHeadAttentionNew ( n_heads , d_k = d_k , d_v = d_v , d_model = d_model )) . eval () . cuda () args = dict ( q = torch . randn ( batch , l , d_model ), k = torch . randn ( batch , t , d_model ) * 0.1 , v = torch . randn ( batch , t , d_model ), mask = torch . randn ( batch , l , t ) > 0 , ) args = { k : v . cuda () for k , v in args . items ()} o1 , a1 = layer1 ( ** args ) o2 , a2 = layer2 ( ** args ) In [57]: a1 . shape , a2 . shape Out[57]: (torch.Size([240, 53, 51]), torch.Size([8, 30, 53, 51])) In [58]: assert torch . allclose ( o1 , o2 ) In [59]: % timeit - n 200 layer1 ( ** args ); torch . cuda . synchronize () % timeit - n 200 layer2 ( ** args ); torch . cuda . synchronize () 4.82 ms \u00b1 73.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 200 loops each) 4.6 ms \u00b1 116 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 200 loops each)","title":"Transformer's attention needs more attention"},{"location":"source_examples/Pytorch/#self-attention-gans","text":"SAGANs are currently SotA for image generation, and can be simplified using same tricks. In [60]: #left class Self_Attn_Old ( nn . Module ): \"\"\" Self attention Layer\"\"\" def __init__ ( self , in_dim , activation ): super ( Self_Attn_Old , self ) . __init__ () self . chanel_in = in_dim self . activation = activation self . query_conv = nn . Conv2d ( in_channels = in_dim , out_channels = in_dim // 8 , kernel_size = 1 ) self . key_conv = nn . Conv2d ( in_channels = in_dim , out_channels = in_dim // 8 , kernel_size = 1 ) self . value_conv = nn . Conv2d ( in_channels = in_dim , out_channels = in_dim , kernel_size = 1 ) self . gamma = nn . Parameter ( torch . zeros ( 1 )) self . softmax = nn . Softmax ( dim =- 1 ) # def forward ( self , x ): \"\"\" inputs : x : input feature maps( B X C X W X H) returns : out : self attention value + input feature attention: B X N X N (N is Width*Height) \"\"\" m_batchsize , C , width , height = x . size () proj_query = self . query_conv ( x ) . view ( m_batchsize , - 1 , width * height ) . permute ( 0 , 2 , 1 ) # B X CX(N) proj_key = self . key_conv ( x ) . view ( m_batchsize , - 1 , width * height ) # B X C x (*W*H) energy = torch . bmm ( proj_query , proj_key ) # transpose check attention = self . softmax ( energy ) # BX (N) X (N) proj_value = self . value_conv ( x ) . view ( m_batchsize , - 1 , width * height ) # B X C X N out = torch . bmm ( proj_value , attention . permute ( 0 , 2 , 1 ) ) out = out . view ( m_batchsize , C , width , height ) out = self . gamma * out + x return out , attention In [61]: #right class Self_Attn_New ( nn . Module ): \"\"\" Self attention Layer\"\"\" def __init__ ( self , in_dim ): super () . __init__ () self . query_conv = nn . Conv2d ( in_dim , out_channels = in_dim // 8 , kernel_size = 1 ) self . key_conv = nn . Conv2d ( in_dim , out_channels = in_dim // 8 , kernel_size = 1 ) self . value_conv = nn . Conv2d ( in_dim , out_channels = in_dim , kernel_size = 1 ) self . gamma = nn . Parameter ( torch . zeros ([ 1 ])) def forward ( self , x ): proj_query = rearrange ( self . query_conv ( x ), 'b c h w -> b (h w) c' ) proj_key = rearrange ( self . key_conv ( x ), 'b c h w -> b c (h w)' ) proj_value = rearrange ( self . value_conv ( x ), 'b c h w -> b (h w) c' ) energy = torch . bmm ( proj_query , proj_key ) attention = F . softmax ( energy , dim = 2 ) out = torch . bmm ( attention , proj_value ) out = x + self . gamma * rearrange ( out , 'b (h w) c -> b c h w' , ** parse_shape ( x , 'b c h w' )) return out , attention In [62]: model_old = initialize ( Self_Attn_Old ( 128 , None )) model_new = initialize ( Self_Attn_New ( 128 )) In [63]: x = torch . randn ( 2 , 128 , 30 , 30 ) assert torch . allclose ( model_old ( x )[ 0 ], model_new ( x )[ 0 ], atol = 1e-4 ) # returned attention is transposed assert torch . allclose ( model_old ( x )[ 1 ], model_new ( x )[ 1 ], atol = 1e-4 ) In [64]: % timeit model_old ( x )[ 0 ] . sum () . item () % timeit model_new ( x )[ 0 ] . sum () . item () # surprise - I had slow down here due to the order of softmax, not einops 15.9 ms \u00b1 990 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) 14.5 ms \u00b1 81.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)","title":"Self-attention GANs"},{"location":"source_examples/Pytorch/#improving-time-sequence-prediction","text":"While this example was considered to be simplistic, I had to analyze surrounding code to understand what kind of input was expected. You can try yourself. Additionally now the code works with any dtype, not only double; and new code supports using GPU. In [65]: #left class SequencePredictionOld ( nn . Module ): def __init__ ( self ): super ( SequencePredictionOld , self ) . __init__ () self . lstm1 = nn . LSTMCell ( 1 , 51 ) self . lstm2 = nn . LSTMCell ( 51 , 51 ) self . linear = nn . Linear ( 51 , 1 ) def forward ( self , input , future = 0 ): outputs = [] h_t = torch . zeros ( input . size ( 0 ), 51 , dtype = torch . double ) c_t = torch . zeros ( input . size ( 0 ), 51 , dtype = torch . double ) h_t2 = torch . zeros ( input . size ( 0 ), 51 , dtype = torch . double ) c_t2 = torch . zeros ( input . size ( 0 ), 51 , dtype = torch . double ) for i , input_t in enumerate ( input . chunk ( input . size ( 1 ), dim = 1 )): h_t , c_t = self . lstm1 ( input_t , ( h_t , c_t )) h_t2 , c_t2 = self . lstm2 ( h_t , ( h_t2 , c_t2 )) output = self . linear ( h_t2 ) outputs += [ output ] for i in range ( future ): # if we should predict the future h_t , c_t = self . lstm1 ( output , ( h_t , c_t )) h_t2 , c_t2 = self . lstm2 ( h_t , ( h_t2 , c_t2 )) output = self . linear ( h_t2 ) outputs += [ output ] outputs = torch . stack ( outputs , 1 ) . squeeze ( 2 ) return outputs In [66]: #right class SequencePredictionNew ( nn . Module ): def __init__ ( self ): super ( SequencePredictionNew , self ) . __init__ () self . lstm1 = nn . LSTMCell ( 1 , 51 ) self . lstm2 = nn . LSTMCell ( 51 , 51 ) self . linear = nn . Linear ( 51 , 1 ) def forward ( self , input , future = 0 ): b , t = input . shape h_t , c_t , h_t2 , c_t2 = torch . zeros ( 4 , b , 51 , dtype = self . linear . weight . dtype , device = self . linear . weight . device ) outputs = [] for input_t in rearrange ( input , 'b t -> t b ()' ): h_t , c_t = self . lstm1 ( input_t , ( h_t , c_t )) h_t2 , c_t2 = self . lstm2 ( h_t , ( h_t2 , c_t2 )) output = self . linear ( h_t2 ) outputs += [ output ] for i in range ( future ): # if we should predict the future h_t , c_t = self . lstm1 ( output , ( h_t , c_t )) h_t2 , c_t2 = self . lstm2 ( h_t , ( h_t2 , c_t2 )) output = self . linear ( h_t2 ) outputs += [ output ] return rearrange ( outputs , 't b () -> b t' ) In [67]: seq_old = SequencePredictionOld () . double () seq_new = SequencePredictionNew () . double () initialize ( seq_old ) initialize ( seq_new ) x = torch . randn ([ 10 , 10 ], dtype = torch . double ) In [68]: result_old = seq_old ( x ) result_new = seq_new ( x ) assert torch . allclose ( result_old , result_new )","title":"Improving time sequence prediction"},{"location":"source_examples/Pytorch/#transforming-spacial-transformer-network-stn","text":"In [69]: #left class SpacialTransformOld ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () # Spatial transformer localization-network self . localization = nn . Sequential ( nn . Conv2d ( 1 , 8 , kernel_size = 7 ), nn . MaxPool2d ( 2 , stride = 2 ), nn . ReLU ( True ), nn . Conv2d ( 8 , 10 , kernel_size = 5 ), nn . MaxPool2d ( 2 , stride = 2 ), nn . ReLU ( True ) ) # Regressor for the 3 * 2 affine matrix self . fc_loc = nn . Sequential ( nn . Linear ( 10 * 3 * 3 , 32 ), nn . ReLU ( True ), nn . Linear ( 32 , 3 * 2 ) ) # Initialize the weights/bias with identity transformation self . fc_loc [ 2 ] . weight . data . zero_ () self . fc_loc [ 2 ] . bias . data . copy_ ( torch . tensor ([ 1 , 0 , 0 , 0 , 1 , 0 ], dtype = torch . float )) # Spatial transformer network forward function def stn ( self , x ): xs = self . localization ( x ) xs = xs . view ( - 1 , 10 * 3 * 3 ) theta = self . fc_loc ( xs ) theta = theta . view ( - 1 , 2 , 3 ) grid = F . affine_grid ( theta , x . size ()) x = F . grid_sample ( x , grid ) return x In [70]: #right class SpacialTransformNew ( nn . Module ): def __init__ ( self ): super ( Net , self ) . __init__ () # Spatial transformer localization-network linear = nn . Linear ( 32 , 3 * 2 ) # Initialize the weights/bias with identity transformation linear . weight . data . zero_ () linear . bias . data . copy_ ( torch . tensor ([ 1 , 0 , 0 , 0 , 1 , 0 ], dtype = torch . float )) self . compute_theta = nn . Sequential ( nn . Conv2d ( 1 , 8 , kernel_size = 7 ), nn . MaxPool2d ( 2 , stride = 2 ), nn . ReLU ( True ), nn . Conv2d ( 8 , 10 , kernel_size = 5 ), nn . MaxPool2d ( 2 , stride = 2 ), nn . ReLU ( True ), Rearrange ( 'b c h w -> b (c h w)' , h = 3 , w = 3 ), nn . Linear ( 10 * 3 * 3 , 32 ), nn . ReLU ( True ), linear , Rearrange ( 'b (row col) -> b row col' , row = 2 , col = 3 ), ) # Spatial transformer network forward function def stn ( self , x ): grid = F . affine_grid ( self . compute_theta ( x ), x . size ()) return F . grid_sample ( x , grid ) new code will give reasonable errors when passed image size is different from expected if batch size is divisible by 18, whatever you input in the old code, it'll fail no sooner than affine_grid.","title":"Transforming spacial transformer network (STN)"},{"location":"source_examples/Pytorch/#improving-glow","text":"That's a good old depth-to-space written manually! Since GLOW is revertible, it will frequently rely on rearrange -like operations. In [71]: #left def unsqueeze2d_old ( input , factor = 2 ): assert factor >= 1 and isinstance ( factor , int ) factor2 = factor ** 2 if factor == 1 : return input size = input . size () B = size [ 0 ] C = size [ 1 ] H = size [ 2 ] W = size [ 3 ] assert C % ( factor2 ) == 0 , \" {} \" . format ( C ) x = input . view ( B , C // factor2 , factor , factor , H , W ) x = x . permute ( 0 , 1 , 4 , 2 , 5 , 3 ) . contiguous () x = x . view ( B , C // ( factor2 ), H * factor , W * factor ) return x def squeeze2d_old ( input , factor = 2 ): assert factor >= 1 and isinstance ( factor , int ) if factor == 1 : return input size = input . size () B = size [ 0 ] C = size [ 1 ] H = size [ 2 ] W = size [ 3 ] assert H % factor == 0 and W % factor == 0 , \" {} \" . format (( H , W )) x = input . view ( B , C , H // factor , factor , W // factor , factor ) x = x . permute ( 0 , 1 , 3 , 5 , 2 , 4 ) . contiguous () x = x . view ( B , C * factor * factor , H // factor , W // factor ) return x In [72]: #right def unsqueeze2d_new ( input , factor = 2 ): return rearrange ( input , 'b (c h2 w2) h w -> b c (h h2) (w w2)' , h2 = factor , w2 = factor ) def squeeze2d_new ( input , factor = 2 ): return rearrange ( input , 'b c (h h2) (w w2) -> b (c h2 w2) h w' , h2 = factor , w2 = factor ) term squeeze isn't very helpful: which dimension is squeezed? There is torch.squeeze , but it's very different. in fact, we could skip creating functions completely - it is a single call to einops anyway","title":"Improving GLOW"},{"location":"source_examples/Pytorch/#detecting-problems-in-yolo-detection","text":"In [73]: #left def YOLO_prediction_old ( input , num_classes , num_anchors , anchors , stride_h , stride_w ): bs = input . size ( 0 ) in_h = input . size ( 2 ) in_w = input . size ( 3 ) scaled_anchors = [( a_w / stride_w , a_h / stride_h ) for a_w , a_h in anchors ] prediction = input . view ( bs , num_anchors , 5 + num_classes , in_h , in_w ) . permute ( 0 , 1 , 3 , 4 , 2 ) . contiguous () # Get outputs x = torch . sigmoid ( prediction [ ... , 0 ]) # Center x y = torch . sigmoid ( prediction [ ... , 1 ]) # Center y w = prediction [ ... , 2 ] # Width h = prediction [ ... , 3 ] # Height conf = torch . sigmoid ( prediction [ ... , 4 ]) # Conf pred_cls = torch . sigmoid ( prediction [ ... , 5 :]) # Cls pred. FloatTensor = torch . cuda . FloatTensor if x . is_cuda else torch . FloatTensor LongTensor = torch . cuda . LongTensor if x . is_cuda else torch . LongTensor # Calculate offsets for each grid grid_x = torch . linspace ( 0 , in_w - 1 , in_w ) . repeat ( in_w , 1 ) . repeat ( bs * num_anchors , 1 , 1 ) . view ( x . shape ) . type ( FloatTensor ) grid_y = torch . linspace ( 0 , in_h - 1 , in_h ) . repeat ( in_h , 1 ) . t () . repeat ( bs * num_anchors , 1 , 1 ) . view ( y . shape ) . type ( FloatTensor ) # Calculate anchor w, h anchor_w = FloatTensor ( scaled_anchors ) . index_select ( 1 , LongTensor ([ 0 ])) anchor_h = FloatTensor ( scaled_anchors ) . index_select ( 1 , LongTensor ([ 1 ])) anchor_w = anchor_w . repeat ( bs , 1 ) . repeat ( 1 , 1 , in_h * in_w ) . view ( w . shape ) anchor_h = anchor_h . repeat ( bs , 1 ) . repeat ( 1 , 1 , in_h * in_w ) . view ( h . shape ) # Add offset and scale with anchors pred_boxes = FloatTensor ( prediction [ ... , : 4 ] . shape ) pred_boxes [ ... , 0 ] = x . data + grid_x pred_boxes [ ... , 1 ] = y . data + grid_y pred_boxes [ ... , 2 ] = torch . exp ( w . data ) * anchor_w pred_boxes [ ... , 3 ] = torch . exp ( h . data ) * anchor_h # Results _scale = torch . Tensor ([ stride_w , stride_h ] * 2 ) . type ( FloatTensor ) output = torch . cat (( pred_boxes . view ( bs , - 1 , 4 ) * _scale , conf . view ( bs , - 1 , 1 ), pred_cls . view ( bs , - 1 , num_classes )), - 1 ) return output In [74]: #right def YOLO_prediction_new ( input , num_classes , num_anchors , anchors , stride_h , stride_w ): raw_predictions = rearrange ( input , 'b (anchor prediction) h w -> prediction b anchor h w' , anchor = num_anchors , prediction = 5 + num_classes ) anchors = torch . FloatTensor ( anchors ) . to ( input . device ) anchor_sizes = rearrange ( anchors , 'anchor dim -> dim () anchor () ()' ) _ , _ , _ , in_h , in_w = raw_predictions . shape grid_h = rearrange ( torch . arange ( in_h ) . float (), 'h -> () () h ()' ) . to ( input . device ) grid_w = rearrange ( torch . arange ( in_w ) . float (), 'w -> () () () w' ) . to ( input . device ) predicted_bboxes = torch . zeros_like ( raw_predictions ) predicted_bboxes [ 0 ] = ( raw_predictions [ 0 ] . sigmoid () + grid_w ) * stride_w # center x predicted_bboxes [ 1 ] = ( raw_predictions [ 1 ] . sigmoid () + grid_h ) * stride_h # center y predicted_bboxes [ 2 : 4 ] = ( raw_predictions [ 2 : 4 ] . exp ()) * anchor_sizes # bbox width and height predicted_bboxes [ 4 ] = raw_predictions [ 4 ] . sigmoid () # confidence predicted_bboxes [ 5 :] = raw_predictions [ 5 :] . sigmoid () # class predictions # merging all predicted bboxes for each image return rearrange ( predicted_bboxes , 'prediction b anchor h w -> b (anchor h w) prediction' ) We changed and fixed a lot: new code won't fail if input is not on the first GPU old code has wrong grid_x and grid_y for non-square images new code doesn't use replication when broadcasting is sufficient old code strangely sometimes takes .data , but this has no real effect, as some branches preserve gradient till the end if gradients not needed, torch.no_grad should be used, so it's redundant","title":"Detecting problems in YOLO detection"},{"location":"source_examples/Pytorch/#simpler-output-for-a-bunch-of-pictures","text":"Next time you need to output drawings of you generative models, you can use this trick In [75]: fake_batch = torch . rand ([ 100 , 3 , 1 , 1 ]) + torch . zeros ([ 100 , 3 , 32 , 32 ]) from matplotlib import pyplot as plt import torchvision.utils as vutils In [76]: #right device = 'cpu' plt . imshow ( np . transpose ( vutils . make_grid ( fake_batch . to ( device )[: 64 ], padding = 2 , normalize = True ) . cpu (),( 1 , 2 , 0 ))) Out[76]: <matplotlib.image.AxesImage at 0x7fdd8d606908> In [77]: #right padded = F . pad ( fake_batch [: 64 ], [ 1 , 1 , 1 , 1 ]) plt . imshow ( rearrange ( padded , '(b1 b2) c h w -> (b1 h) (b2 w) c' , b1 = 8 ) . cpu ()) Out[77]: <matplotlib.image.AxesImage at 0x7fdd8d595390> In [78]: # TODO: Hierarchical softmax # TODO: some reinforcement stuff would also be needed","title":"Simpler output for a bunch of pictures"},{"location":"source_examples/Pytorch/#instead-of-conclusion","text":"Better code is a vague term; to be specific, code is expected to be: reliable: does what expected and does not fail. Explicitly fails for wrong inputs maintainable and modifiable reusable: understanding and modifying code should be easier than writing from scratch fast: in my measurements, proposed versions have speed similar to the original code readability counts, as a mean to achieve previous goals Provided examples show how to improve these criteria for deep learning code. And einops helps a lot.","title":"Instead of conclusion"},{"location":"source_examples/Pytorch/#links","text":"pytorch and einops significant part of the code was taken from the official examples and tutorials . All code fragments were taken for educational purpose. (references for other code are given in source of this html) einops has a tutorial for a more gentle introduction In [ ]:","title":"Links"}]}