{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einops tutorial, part 2: deep learning\n",
    "\n",
    "[Previous part](https://github.com/arogozhnikov/einops/tree/master/docs) provides visual examples with numpy.\n",
    "\n",
    "## What's in this tutorial?\n",
    "\n",
    "- working with deep learning packages\n",
    "- important cases for deep learning models\n",
    "- `einsops.asnumpy` and `einops.layers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "x = numpy.random.RandomState(42).normal(size=[10, 32, 100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def guess(x):\n",
    "    display_html(\"\"\"\n",
    "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">{x}</a> (hover to see)</h4>\n",
    "    \"\"\".format(x=tuple(x)), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select your flavour\n",
    "\n",
    "Switch to the framework you're most comfortable with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one from 'chainer', 'gluon', 'tensorflow', 'pytorch' \n",
    "flavour = 'pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected pytorch backend\n"
     ]
    }
   ],
   "source": [
    "if flavour == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "    tf.enable_eager_execution()\n",
    "    tape = tf.GradientTape(persistent=True)\n",
    "    tape.__enter__()\n",
    "    x = tf.contrib.eager.Variable(x) + 0\n",
    "elif flavour == 'pytorch':\n",
    "    import torch\n",
    "    x = torch.from_numpy(x)\n",
    "    x.requires_grad = True\n",
    "elif flavour == 'chainer':\n",
    "    import chainer\n",
    "    x = chainer.Variable(x)\n",
    "else:\n",
    "    assert flavour == 'gluon'\n",
    "    import mxnet as mx\n",
    "    mx.autograd.set_recording(True)\n",
    "    x = mx.nd.array(x, dtype=x.dtype)\n",
    "    x.attach_grad()\n",
    "print('selected {} backend'.format(flavour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([10, 32, 100, 200]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x), x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple computations \n",
    "\n",
    "- simple computations are not different from those for numpy.\n",
    "- same code works for tensors of different libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(10, 100, 200, 32)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's start with a simple example\n",
    "# converting bchw to bhwc format and back is a common operation.\n",
    "# try to predict output shape and then check your guess\n",
    "y = rearrange(x, 'b c h w -> b h w c')\n",
    "guess(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "- Gradients are a corner stone of deep learning\n",
    "- You can normally backpropagate through einops operations (just if those were native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(320., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y0 = x\n",
    "y1 = reduce(y0, 'b c h w -> b c', 'max')\n",
    "y2 = rearrange(y1, 'b c -> c b')\n",
    "y3 = reduce(y2, 'c b -> ', 'sum')\n",
    "\n",
    "if flavour == 'tensorflow':\n",
    "    print(reduce(tape.gradient(y3, x), 'b c h w -> ', 'sum'))\n",
    "else:\n",
    "    y3.backward()\n",
    "    print(reduce(x.grad, 'b c h w -> ', 'sum'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(10, 640000)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# flattening is another common operation, \n",
    "# which happens on a boundary between convolutional layers and fully connected layers\n",
    "y = rearrange(x, 'b c h w -> b (c h w)')\n",
    "guess(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(10, 128, 50, 100)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# space-to-depth operation\n",
    "y = rearrange(x, 'b c (h h1) (w w1) -> b (h1 w1 c) h w', h1=2, w1=2)\n",
    "guess(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(10, 8, 200, 400)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# depth-to-space operation\n",
    "y = rearrange(x, 'b (c h1 w1) h w -> b c (h h1) (w w1)', h1=2, w1=2)\n",
    "guess(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(10, 32)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simple global average pooling.\n",
    "y = reduce(x, 'b c h w -> b c', reduction='mean')\n",
    "guess(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(10, 32, 50, 100)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# max-pooling with a kernel 2x2\n",
    "y = reduce(x, 'b c (h h1) (w w1) -> b c h w', reduction='max', h1=2, w1=2)\n",
    "guess(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d, 2d and 3d pooling are defined in a similar way\n",
    "\n",
    "for sequential 1-d models, you'll probably want pooling over time\n",
    "```python\n",
    "reduce(x, '(t t2) b c -> t b c', reduction='max', t2=2)\n",
    "```\n",
    "\n",
    "for volumetric models, all three dimensions are pooled\n",
    "```python\n",
    "reduce(x, 'b c (x x2) (y y2) (z z2) -> b c x y z', reduction='max', x2=2, y2=2, z2=2)\n",
    "```\n",
    "\n",
    "Uniformity is a strong point of `einops`, and you don't need specific operation for each particular case.\n",
    "\n",
    "\n",
    "### Good exercises \n",
    "\n",
    "- write a version of space-to-depth for 1d and 3d (2d is provided above)\n",
    "- write an average / max pooling for 1d models. Those are frequently in NLP to woth with lengths of arbitrary length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze and unsqueeze (expand_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models typically work only with batches, \n",
    "# so to predict a single image ...\n",
    "image = rearrange(x[0, :3], 'c h w -> h w c')\n",
    "# ... create a dummy 1-element axis ...\n",
    "y = rearrange(image, 'h w c -> () c h w')\n",
    "# ... imagine you predicted this with a convolutional network for classification, we'll just flatten axes ...\n",
    "predictions = rearrange(y, 'b c h w -> b (c h w)')\n",
    "# ... finally, decompose (remove) dummy axis\n",
    "predictions = rearrange(predictions, '() class -> class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keepdims-like behavior for reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(10, 32, 100, 200)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(10, 32, 100, 200)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# empty composition () provides dimensions of length 1, which are broadcastable, \n",
    "\n",
    "# per-channel mean-normalization for each image:\n",
    "y = x - reduce(x, 'b c h w -> b c () ()', 'mean')\n",
    "guess(y.shape)\n",
    "\n",
    "# per-channel mean-normalization for whole batch:\n",
    "y = x - reduce(y, 'b c h w -> () c () ()', 'mean')\n",
    "guess(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(10, 100, 200, 32)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(100, 200, 32, 10)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stack along first dimension\n",
    "list_of_tensors = list(x)\n",
    "tensors = rearrange(list_of_tensors, 'b c h w -> b h w c')\n",
    "guess(tensors.shape)\n",
    "# or maybe stack along last dimension?\n",
    "tensors = rearrange(list_of_tensors, 'b c h w -> h w c b')\n",
    "guess(tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(1000, 200, 32)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(100, 200, 320)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# concatenate over first dimension?\n",
    "tensors = rearrange(list_of_tensors, 'b c h w -> (b h) w c')\n",
    "guess(tensors.shape)\n",
    "\n",
    "# or maybe concatenate along last dimension?\n",
    "tensors = rearrange(list_of_tensors, 'b c h w -> h w (b c)')\n",
    "guess(tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling within a dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(10, 32, 100, 200)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(10, 32, 100, 200)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# channel shuffle (as it is drawn in shufflenet paper)\n",
    "y = rearrange(x, 'b (g1 g2 c) h w-> b (g2 g1 c) h w', g1=4, g2=4)\n",
    "guess(y.shape)\n",
    "\n",
    "# simpler version of channel shuffle\n",
    "y = rearrange(x, 'b (g c) h w-> b (c g) h w', g=4)\n",
    "guess(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split a dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(10, 8, 100, 200)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>Answer is: <a class=\"anchor-link jp-InternalAnchorLink\" href=\"#Z\">(10, 100, 200)</a> (hover to see)</h4>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NB: some symbolic backends don't support simply iterating over the first dimension\n",
    "# when network predicts several bboxes for each position, here's a convenient way to work with it\n",
    "# 8 bboxes, 4 coordinates each\n",
    "bbox_x, bbox_y, bbox_w, bbox_h = rearrange(x, 'b (coord bbox) h w -> coord b bbox h w', coord=4, bbox=8)\n",
    "max_bbox_area = reduce(bbox_w * bbox_h, 'b bbox h w -> b h w', 'max')\n",
    "guess(bbox_x.shape)\n",
    "guess(max_bbox_area.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when implementing custom gated activation (GLU), split is needed\n",
    "y1, y2 = rearrange(x, 'b (split c) h w -> split b c h w', split=2)\n",
    "# typically result = y1 * sigmoid(y2) or something very similar\n",
    "\n",
    "# ... but we could split differently\n",
    "y1, y2 = rearrange(x, 'b (c split) h w -> split b c h w', split=2)\n",
    "\n",
    "# first one splits channels into consequent groups: y1 = x[:, :x.shape[1] // 2, :, :]\n",
    "# while second takes channels with a step: y1 = x[:, 0::2, :, :]\n",
    "\n",
    "# these make big difference when input is \n",
    "# - a result of group convolution\n",
    "# - a result of bidirectional LSTM/RNN\n",
    "# Let's focus on the second case, since it is less obvious. \n",
    "# For instance, in cudnn LSTM output is concatenated of forward-in-time and backward-in-time outputs\n",
    "# Also in pytorch GLU splits channels into consequent groups (first way)\n",
    "# So when LSTM's output comes to GLU, ...\n",
    "# ... forward-in-time produces linear part, and backward-in-time produces activation ... \n",
    "# ... and role of directions is different, and gradients coming to two parts are different\n",
    "\n",
    "# einops notation helps detecting such inconsistencies when packing several things into a single dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_2d(x):\n",
    "    # imagine we have a simple 2d convolution with padding, so output has same shape as input\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import parse_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagine we are working with 3d data\n",
    "x_5d = rearrange(x, 'b c x (y z) -> b c x y z', z=20)\n",
    "# but we have only 2d convolutions. \n",
    "# That's not a problem, since we can apply\n",
    "y = rearrange(x_5d, 'b c x y z -> (b z) c x y')\n",
    "y = convolve_2d(y)\n",
    "# not just specifies additional information, but verifies that all dimensions match\n",
    "y = rearrange(y, '(b z) c x y -> b c x y z', **parse_shape(x_5d, 'b c x y z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 10, 'c': 32, 'x': 100, 'y': 10, 'z': 20}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_shape(x_5d, 'b c x y z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch': 10, 'c': 32}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can skip some dimensions by writing underscore\n",
    "parse_shape(x_5d, 'batch c _ _ _')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Striding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, how to convert any operation into a strided operation \n",
    "# (like convolution with strides, aka dilated/atrous convolution)\n",
    "\n",
    "# each image is split into subgrids, each is now a separate \"image\"\n",
    "y = rearrange(x, 'b c (h hs) (w ws) -> (hs ws b) c h w', hs=2, ws=2)\n",
    "y = convolve_2d(y)\n",
    "y = rearrange(y, '(hs ws b) c h w -> b c (h hs) (w ws)', hs=2, ws=2)\n",
    "\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "For frameworks that prefer operating with layers, layers are available.\n",
    "\n",
    "You'll need to import a proper one depending on your backend:\n",
    "\n",
    "```python\n",
    "from einops.layers.chainer import Rearrange, Reduce\n",
    "from einops.layers.gluon import Rearrange, Reduce\n",
    "from einops.layers.keras import Rearrange, Reduce\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "```\n",
    "\n",
    "`Einops` layers are behaving in the same way as operations, and have same parameters \n",
    "(for the exception of first argument, which should be passed during call)\n",
    "\n",
    "```python\n",
    "layer = Rearrange(pattern, **axes_lengths)\n",
    "layer = Reduce(pattern, reduction, **axes_lengths)\n",
    "\n",
    "# apply layer to tensor\n",
    "x = layer(x)\n",
    "```\n",
    "\n",
    "Usually it is more convenient to use layers, not operations, to build models\n",
    "```python\n",
    "# example given for pytorch, but code in other frameworks is almost identical\n",
    "from torch.nn import Sequential, Conv2d, MaxPool2d, Linear, ReLU\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "model = Sequential(\n",
    "    Conv2d(3, 6, kernel_size=5),\n",
    "    MaxPool2d(kernel_size=2),\n",
    "    Conv2d(6, 16, kernel_size=5),\n",
    "    # combined pooling and flattening\n",
    "    Reduce('b c (h h2) (w w2) -> b (c h w)', 'max', h2=2, w2=2), \n",
    "    Linear(16*5*5, 120), \n",
    "    ReLU(),\n",
    "    Linear(120, 10), \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- `einops` operates with different deep learning frameworks, accepts various tensors and allows automatic gradient computation\n",
    "- interface is uniform: same code works for different frameworks\n",
    "- code for different dimensionality is written in a uniform way\n",
    "- layers are provided for easier integration of `einops` into models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
